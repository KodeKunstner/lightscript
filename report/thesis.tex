\documentclass[11pt]{report} 
\usepackage{a4} 
%\usepackage[paperheight=220mm,paperwidth=170mm]{geometry}
%\usepackage[paperheight=150mm,paperwidth=200mm]{geometry}
\usepackage{geometry}
\usepackage{makeidx}
%\usepackage[danish]{babel} 
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx} 
\usepackage{verbatim} 
\usepackage{fancyhdr}
\usepackage{listings} 
%\usepackage[colorlinks,pagebackref]{hyperref}
\usepackage[colorlinks, linkcolor=black, anchorcolor=black, citecolor=black, menucolor=black, pagecolor=black, urlcolor=black]{hyperref}
%\usepackage{backref}
\usepackage{url}
\frenchspacing
\makeindex
\pagestyle{plain}
\lstset{language=java}
\lstset{escapeinside=`'}
\lstset{basicstyle=\scriptsize}

\title{
\emph{Document currently under development} \\ ~ \\
Design and implementation of \\
compact scripting languages \\ 
for low-end mobile devices \\
{\scriptsize version \input{version}}}

\author{
  Rasmus Erik Voel Jensen\footnote{
    sumsar@solsort.dk
  }
} 

% Remember 
%    \index terms
%    update template as well as document when structural updates.
\date{2009}

\begin{document}

\maketitle
\begin{abstract}
This thesis creates two new languages: LightScript and Yolan, which run on top of Java Mobile Edition and enable scripting on very low-end mobile devices.

The code footprint is the major limitation on low end mobile devices, and both languages have a significantly smaller code footprint than existing scripting languages. 
The languages are comparable in speed to larger scripting language implementations,
and an order of magnitude faster than most of the benchmarked scripting languages for mobile devices.

Both scripting languages have first class functions, dynamic typing, built-in support for hashtables, stacks, etc., and support interactive programming. They are also able to load and execute scripts presented in source form at run-time. 
The Java Mobile Edition does not support dynamic loading of code, so both languages are interpreted.

LightScript is a subset of JavaScript; it is static scoped, and also includes support for objects and exceptions. It is compiled to, and executed on, a stack-based virtual machine. One of things that makes the compact implementation possible is that the LightScript parser is an imperative optimised version of the top-down operator precedence parser.

Yolan is highly optimised for reducing the size of the implementation code footprint. It has dynamic scope, a Lisp-inspired syntax, and is interpreted by evaluating each node of the syntax tree.

Yolan has a code footprint less than half the size of LightScript, and they are similar in speed, even though they have very different evaluation strategies.
\end{abstract}

\setcounter{tocdepth}{1}
\tableofcontents

\chapter*{TODO-list for the report}
{\scriptsize
add chapter introductions and summaries
\paragraph{Introduction}
drafted. perhaps note on mojax, switch of direction, and back
\begin{verbatim}
SURVEY:
- organise and write mobile-platform section
   - finish embedded systems
   - add smartphones and pda's - symbian, linux, android, iphone, rim, palm, wince, openmoko
   - add web-platform
- JavaScript
- write about mobile scripting
- add programming languages/ideas: haskell, scheme/lisp, tcl, lua, python, smalltalk, ...
- maybe JIT compilation
- maybe monadic parsers
- maybe region based GC
\end{verbatim}
\paragraph{Approach}
rewrite intro
\paragraph{LightScript}
add language specification. add language design, perhaps more implemenation details
\paragraph{Yolan}
add design and implementation details, and semantic details in specification
\paragraph{Benchmarks}
add summary for each result section
\paragraph{Discussion}
MISSING
\paragraph{Conclusion}
MISSING
\paragraph{Bibliography}
MISSING
\paragraph{Benchmark source}
change to lstlisting
\paragraph{Yolan source}
ok for now, maybe add other class
\paragraph{LightScript source}
Needs more comments. Maybe add other classes
\paragraph{Index}
MISSING. Walk through when other parts are less drafty
}
\chapter{Introduction}
This chapter introduces the project, starting with description and then motivation for the project, and a bit about the structure of this report.


\section{Project description NB:SYNOPSIS-NEEDS-SYNC}
    The topic of the project is to design and implement a scripting language
that runs on very low-end mobile devices. This is both to create a practical tool, and
also a focus for exploration of programming language theory. 

\begin{comment}
The motivation is that a scripting language makes it is easier to make applications for mobile 
devices, and that existing freely available scripting languages
are very limited, slow, or simply does not run on the low-end mobile devices.
\end{comment}

    The educational goals are to learn about programming language design and
implementation, and to learn about programming on mobile devices. Through the
project, I should be able to evaluate and choose programming language features
and implementation techniques, and design and implement a scripting language.

    The focus of the language is that it should be portable, embeddable and have
a low memory footprint. Portable implies that it should run on different devices,
from very low-end mobile phones to high-end computers, possibly also within a web browser. 
Embeddable implies that it should be easy to include within and interface with
other applications. Low memory footprint implies that it should be suitable for
running on platforms where the available memory is measured in kilobytes rather
than megabytes. 

    The approach will be pragmatic and favor simplicity.

\section{Motivation}

Scripting languages make it easier to write applications\cite{scripting-ousterhout}. Beside higher productivity, they are also for more customisation, possibly user based.
On more power full devices, ranging from high-end smartphones to personal computers, there are very good scripting languages available.
Scripting language implementations usually take a lot of resources, which is a problem on low-end mobile devices, where they may not be available, are very slow, or have limitations, such as they cannot be executed directly, but need to be compiled to another devices, or they do not have basic datatypes.
The focus on better implementation of a scripting languages for mobile devices is thus a niche, where the result may actually be of practical use.

The focus on low-end devices also has another benefit:
it broadens the number of devices on which the language may run.
While very low-end devices are becoming uncommon in Denmark,
they still live on in countries with less information technology penetration.
Thus by targeting the very low-end devices, 
this may make scripting, and thus easier content creation,
available where it has not been available before,
and thus could be the beginning of stepping stone 
towards more information and computing literacy.
The restrictions of low end mobile devices also imposes challenges, that may lead to interesting solutions.

\begin{comment}
From a personal point of view, 
I would like to get started on development for mobile devices, 
and would also like to brush up on programming language implementation.
Design and implementation of a scripting language for mobile devices is spot on this topic.
\end{comment}


\section{The structure and content of the report}

The report starts out with some background information: this introduction, and a survey on topics related to the project in Chapter~\ref{survey}.
Then the methodology and design for the project are elaborated in Chapter~\ref{method}, including some of the principal design choices for the language implementations.
The next couple of chapters are then the actual results: the LightScript language in Chapter~\ref{lightscript}, the Yolan language in Chapter~\ref{yolan}, and some benchmarking of the two new languages compared to existing scripting languages in Chapter~\ref{benchmark}.
Finally there is a discussion of the results in Chapter~\ref{discussion}, rounding off with a conclusion in Chapter~\ref{conclusion}

Some parts of the developed source code has been attached, such that the implementation details described in Chapter~\ref{lightscript},~\ref{yolan} and~\ref{benchmark}, can also be seen in practice.
There is also an index to make it easier to find specific parts of the report.

Each chapter contains a short introduction and summary, which makes it easier to skim, and get an overview of the report.

\section{Summary}
The purpose of this project is create and learn about scripting language implementation on low end mobile devices.
This is motivated by that it is a niche where such development may actually be practical.
The report is structured with: introduction, survey, approach, language implementations, benchmarks, discussion and conclusion.

\chapter{Survey}
\label{survey}

\section{Mobile platforms}

Mobile devices ranges from low end phones, which, if they are programmable at all, only support limited Java Mobile Edition midlets, up to advanced smartphones with performance resource similar to older desktop computers.

When looking mobile devices, it should be noticed some survey \cite{smartphonesurvey} only focus smartphones, and thus the market shares does not include the more low end devices.

Most low end devices support some kind of mobile Java.
Here there are different APIs and device profiles, but the basic execution and deployment model is the same for those.

Low end devices rarely support loading of native code, and higher end devices may require different kind of code signing to allow native programs to be distributed.



\begin{comment}
Development for mobile devices is very diverse.
Surveys about mobile devices often focus on smartphones (high end mobile devices) and thus market shares in surveys, such as \cite{smartphonesurvey} does not include the more low end devices.
On the contrary, this project focuses on development also targeting low end devices.

Looking at lowest end mobiled devices: if user applications are loadable at all, it is in the shape of Java Midlets.
...

Higher end devices often supports native code.
This has different APIs and there are lots of different platforms.
Some of the main platforms for smartphones, and similar are:
Symbian, Qt-embedded (a linux), RIM-BlackBerry, iPhone, Android (a linux with a Java-like vm on top), Windows CE, etc.
... description of different operating systems...

Many of these platforms requires some fees/licenses and code signing for developing and/or distributing applications. 
This makes them less attractive for further studies as well as further development.

Additionally, the BREW mobile platform seems to play an important role in North America, but it is closed and needs code signing, and at the same time it is not common in Europe, so I will not look further into that platform.

Besides Java and native code, JavaScript is becoming a potential language for applications for high end phone. 
This is both due to its integration with the web, which means that it will be available on the phones with advanced browsers, due to the increased amount of memory on high end phones
and due to recent major performance advances within the JavaScript implementations, which are propagating towards mobile devices.

\end{comment}

\subsection{Embedded systems}
The personal computers, PCs, as we know them, are only a very small fraction of the computers in use.
Billions of electronic devices nowadays have small computers embeddes. They are in mobile phones, kitchen equipment, washing machines, music instruments, car, even advanced greeting cards. 
These embedded computer systems, vary tremendously, from low end micro controllers with memory measured in bytes, up to powerful CPUs with vector processing units and many megabytes of memory.

\subsubsection{Machine architecture}
There are two significantly different classes of computers: The Harvard architecture, and the von Neumann architecture. The difference is whether the program code is in the same memory as the data or if they are separated.

Larger computers usually use the von Neumann architecture where program and data is in the same memory, and this is the most commonly known model.
On small micro controllers on the other hand, the data memory is usually separated from the program memory, which is what is called the Harvard architecture. 
These two approaches to architecture dates back to some of the early computers: the Harvard Mark I \cite{harvard-mark1} and the EDVAC \cite{edvac} by John von Neumann.

On small micro controllers, programs are usually stored in flashable ROM, and on the Harvard architecture the separation of data and code makes it easier and cheaper to implement, for example size of the code words can be independent of the size of the data words.
These may have less than a kilobyte of RAM, and some kilobytes of possibly flashable ROM for the executable code. Examples of this are the PIC-processors \cite{picspec}, or the ATmega 8-bit microcontrollers \cite{atmegaspec}.
An interesting operating system for low end devices is the Contiki \cite{contiki}, which also has some details on very lightweight implementation of threads.

On more powerfull embedded devices, for example mobile phones, and larger computers,
it may sometimes be practical to be able to work with the code as data, and thus von Neumann architecture makes more sense there.
For mobile and larger embedded devices a typical CPU architecture is the ARM \cite{arm-architecture}, which is a 32 bit RISC architecture.

\subsubsection{Development platform}

While embedded systems are very very common, the software is a shipped, or possibly flashable updateable on larger system, but there are generally no support for software development.
So usually access to debug boards and expensive hardware is needed to get experiment with embedded devices.
Another approach is that some devices is hackable in the sense that somebody has found out how to upload and customise the firmware, usually without documentation and support from the manufacturer -- examples here ranges from digital cameras, mobile phones and handheld gaming devices.
An exceptional case on small embedded devices, where the manufacturer has actually opened up for development is the Lego brick that is a part of the Lego Mindstorm NXT, which will be elaborated in the next section.
On larger devices it happens more often, though it is still rare: two smartphones \cite{openmoko, htc-android}, an E-book-reader, and some wireless routers and network attached storage \cite{wrtg, nslu, buffalo} have also opened, or partly opened, up for development.

The Lego Mindstorm NXT is interesting because it is an easy to get started with embedded development platform. 
The main cpu is an ARM7TDMI \cite{arm7tdmi}, which is commonly used in mobile phones, and also in many other embedded devices. 
It is clocked at 48MHz, and is attached to 256 KB of flashable ROM and 64 KB of RAM, which means that it is low end, compared to mobile phones, while powerful viewed as an embedded device.
There is also an ATmega48 \cite{atmega48} 8-bit coprocessor, which has 4KB of flashable ROM and 512 bytes of RAM, which is a typical microcontroller. 
The coprocessor usually reads sensors, controls motors, and can power the main cpu down, to reduce power usage.
The system also have a 100x64 black/white lcd display, and a couple of buttons, so it has a lot of potential as a development substitute platform for low-end mobile device, which does not allow 3rd party firmware.

\subsection{The J2ME / Java Micro Edition}
J2ME (Java Micro Edition\footnote{Java Micro Edition is a recent rebranding of J2ME}) is the most common platform for mobile application, supported by more than a billion devices \cite{sun-j2me}. 
Most mobile devices either require strict code signing, or do not allow native applications to be loaded at all, which leads to that J2ME is often the only for mobile application development.
J2ME is a trimmed down Java Virtual Machine (JVM) so it has most of the features and limitations of a standard Java JVM, with some notable exceptions, such as the lack of dynamic class loading and reflection. It is fragmented platform which makes it more difficult to make applications that works across devices: there are different APIs/extensions from different vendors, and different device profiles for different hardware capabilities.

Applications for J2ME, are called Midlets and are distributed via JAR-files (Java ARchive). A JAR file is essentially a zip file containing the compiled Java classes, data files, and some meta information.

J2ME has two device configurations CLDC 1.0 (Compact Limited Device Configuration 1.0) \cite{cldc10}  and CLDC 1.1  \cite{cldc11}. The major difference between the two is that CLDC 1.0 is integer only, whereas CLDC 1.1 supports floating point numbers.
A limitation of both CLDCs is that they lack of support for reflection and dynamic code loading.
The lack of a reflection API impacts a scripting language in that it cannot discover native functionsn Instead such functions have to be added explicitly to the system.
The lack of run-time class loading, means that the only way to execute run-time loaded scripts is through intepretation. JIT compiling to JVM or native code is not possible.

The CLDCs are based upon the Java JVM \cite{jvmref}, with some instructions removed, and some metadata added to ensure stack discipline. 
In order to simplify the J2ME JVM implementation, Java class files has to be preverified, before they can be loaded.
The preverification add meta data about stack use, and removes certain instructions, such as local jump-and-save-register, to make it easier to implement a JVM, that is safe against malicious code trying to overflow the stack.
The reference implementation of CLDC 1.0, kvm, is a switch-based interpreter with a compacting mark-and-sweep garbage collector \cite{kvm}.


The JVM limits interpreter implementation: it does not support label references as values, nor does it support functions as values. This makes some of the optimisations discussed in 
Section~\ref{interpreter-implementation} impossible.
Instead it does have a builtin switch opcode as well as support for method-dispatch based on the type of an object, so an interpreter would either be switch-based, or have a class for every opcode.

The resources available for J2ME applications on mobile phones start at 64KB for the size of the JAR-file and 200KB for the run time memory on the lowest end devices, and goes upwards from there \cite{nokia-optim}.
These numbers are for the full application, so the resources available for an embeddable scripting language, could be significantly less than this, depending on the resource usage of the host application.

\subsubsection{Reducing the memory usage of J2ME applications}

Some devices only have little memory available,
and an optimisation here is to be able to avoid having memory intensive parts of the program run at the same time. For example, with a scripting language, it is desirable to be able to garbage collect the executed parts of long script when they are done, so the memory becomes available for other computations.
The usual size optimisations, such as finding compact representations, trimming dynamic data structures, avoiding sparse data, etc., are also applicable.
Here may be a tradeoff between the code footprint and run time memory, as compact representations and other optimisations may require more code to be implemented.

\subsubsection{Reducing the footprint of J2ME applications}
Some optimisations to reduce the code or JAR-file footprint:
\begin{itemize}
\item Reduce the number of class-files.
\item Write initialisation manually, where the automatic generated initialisation is inefficient.
\item Use a JAR-optimiser/obscurifier.
\item Put the classes in the unnamed package.
\end{itemize}

Another optimisation is to reduce the number of classes \cite{nokia-optim, kahlua-thesis}.
This may reduce the size of the JAR file significantly,
even though it does not change the amount of code:
Each class file has its own symbol table, which means that if classes are merged, then common symbols only needs to be represented once, rather than once for each class.
Furthermore JAR files are essentially zip-archives, and each file in a zip archive is compressed individually \cite{zipspec}, which means that small files typically get compressed less than larger files, due to the small compression context.
The downside of reducing the number of classes is that it could go against the object oriented design, and the implementation may more difficult to read and edit.

Initial values are not supported directly by the Java class file format, but instead
JVM-code is generated, which does the initialisation. 
This often inefficient. For example the initialisation:
\begin{verbatim}
byte[] bytes = { 1, 4, 3, 4, 5, 6, 2, 3, 1 } ;
\end{verbatim}
generates the code corresponding to
\begin{verbatim}
byte[] bytes = new byte[9];
bytes[0] = 1; bytes[1] = 4; bytes[2] = 3;
bytes[3] = 4; bytes[4] = 5; bytes[5] = 6;
bytes[6] = 2; bytes[7] = 3; bytes[8] = 1;
\end{verbatim}
which for larger initialisation is significantly more expensive than manually written initialisers,
which for the above example could be:
\begin{verbatim}
byte[] bytes = "\001\u004\u003\u004\u005\u006\u002\u003\u001".getBytes();
\end{verbatim}

Another thing to be aware of is, that strings in Java class files are encoded, such that only characters with unicode values between 1 and 127 (inclusive) uses one byte per character, so for large binary data, it may make sense to load it from external sources and not include it in the class file.

JAR file optimisation/obfuscation can beneficial for code footprint because it may:
remove unused code,
rename methods and classes, such that they use less space in the symbol table,
make \verb|static const|s work as \verb|#define|s,
 and optimise the code, thus shortening it.
This also allows one to make more readable code with less concern of code footprint, knowing that some of the more verbose parts will be optimised away.

Using the unnamed package also saves space in the symbol table, as class references becomes shorter.

The code footprint limit may be tighter that the run time memory limit, and it may be possible to partition the execution such that different parts of the applications do not need to use run time memory at the same time.
Thus, in this project with the design of an embedded scripting language, code footprint will be prioritised slightly higher than the run time memory usage, although both are important.

\subsection{Smartphones}

TODO:rewrite
Many high end phones -- Smartphones --  allows loading of native code, but with different restrictions.
Most require some kind of cryptographical signing of the code, before it can be distributed in any way, and sometimes there are also very tight control of the distribution channels.
Another issue is the very different execution platforms.
The main operating systems for smartphones are:

\subsubsection{OpenMoko}

OpenMoko is a project towards having open source phones.
It is both a software stack for smartphones, and it is also a small development division of the hardware company FIC, where they are making the first open source phones.
The two phones released so far have lots of rough edges both software and hardware wise, but from a developer point of view, these very interesting as they are not just phones but full Linux computers with X11 and most things as known from larger scale Linux environments -- and the source code is available for everything, so it is even possible to write new operating systems for them, and customise them in every way.



\subsubsection{Android}

Android is a software stack for smartphones. It is open source, and driven forward by google.
It is build upon a linux kernel, but user applications are not distributed natively, but executed on the Dalvik vm. Applications are written in Java, which is then compiled to Dalvik vm, rather than the usual JVM. The motivation for this is both to overcome some of the shortcomings of the JVM on mobile devices, and possibly also some cooperate issues with ownership of JVM technology.
Beside supporting Dalvik applications, there is also a modern webbrowser -- a WebKit derivative -- meaning that JavaScript can also be executed on the device.

While the software stack itself is open source, it also allows the manufactorer to make and keep proparitary changes for themselves. 
Meaning that actual phones designed with the stack -- at least those currently on the market -- are not truely open, in the sense that thay contain properitery drivers and special developer-versions of the phones are needed if you want to be able to load customised operating system images/native code.

Android is of course being ported to the devices from the OpenMoko project, and in that case it is of course truely open.

\subsubsection{Symbian}

Symbian is the classical operating system for smartphones. Applications are usually written in C/C++, with some restrictions to allow it to run better on with a moderately small amount memory. 
In order to distribute native applications on Symbian, it has to be cryptographically signed \cite{symbiansigned}.
Symbian is a currently closed source, but is on its way to be released as open source \cite{symbianopensource}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programming languages}
... introduction, mobile scripting...

\subsection{C and C++}

\subsection{Forth and other stack languages}
Forth is a stack based language.
It is very interesting for this project, as it is an example of an interpreted minimalistic language running on even very low-end devices.

The syntax is differnent from most modern languages, due to the reverse polish notation.
The functions work on a stack, which the programmer is explicit aware of. 
Functions or `words', replaces the parameters on the top of the stack with the result. 
New words/functions can be defined, and are concatenative in the sense that functions written after each other, has the same semantics as content of the functions written after eachother.
Beside the stack for parameters, there is also a stack for return addresses.

Forth looks very strange when starting looking at the programs, but it seems like it is the question of getting habit of being aware of the stack.
The programming style has focus towards decompositions. It also supports meta programming.
Forth systems has explicit compilation, the system has two modes: compile mode and interpret mode. 
It is possible to create words that are executed at compile time, using the immediate keyword.
Words -- ``functions'' in Forth -- are first class data types.

A couple of very interesting pointers related to Forth are: 
``Thinking Forth'' is a very good book, not only on Forth programming, but also touches a lot programming and problem solving in general.
There is also a tutorial Forth implementation \cite{jonesforth}, which is a good place to get started on how Forth can be implemented - it is the implementation of a full Forth system, written in assembler, but very readable and well documented.

Forth is also interesting from the virtual machine point of view, as this is a programming language which is similar to stack based virtual machines. So forth implementation techniques is shared with a lot of virtual machines. And research within this topic overlaps, an example is a dissertation on implementation of stack languages on register based machine \cite{ertl-dissertation}, as the representitive stack-based language.

Other more recent stack languages are Joy \cite{joy-language}, Cat \cite{cat-language} and Factor \cite{factor-language}.
Interesting development here are more typing, and also the use of anonymous code blocks, 
to create the control structure.


\subsection{Haskell}
There are two very interesting aspects of Haskell: it is pure functional, and it has lazy evaluation.

\subsection{Hecl and Tcl}

\subsection{Java}

\subsection{JavaScript/EcmaScript}
\label{JavaScript}
\subsubsection{Language description}
\subsubsection{Implementations}
... only open implementatoins

\paragraph{JavaScriptCore} is the new implementation of JavaScript in Webkit.
The latest version has is called SquirrelFish and is an optimised register-based virtual machine,
with emerging support for JIT compilation.
TODO:JavaScriptCore - old edition

\paragraph{QScript}

\paragraph{Rhino}

\paragraph{SpiderMonkey}
\label{spidermonkey}
SpiderMonkey \cite{spidermonkey} is the JavaScript engine 

\paragraph{TraceMonkey} is the next JavaScript engine from Mozilla.
It has a JIT compiler based on trace tree\cite{trace-tree}, 
which rather than JIT compiling the entire program,
traces and then compiles the most executed paths through the program.
This means that the JIT compiled code gets optimised to the actual execution,
and the approach also reduces the amount of code needed to be compiled.

\paragraph{V8} V8 \cite{v8} is a JavaScript implementation made by Google, and released in 2008 in conjunction with the release of their browser ``Chrome''. 
While it is not directly connected with the Android platform, a JIT compiler is already in place targeting the ARM CPU, so it seems likely that it will also target mobile devices in the long term.

A main focus and benefit of V8 is high execution speed.
The design builds upon experience from Self/Smalltalk implementations \cite{articles-before-v8}.
It is designed for JIT compilation from the beginning, and it also does some class inference to optimise methods and property accesses.
The idea here is that whenever a property is added to a JavaScript object this create an implicit class. When code is JIT'ed, a property access is compiled to a type check followed by fast code for accessesing the property, similar to that of more static, class based languages. This is possible due to the type check and implicit class, and it is much faster than a traditional JavaScript object property lookup.

The garbage collector is fast, - it is a generational garbage collector with two generations. The young generation is collected with a copy-collector which is linear in the time of live data, and thus heap allocation of activation records is almost free. 
This is combined with a mark and sweep collector when a full collect is needed.

\subsubsection{Mobile offsprings}
... mojax, and the other guy

\subsection{Lua}
... and kahlua

\subsection{Python}
As phones are becoming more powerful, Python is beginning to play a role as a mobile scripting language on high end devices. This is carrying over from being a popular and major scripting language on computer platforms.

Python have some nice features, that may be inspiring when implementing other languages. Comments/documentation is integrated in the language via the concept of docstrings, which is a special kind of comment that will also be available for inspection during runtime. Unit testing is also integrated within the documentation framework, where a special syntax indicates that code in the documentation can also be executed when running tests.
Required indentation leads to easier readable programs.

The issue with python on mobile devices is, that it is relatively resource intensive, and thus use more energy, and is only available on high end phones.

\subsection{Scheme and Lisp}
\subsection{Self}
\label{survey-self}
Self \cite{self} is especially interesting due to the prototypical inheritance.
In traditional object oriented programming, we have classes and objects, where classes can inherit from other classes, and objects are instantiation of classes.
Instead of classes and subclasses for inheritance, Self has a clone operator, which creates a new object using an existing object as blueprint.
An object in self contains a pointer to the parent (cloned) object, and a mapping from propertynames to values or methods. When a property is read, mapping is first searched, and if the name is not found there, the parent object is searched.
\subsection{Simkin}
\subsection{Smalltalk}

\section{Interpreter implementation}
\label{interpreter-implementation}
This section looks into a couple of aspects of interpreter implementation.
First is a discussion on different types of virtual machines, namely register and stack machines, and their different benefits and tradeoffs.
Then there is a discussion on the implementation of dispatch, which is one of the bottlenecks of intepreters.
Third there is a discussion of garbage collection techniques.
And finally there is a discussion on different approaches managing to run time stack.

\subsection{Register and stack machines}
Virtual machines are usually either register based or stack based.

Stack based virtual machines operates similar to the language Forth, where the operations work on the top of the stack. 
Operands are implicit coded, such that for example the add instruction just pops the two top elements of the stack, and pushes the sum. 
Stack machines are easy to compile to, 
which can simply be done by emitting the opcodes of a post-order walk through of the abstract syntax tree. 
There are no issues of register allocation, spilling, etc.
Stack machines are commonly used, the most known example is the Java Virtual Machine, and there many others when looking under the hood, for example: Python, the SpiderMonkey JavaScript implementation, and the .NET Common Intermediate Language.

Register based virtual machines are becoming more common. 
Usually they have a high number of registers, leading to longer opcodes than stack based virtual machines. But on the other hand, fewer opcodes are needed leading to faster execution \cite{register-vs-stack1, register-vs-stack2}. 
Examples of register based virtual machines are the Dalvik \cite{dalvik-vm} virtual machine, LLVM \cite{llvm}, Parrot \cite{parrot}, and the virtual machine of Lua 5.0 \cite{luavm}.

A third approach to virtual machines is just to use the abstract syntax tree for evaluation. This was for example used in earlier versions of WebKits JavaScript implementation, but has now been superseded by a stack virtual machine, and is currently being replaced by JIT compilation.

\subsection{Optimising the dispatch}
The dispatch is an important bottleneck when implementing interpreters.
An interpreter usually has a loop that fetches the next instruction, and executes it. The process of dispatching is to fetch opcode and goto the code for the next virtual instruction.
The reason this is a bottleneck it has to be executed for every opcode, and while dispatching may be done in few instructions, so may the code for the virtual instruction, and the dispatch is still a relatively large part of the execution time. 

The usual way is just to have a switch statement, which usually compiles to a jump table, such that:
\begin{verbatim}
for(;;) {
    // pc is the program counter into an array
    // of virtual instructions.
    switch(*(++pc)) { 
        case OP_FOO:
            ... implementation of FOO ...
            break; 
        case OP_BAR:
            ... implementation of BAR ...
            break; 
        case OP_BAZ:
            ... implementation of BAZ ...
            break; 
    }
}
\end{verbatim}
compiles optimised into something like:
\begin{verbatim}
.data:
jumptable = {
address of FOO,
address of BAR,
address of BAZ}

.code:
...
label looptop:
   inc register1  ; register1 contains the variable pc
   load register1 -> register2
   load jumptable[register2] -> register2
   jump_to register2
FOO:
    ... implementation of FOO ...
    jump looptop
BAR:
    ... implementation of BAR ...
    jump looptop
BAZ:
    ... implementation of BAZ ...
    jump looptop
...
\end{verbatim}
which is a simple and working approach, but is a bit inefficient. It is portable across compiler, and is the only option on some virtual machines,
This approach is used in SpiderMonkey, KVM\footnote{The reference implementation of JVM for mobile devices}, and in many other implementation.

If the compiler supports direct addressing of labels, we can instead do something like:
\begin{verbatim}
ops = { &OP_FOO, &OP_BAR, &OP_BAZ };
   
goto ops[++pc];

OP_FOO:
    ... implementation of FOO ...
    goto ops[++pc];
OP_BAR:
    ... implementation of BAR ...
    goto ops[++pc];
OP_BAZ:
    ... implementation of BAZ ...
    goto ops[++pc];
\end{verbatim}
This has two benefits: it removes the jump to the loop top, as it jumps directly to the next instruction after the table lookup, and the jumps themselves may be faster due to better branch prediction.
The reason the branch prediction may improve, is that branch prediction is often based on where previous branch at the specific code position were taken taken to. 
When the branch is at the end of the code for the virtual instruction,
the branch predictor has the current instruction as context for predicting the next instruction.
When the implementation is a switch based interpreter, the branch predictor has no context for predicting the next instruction.
This approach is called threaded code, and are up to twice as fast as switch based implementations \cite{ertl-efficient-2003}.
The cost is a bit more code, as the code for the table lookup needs to be at each instruction, rather than a single place. 

A further optimisation of the dispatch is to remove the table lookup. This is often done by replacing virtual machine instructions with the address of the implementation of the instruction. At the same time this costs space as an address usually uses 32 bit, where an virtual machine instruction often uses 8 bit. This is also better suited for stack machines, where the operands are coded implicit, rather that register machines, where the operands need to encoded beside the address of the implementation. This optimisation is commonly used in Forth implementations, and is also used other virtuel machines, for example optionally in the QScript \cite{qscript} and SquirrelFish \cite{webkit-source} JavaScript implementations.

Another way to remove the table lookup for the dispatch is to have a fixed size of each instruction implementation, and then calculate the position of the implementation from the opcode \cite{dalvik-talk}. When some instruction implemenations are significantly longer than others, the longer instruction implementations can contain a jump to the code that is beyond the fixed instruction implementation size.
For example:
\begin{verbatim}
INSTRUCTION_IMPLEMENTATIONS:
   ... implementation of FOO ...
   goto INSTRUCTION_IMPLEMENTATIONS + IMPLEMENTATION_SIZE * (++pc)
   ... padding ...
// address INSTRUCTION_IMPLEMENTATIONS + 1 * IMPLEMENTATION_SIZE
   ... implementation of BAR ...
   goto INSTRUCTION_IMPLEMENTATIONS + IMPLEMENTATION_SIZE * (++pc)
   ... padding ...
// address INSTRUCTION_IMPLEMENTATIONS + 2 * IMPLEMENTATION_SIZE
   ... first part of implementation of BAZ  which is a long instruction...
   goto REMAINING_IMPLEMENTATION_OF_BAZ
// address INSTRUCTION_IMPLEMENTATIONS + 3 * IMPLEMENTATION_SIZE
   ... implementation of other instructions...

REMAINING_IMPLEMENTATION_OF_BAZ:
   ... more implemenation of BAZ...
   goto INSTRUCTION_IMPLEMENTATIONS + IMPLEMENTATION_SIZE * (++pc)
\end{verbatim}
This has the benefit of being even faster, as the table lookup is eliminated,
but it is also more difficult to implement, 
as the size of the implementation depends on the machine architechture
and thus it is not possible to do in a high level language.
Notice that \verb|IMPLEMENTATION_SIZE| should be a power of two such that the multiplication is just a shift.
This optimisation is for example used in the Dalvik Virtual Machine \cite{dalvik-vm}


\subsection{Garbage collection}
Garbage collection is an important issue in the design of virtual machines.
There is a good introduction/survey on the topic \cite{gc-survey}.
An additional way to get into the subject is to see what approach recent scripting language implementations have taken.

The next section looks into aspects of garbage collections.
The following sections describes different approaches to garbage collection,
with a final section looking at garbage collection implementation in actual language implementations.

\subsubsection{Issues of dynamic memory management}
Some of the following issues are relevant for dynamic memory management in general, and other are more focussed towards garbage collection.

Locality covers that memory that is used at similar times also lies together in memory.  This has impact on the performance, due to levels of caching, and poor locality may also lead to more swapping to secondary storage when memory is scarce, and secondary storage is available.

Fragmentation is when there is unused space between allocated blocks of memory. 
Compacting garbage collection algorithms addresses this problem, by moving the blocks of memory to remove the fragmentation.

Performance of the garbage collection is also an issue. This covers both CPU usage, and delays during execution.
The garbage collection takes time, -- this comes both from the time from running the actual garbage collection algorithm, and also from lost caching, as garbage collection methods usually messes up the cache.
Another issue with garbage collection is whether it is interactive or that there may be longer interruptions or delay of execution, in which the garbage collection cycle runs. This leads to development concurrent garbage collection, that may take more time overall as a tradeof of shorter interruptions of the execution.

Garbage collection algorithms can be conservative or exact, where conservative garbage collection is not directed by strong typing and assumes that data are pointers, whereas exact garbage collection knows which of the data that are actually pointers that need to be tracked.

\subsubsection{Reference counting}

Each allocated block of memory also keeps track of the number of pointers to it, and when this is decreased to zero, the block is deallocated.
This has the benefit of being reasonable interactive.
The additional bookkeeping for each pointer assignment, gives it a bit slow performance, which i proportional to the program execution rather than the memory allocation. 
Another issue is that cyclic references does not get caught by it, so it needs to be combined with another garbage collector or some kind of cycle detection.

\subsubsection{Copying collector}
A typical copying garbage collector, splits the heap in two, and when one part of it is filled up, it copies the root nodes and all of their descendents into the other part, and updates the pointers.  This also has the benefit that it removes fragmentation. 
The execution time only depends on the amount of live data, and is executed whenever a part of the heap is filled up.
The issue here is that the heap is split in two, leading to less available memory, and also that this process is often a stop-the-world approach, leading to a moderately long delay when the collection occur.

\subsubsection{Mark and sweep}
Mark and sweep essentially works in two phases: first walk through the live objects and mark them as alive, and then walk through the heap and make unmarked objects available as free.
There are several variations over this topic. Rather than stop-the-world, it can be made more interactive by running in parallel with the execution, for example by coloring/marking ``alive and children marked'', ``alive but children not marked yet'' and ``not marked yet''.
Instead of a mark and sweep, a mark and compact approach is also possible, where fragmentation is removed at the cost of a bit more execution time.

\subsubsection{Generational Garbage Collection}
\label{generational-gc}
Generational garbage collection, can improve the performance of the collection. This is based on the hypothesis that recent allocated memory are more likely to be freed than older memory.
This is exploited by dividing the memory into generations, where the youngest generations are garbage collected more often than the older ones. 
Thus by only looking at a single generation/a smaller part of the memory, the garbage collection cycle is significant faster.
An issue here is to keep track of pointers from older generations to younger ones, such that the younger generations can be collected independently.
Another question is when to promote memory from younger to older generations.

\subsubsection{Examples of garbage collection implementation}
A common approach is to combine reference counting with some kind of cycle cleanup. 
An example of this is the python implementation. 
One of the benefits of this approach is most of the garbage collection only yields small pauses which will not be felt by the user in an interactive environment, unlike the stop-the-world approach.

The KVM, which is the reference implementation for mobile Java Virtual Machines, is now using a compacting mark and sweep. It originally started out with a copying collector, which had a larger memory requirement, and also had a period with non-compacting mark and sweep.


A practical example of a generational garbage collector is the one which is a part of the V8 JavaScript implementation.
It has two generations, where the young generation is a copying-compacting collector, which is called often, combined with a mark-and-sweep for the full heap, when that is needed.
The complexity of a copying collector is linear in the size of live objects, which means that there are essentially no overhead for heap allocating the run time stack with this garbage collection strategy, which solves some of the problems of the run time stack, as described later on.

\section{Parsing}
A language implementation must parse the source text into the abstract syntax tree to work on. 

An issue here is that parsers often take quite a large amount of code, especially if they are generated by compiler-compilers.
Generated lexers and LALR(1) parsers, generated by for example  \cite{yacc, yacc2}, usually have quite large state tables.
Recursive descent parsers seems to use a bit less code footprint that LALR parsers, but still requires functions for the all the grammar productions, which may still be expensive.

Grammar based parser generation and implementation is extensively studied and described for example in  \cite{basics-of-compiler-design, grammar}, and will not be discussed further in these sections.
Another approach, which is also very elegant, 
but have recieved less focus is the
top down operator precedences parser,
which will be discussed below.

\subsection{Top down operator precedence parsers}
\label{tdop}
Top down operator precedence parsing combines recursive descent parsing with operator precedence, which simplifies the implementation significantly.
It was described thirty years ago in  \cite{top-down-operator-precedence}, but has not had much attention until lately with  \cite{beautiful-code}.

A token can have a null denominator function, a left denominator function and a precedence.
The null denominator function is used to build the abstract syntax tree node, if the token stands first (is leftmost) in an expression.
The left denominator function is used to build the abstract syntax tree node, if we already have something to the left of the token within the expression.
The precedence of the next token is used to determine whether we are done parsing an expression or need to use the that token to build another abstract syntax tree node, by calling left denominator function of the token.

To be able to build the abstract syntax tree node, the left denominator function gets the parsed node to the left of the token as a parameter. Additionally it is possible for the denominator functions to parse expressions to the right of the token by calling the parsing function recursively. Here it is necessary that the parsing function also takes a priority as a parameter. This priority parameter makes sure that you do not call left denominator functions for tokens with lower priority.

The parsing itself is then simply done, by first calling null denominator function of the first token, and then calling the left denominator functions of the next tokens, as long as the next token has higher priority than the priority passed to the parsing function.
The denominator functions attached to each token are then responsible for building the syntax tree.
So the core loop of the parser is implemented like:
\begin{verbatim}
define parse(int priority):
    syntax_tree = next_token.null_denominator()
    while next_token.priority > priority:
        syntax_tree = next_token.left_denominator(syntax_tree)
    return syntax_tree
\end{verbatim}

Now we need to assign meaningfull precedence and denominator functions to the tokens:
Atoms, variable names, literals etc., just need to have a null denominator functions that return their node. Unary operations such as minus, not, ..., needs to have null denominator that calls parse once, and create a node that applies the operator to the parsed node.
Binary (infix) operators are made by assigning the left denominator function to something that create a node with its parameter as the left hand side and calling parse to make the right hand side. Binary operators can be made right associative by reducing the priority passed to parse.
Lists just calls parse until the end of the list is reached. And other constructions can be made similarly. 

The limitation of this parser as described here is that only one left hand side expression is passed to the left denominator function, making reverse polish notation languages difficult to implement. A non-recursive version with an explicit stack instead could solve that, though it may not be an issue with most syntaxes. 

\section{Scope}
The scope is important when designing programming languages.
There are two major kind of scopes: static scoping and dynamic scoping.
Static scope is also called lexical scope, and corresponds to the lexical structure of the source code.
This is in contrast to dynamic scope, where the variables are accessible other places than in the blocks where they are defined. This is exemplified by the following:
\begin{verbatim}
function f() {
    x := 17
}
function g(x) {
    f()
    print(x)
}
g(42)
\end{verbatim}
which would print 42 if it were written in a statically scoped language, and would print 17 if it were written in a dynamically scoped language.

Static scope is more natural as the scope matches the structure of the code. Dynamic scope requires more discipline, as it allows the programmer to tamper with local variables of other parts of the code, and counters good habits of information hiding. A very importeant feature is also that static scope can be used to create closures for functions, which gives extra flexibility to the language.
On the other hand, dynamic scope very trivial to implement, and can thus use slightly less space in the code footprint. Dynamic scope does also not have the issue with the funarg problem discussed in Section~\ref{funarg}.
For more advanced language implementations static scope has the benefit over dynamic scope, that it is easier to analyse and optimise due to locality, in the sense that its use is limited to the local lexical scope, and thus the optimisations on this part of the code need be concerned with other parts of the code.

\subsection{Stacks and activation records}
A typical way to implement local variables is with the use of a stack.
The following text pretends we are on a full stack machine, 
in practical implementations on CPUs, the top of the stack is usually a number of registers instead.

Whenever a function is called, the parameters is pushed onto the stack. Then when the function is entered, a return pointer is often pushed to the same stack, and there is allocated space for the local variables, the computations are done, and we return, jumping back and restoring the stack size to the original.
The allocation on the stack allows for local variables, and functions can also be recursive without a problem. The space the stack with the local variables, etc., is called the activation record for the function.

\subsection{Higher order functions, and the funarg problem}
\label{funarg}

When we have higher order functions and static scoping, we cannot just place the activation records on the stack. The reason for this is that inner functions may live on, and acces local variables of the outer function, after the outer function has returned. Consider the following code:
\begin{verbatim}
function f(x) {
    function g(y) {
        return x + y
    }
    return g
}
\end{verbatim}

The issue in this code is that the new function $g$ lives on after $f$ has returned, and at the same time $g$ uses a value that lies on the activation record of $f$. Therefor, if the activation record is just allocated on the call stack, and nothing further is done about it, the value of $x$ will no longer be available at the time $g$ is called.

There are several solutions to this problem:
\begin{itemize}
\item Disallowing/not supporting first order nested functions or access to outer scope within nested functions. Or not having static scope.
\item Outer scope variables cannot be changed from inner scope, but is instead passed as immutable values at function/scope creation. 
\item Allocate the activation records on the heap, instead of the stack, and let them be garbage collected. 
\item Keep track of which may live on after the function exits, and move those to the heap.
\end{itemize}

Not having nested functions, only having a single-function-local and a global scope, or not having static scope, simplifies the implementation and is a solution in mostly imperative languages.
The effect of this is that the funarg-problem is disallowed from the language rather than solved, and has the cost of not supporting closures either.

Copying outer scope variables to inner scope makes good sense in functional languages, where they are will not be mutated. The issue with this approach is that if the variable is mutated in an inner function, it is only the copy that is mutated, and the mutation is local to the function.
A workaround for mutations in languages that copies variables from outer to inner scope is to us a reference to the mutated variable rather than the variable itself, such that when the reference is copied to the inner scope it is still possible to mutate the value, as we need not mutat the reference itself.

Another approach is to let the outer scope stay alive until the inner closure dies, and then just reference variables as usual.
This can be done by allocating the activation records on a garbage collected heap,
which is not as expensive as it may seem, due to the high effiency of modern garbage collectors.
Another way to do this is to keep track on which variables may stay alive at the exit of a function, and move them to the heap at that time.
Or just identify variables that are used by in inner functions, and put those on the heap.

\subsection{Implementation of variables}
When we are implementing the scope, an issue is how to implement the variables. 
Here are several options: 

The  classical good-performing approach is to implement them on a stack. On each function call, an activation record is added on a linear stack, and each of the variables can be spilled into its place on the activation record. This is very fast as allocation and deallocation is just a question of increasing/decreasing the stack pointer. The issue with this approach is that extra code is needed to handle the funarg problem.

Another approach is to allocate the stack frame on the heap, and let it be cleaned up with the garbage collector. The quality of this approach depends a lot on the garbage collector, as it has nearly no overhead with a good generational garbage collector \cite{generational-heap}, see section~\ref{generational-gc}.
This also solves the funarg problem as the closure has a reference to the activation record, which will then not be garbage collected.

Usually the variable is resolved at compilation time, or the first time it is encountered, but 
some interpreters have a less efficient approach, where they lookup up variables at run time, either through a stack, or a table of variables.

\section{Summary}
\chapter{Approach}
\label{method}
\section{Overall methodology}

When making projects, the approach can be
more or less bottom-up or top-down.
A top-down approach could be like the waterfall model\cite{waterfall} of software devlopment,
where we start with the specification, design and then implementation and test.
The bottom-up is could be like stepwise refinement\cite{stepwise-refinement}, more agile\cite{agile-manifesto, extreme-programming}, starting out with a quick prototype, which gets more and more refined.

The purpose of this project is to learn about programming languages and mobile development, and create a scripting language. 
The bottom-up approach is choosen, as it opens more up for experimentation, and new ideas are easier follow, than with the top-down approach.

The methodology of the project is to make a series of prototypes related to mobile scripting language implementation, while surveying the field. 
The best parts of the prototypes are expanded and built further on, to get the scripting language, while documenting and specifying the language in parallel.

\subsection{Prototypes}
The design and implementation of the languages is not just a single design, but comes through ca. 30 prototypes of aspects of the languages.
This leads up to LightScript and Yolan, which will be described in the next chapters.
Discarded prototypes include a Forth-like language, several parser/compiler implementations on top of JavaScript, a couple of virtual machins on the jvm, some experiments towards an implementation in C, experiments with mobile applications and their GUI, and drafts of parts of what becomes LightScript and Yolan.

\section{Some design choices}

This section looks defines some of common the overall design choices and goals for the languages of the project.

The major target platform should be Java Mobile Edition CLDC/1.0, as this is the most limited platform that I have access to. 
If the language runs on top of CLDC/1.0, it will also be able run on other device configurations, with minor change, which is most mobile devices.

The language should be easy to embed, so when targeting the JVM, most of the datatypes should map directly to standard Java classes, in order to make it easier for the developers to interface with the scripting language from the host application.

As with most other scripting languages, the type system should be dynamic. 

The parser may assume that the programs have valid syntax. 
Usually parsers both build a syntax tree of valid programs and also reject programs with invalid syntax. 
By removing the rejecting-part, and only require that the can parsers build a syntax tree from valid programs, the parser may be optimised to run better on the limited devices.
The parser should still be robust and terminate, even with invalid programs though the resulting syntax tree may be strange.

Execution should be online, in the sense that whenever a full statement is read from the input, it should be executed, not needing to read the full file. This is both practical for interactive evaluation, and also has a benefit memorywise as the entire program never needs to be fully in memory, as executed code may be garbage collected.

The language implementations are experimental proof-of-concepts. They may be worth building further on, but they are not fully mature languages from the begining.

The design should be inspired by EcmaScript/JavaScript, as this is the most widely deployed scripting language, and would thus be a good translation target for the scripting language implemented here.

Access to variables should be optimised to be reasonably fast, as the target platform is already very slow. 
This rules out implementation techniques of looking up variables at run time by traversing a stack, or looking up the name in a table.

\section{Evaluation}
The evaluation should compare the code footprint and execution speed of the the developed languages, with existing scripting language.
In addition to this, it could also be interesting to look at how different parts of a language implementation adds to the code footprint.

\section{Summary}
\chapter{Yolan}
\label{yolan}
\index{Yolan}
\section{Design choices}
\begin{comment}
null-is-false
dynamic scope

... every expression yields a value, possibly false. Yes, you can bind a variable to the result of a false condition.

... lazy ffi.

\subsection{* Syntax}
A lisp-like syntax is choosen in order to simplify the parser and thereby reduce the code footprint.
Square brackets are used instead of parenthesis in order to indicate that the lists are not lisp-like cons-lists, but arrays. 

\section{* Implementation details}
\subsection{* AST rewriting}

\end{comment}
\section{Language specification}

\subsection{Syntax}
The syntax is similar to lisp, in the sense that function applications are written as lists, where the first element in the list is the function to be applied.
Programs consist of lists, where each element is either a list, a variable name, or a literal.

\subsubsection{Function application / lists}
Function applications are written in lisp-style as lists. The first element in the list is the function to be applied.
Lists are enclosed within square brackest \verb|[|$\cdots$\verb|]|, and may be nested. The elements within the lists are separated by whitespaces. 
As lists are the notation for function applications, every list must have at least one element, which is the function to be applied.

\subsubsection{Variable names}
A variable name is a sequence of characters. The possible characters are letters, numbers, the symbols \verb"!#$'()*+-,-./:<=>?@\^_`{|}~", and any unicode symbol with an unicode alue of 127 or higher. The first character in the name of a variable must be non-numeric.

\subsubsection{Integer literals}

Integers are written as a sequence of digits (\verb|0123456789|). Only base 10 input is possible and only non-negative numbers can be written as literals. Negative integers must be generated by subtraction.

\subsubsection{Comments and whitespaces}
Characters with a unicode value of 32 or less are regarded as whitespaces. This includes the usual space, tab, newline, and line-feed. Whitespaces are used to separate list elements, and are discarded during parsing. A comment must be preceded by a whitespace, starts with a semicolon \verb|;| and continues until the end of the line. Comments are discarded during parsing.

\subsection{Builtin functions}
The builtin functions are listed in the following sections. As the language is designed for embedding in other applications, there are no standard input/output function, file access, network, etc. as these might not be present or differ significantly between target devices/platforms.

In the following, the function names are written with {\tt fixed width} font, and the parameters are written in $cursive$. Parameters can be any expression, and are named according to their type or function: $num$s are expressions that should evaluate to numbers, $exp$s are expressions that may be optionally evaluated (e.g. in {\tt if}), $val$s are expressions will be evaluated, $string$s are expressions that should evaluate to a string, and so on.

\subsubsection{Variables}
\subsubsection*{\tt{[set }$name$ $value$\tt{]}}
Evaluate $value$ and let $name$ refer to the result.

\subsubsection*{\tt{[locals [}$name_1 \cdots name_n$\tt{]} $expr_1 \cdots expr_n$\tt{]}}
Let $name_1 \cdots name_n$ be local variables in $expr_1 \cdots expr_n$: First save the values corresponding to $name_1 \cdots name_n$, then evaluate $expr_1 \cdots expr_n$, next restore the values of $name_1 \cdots name_n$ and finally return the result of the evaluation of $expr_n$.

\subsubsection{Conditionals and logic}
\subsubsection*{\tt{[if }$cond$ $expr_1$ $expr_2$\tt{]}}
Evaluate $cond$ and if the result is non-$nil$ then evaluate and return $expr_1$, else evaluate and return $expr_2$.

\subsubsection*{\tt{[not }$cond$\tt{]}}
If $cond$ is $nil$ return $true$ else return $nil$.

\subsubsection*{\tt{[and }$expr_1$ $expr_2$\tt{]}}
Evaluate $expr_1$ and if it is non-$nil$, evaluate and return the value of $expr_2$, else return $nil$.

\subsubsection*{\tt{[or }$expr_1$ $expr_2$\tt{]}}
Evaluate $expr_1$ and if it is non-$nil$ return its value, else evaluate and return the value of $expr_2$.

\subsubsection{Repetition and sequencing}
\subsubsection*{\tt{[repeat }$num$ $expr_1 \cdots expr_n$\tt{]}}
Evaluate $expr_1 \cdots expr_n$ $num$ number of times ($num$ must evaluate to a number). The result is the last execution of $expr_n$, or $nil$ if no expressions were evaluated, i.e. $num \leq 0$.

\subsubsection*{\tt{[foreach }$name$ $iterator$ $expr_1 \cdots expr_n$\tt{]}}
For every value from the $iterator$, bind it to the local $name$ and evaluate $expr_1 \cdots expr_n$. The result of the evaluation is the last executed $expr_n$ or $nil$ if no expressions were evaluated. $name$ is a local variable, and is thus saved before the loop, and restored afterwards.

\subsubsection*{\tt{[while }$cond$ $expr_1 \cdots expr_n$\tt{]}}
While $cond$ evaluates to non-$nil$, evaluate $expr_1 \cdots expr_n$, and return the value of the last $expr_n$ or $nil$ if no expressions were evaluated.


\subsubsection*{\tt{[do }$expr_1 \cdots expr_n$\tt{]}}
Evaluate $expr_1 \cdots expr_n$ and return the result of $expr_n$.


\subsubsection{Functions}
\subsubsection*{\tt{[lambda [}$name_1 \cdots name_n$\tt{]} $expr_1 \cdots expr_n$\tt{]}}
Create a new anonymous function, with the parameters $name_1\cdots name_n$. Application of the function will bind its arguments to local variables $name_1\cdots name_n$, evaluate $expr_1\cdots expr_n$ and return $expr_n$.
\subsubsection*{\tt{[defun [}$name_{function}$ $name_1 \cdots name_n$\tt{]} $expr_1 \cdots expr_n$\tt{]}}
Create a new function, and bind it to the variable $name_{function}$. It is equivalent to {\tt{[set }}$name_{function}${\tt{ [lambda [}}$name_1 \cdots name_n${\tt{]}} $expr_1 \cdots expr_n${\tt{]]}}.

\subsubsection*{\tt{[apply }$function$ $param_1 \cdots param_n$\tt{]}}
Apply the $function$ to the parameters $param_1\cdots param_n$. The difference between this and the usual function application {\tt{[}$function$ $param_1\cdots param_n$\tt{]}} is that that \verb|apply| allows $function$ to change between invocations, whereas the usual function application assumes that $function$ is static to be able to optimise it during runtime.

\subsubsection{Integer operations}
\subsubsection*{\tt{[+ }$num_1$ $num_2$\tt{]}}
Calculate the sum of two integers.
\subsubsection*{\tt{[- }$num_1$ $num_2$\tt{]}}
Calculate the difference of two integers, the result is $num_2$ subtracted from $num_1$.
\subsubsection*{\tt{[* }$num_1$ $num_2$\tt{]}}
Calculate the product of two integers.
\subsubsection*{\tt{[/ }$num_1$ $num_2$\tt{]}}
Integer division, $num_1$ is divided by $num_2$.
\subsubsection*{\tt{[\% }$num_1$ $num_2$\tt{]}}
Returns the remainder of dividing $num_1$ by $num_2$.

\subsubsection{Type predicates}
\subsubsection*{\tt{[is-integer }$val$\tt{]}}
Returns $true$ if $val$ is an integer.
\subsubsection*{\tt{[is-string }$val$\tt{]}}
Returns $true$ if $val$ is a string.
\subsubsection*{\tt{[is-list }$val$\tt{]}}
Returns $true$ if $val$ is a list.
\subsubsection*{\tt{[is-dictionary }$val$\tt{]}}
Returns $true$ if $val$ is a dictionary.
\subsubsection*{\tt{[is-iterator }$val$\tt{]}}
Returns $true$ if $val$ is a iterator.

\subsubsection{Polymorphic functions}
\subsubsection*{\tt{[equals }$val_1$ $val_2$\tt{]}}
Compare $val_1$ to $val_2$ and return $true$ if they are the same, or $nil$ if they are different. $val_1$ and $val_2$ must have the same type, and should either be integers or strings.
\subsubsection*{\tt{[is-empty }$val$\tt{]}}
Returns $true$ if a list, dictionary or iterator does not have any elements. Else it returns $nil$.
\subsubsection*{\tt{[put }$container$ $position$ $value$\tt{]}}
Store a value into a a list or a dictionary. If it is a list, the $position$ must be an integer in the range $0,1, \cdots, ${\tt{[size }}$container${\tt{]}}$-1$.
If it is a dictionary, the position must be a string or an integer. An entry is deleted from a dictionary by storing $nil$ as the $value$.
\subsubsection*{\tt{[get }$container$ $position$\tt{]}}
Retrieve a value from a list or a dictionary. It has the same constraints on $position$ as with \verb|put|. Retrieving an uninitialised entry from a dictionary yields $nil$.
\subsubsection*{\tt{[random }$val$\tt{]}}
If $val$ is an integer, return a random number in the range $0,1, \cdots, val -1$. If $val$ is a list, pick a random value from the list.
\subsubsection*{\tt{[size }$val$\tt{]}}
Return the length of a string, the number of values in a list, or the number of entries in a dictionary.
\subsubsection*{\tt{[< }$val_1$ $val_2$\tt{]}}
Compares $val_1$ with $val_2$. If $val_1$ and $val_2$ are an integers, return $true$ if $val_1$ is strictly less than $val_2$ and otherwise $nil$.
If $val_1$ and $val_2$ are strings, do a lexicographical comparison and return $true$ if $val_1$ comes strictly before $val_2$, and otherwise $nil$.
\subsubsection*{\tt{[<= }$num_1$ $num_2$\tt{]}}
Compares $val_1$ with $val_2$. If $val_1$ and $val_2$ are an integers, return $true$ if $val_1$ is less than or equal to $val_2$ and otherwise $nil$.
If $val_1$ and $val_2$ are strings, do a lexicographical comparison and return $true$ if they are equal or $val_1$ comes before $val_2$, and otherwise $nil$.

\subsubsection{String functions}
\subsubsection*{\tt{[stringjoin }$val_1\cdots val_n$\tt{]}}
Create a string by concatenating $val_1\cdots val_n$.
If $val_i$ is an integer or a list, it is converted to a string.
A list is converted to a string by concatenating its elements, as if {\tt stringjoin} were called with the list elements as parameters.

\subsubsection*{\tt{[substring }$string$ $num_{begin}$ $num_{end}$\tt{]}}
Create a substring from a string, starting inclusively at character position $num_{begin}$ and ending exclusively at character position $num_{end}$. The positions starts counting at $0$, so thus {\tt{[substring }$string$ $0$ \tt{[size }$string$\tt{]]}} is the entire string.

\subsubsection{List functions}
\subsubsection*{\tt{[list }$val_1\cdots val_n$\tt{]}}
Create a new list, containing the elements $val_1\cdots val_n$.

\subsubsection*{\tt{[resize }$list$ $num$\tt{]}}
Change the size of the $list$ to be $num$ values. 
If the new size is larger than the previous, new values will be added to the end of the list, and they are initialised to be nil. If it is smaller, then the list will be truncated at the end. The list is returned.

\subsubsection*{\tt{[push }$list$ $val$\tt{]}}
Push the value $val$ at the end of the $list$. The size of the list grows by one, and the last element is now $val$.

\subsubsection*{\tt{[pop }$list$\tt{]}}
Remove the element from the end list. The function returns that element, and reduces the size of the list by one.

\subsubsection{Dictionary functions}
\subsubsection*{\tt{[dict }$key_1$  $val_1$ $\cdots$ $key_n$ $val_n$\tt{]}}
Create a new dictionary with $n$ entries, where $key_1$ maps to $val_1$ and so forth.

\subsubsection{Iterator functions}
\subsubsection*{\tt{[keys }$dictionary$\tt{]}}
Create a new iterator across the keys of a dictionary.
\subsubsection*{\tt{[values }$container$\tt{]}}
Create a new iterator across the values of either a dictionary or a list.
\subsubsection*{\tt{[get-next }$iterator$\tt{]}}
Get the next element from the iterator, or nil if the iterator is empty.

\subsubsection{Debugging}
\subsubsection*{\tt{[log }$string$\tt{]}}
Logs the message from $string$, possibly ignored if debugging is disabled.
\subsubsection*{\tt{[assert }$string$ $val$\tt{]}}
Halt the execution with error message $string$ if $val$ is nil, possibly ignored if debugging is disabled.


\section{Developers guide}
Yolan is a minimal scripting language implemented on Java.
It allows scripting to be added to application, with a minimal overhead on the size of the JAR file -- which is the most limiting factor on low end mobile devices. The features and limitations of Yolan are:
\begin{itemize}
\item A scripting language, with support for higher order functions
\item Support for loading of code at run time
\item Operates directly on standard Java classes, such as \verb|java.lang.Integer|, \verb|java.util.Hashtable| and \verb|java.util.Stack|
\item Runs on Java Micro Edition/J2ME, and requires only CLDC 1.0/MIDP 1.0
\item Adds approximately 5KB to the size of the optimised JAR file. Only single-threaded/non-reentrant, to achive the small size
\item Parses and executes one expression at a time, allowing interactive programming, and implying that the entire program need not to be in memory at once
\item Interpreted - code can be entered directly on the device, not needing an extra step of compilation, and thus it is also suitable for scriptable configuration files, user scripts etc.
\end{itemize}

The implementation of Yolan consists of a single class \verb|Yolan| with the actual implementation, and an interface \verb|Function| which is what a class need to implement in order to be callable from Yolan.
While Yolan only has a single classfile for the implementation, in order to reduce the JAR file size, it consists several logical classes: a parser, a single static runtime, and Yolan instantiated objects are Yolan code that can be evaluated, i.e. delayed computations.

Having a single runtime reduces memory usage, but also limits applications to only execute a single script, and only having a single execution context, at a time. The reduction of memory usage comes from that refererences to the execution context can be hard coded, and thus the delayed computations does not have to carry a reference to the context. There are also a memory reduction due to that less code is needed and the class for the context can be joined into the main class file, as static properties.

The \verb|Function| interface consist of a function that takes an array of Yolan objects -- delayed computations -- as parameter, and the return a value. In this sense Java objects callable from Yolan are essentially lazy functions, and themselves responsible for evaluating their arguments.

\subsection{Getting Started}
The core method of a Yolan object is the \verb|value()| method which evaluates the code the Yolan object represents, and returns the result. This method may throw \verb|Exeception|s as well as \verb|Error|s if the code it represents has faults, so if we are executing user code, or want to be robust against errors in scripts, the Yolan evaluation should be surrounded by a \verb|catch(Throwable)|.

Yolan objects are created with the static \verb|readExpression| method that parses the next Yolan expression from an input stream. So if we want to create a simple interactive interpreter, we can write: \begin{lstlisting}
class Main {
    public static void main(String [] args) throws java.io.IOException {
        Yolan yl = Yolan.readExpression(System.in);
        while(yl != null) {
            try {
                System.out.println("Result: " + yl.value().toString());
            } catch(Throwable yolanError) {
                System.out.println("Error: " + yolanError.toString());
            }
            yl = Yolan.readExpression(System.in);
        }
    }
}\end{lstlisting} 

This code could be saved in a file called Main.java, placed in a directory with Yolan.class and Function.class, and then compiled and executed by executing \verb|javac Main.java| and \verb|java Main|.

Notice that the input stream \verb|System.in| can be replaced with any input stream, so the same basic idea for can be used for evaluating files, programs as strings within the application, or even as streams across the network, where Yolan could work as a shell for remote scripting/controlling an application.

If we want to execute an entire stream, there is a short hand builtin method for doing that: \verb|eval|. For example:
\begin{lstlisting}
class Main {
    public static void main(String [] args) throws java.io.IOException {
        Yolan.eval(new FileInputStream(new File("script.yl")));
    }
}
\end{lstlisting}
This code opens the file "script.yl", and evaluate all the expressions within it. 
\verb|eval| throws away the results of the individual expressions and does not print them,
so the above code is only useful if we have added some user defined functions to Yolan that allows it to do something practical.

\subsubsection{Adding functions to the runtime}
This section describes how to make Java code callable from Yolan.
While the builtin Yolan functions supports basic data structures etc., there is no built in way to do input/output from yolan that is platform dependent: Java Standard Edition supports files, where Java Micro Edition has a record store, and user interfaces ranges between Midlets, Applets, graphical applications, and text standard-in/out.
So when the language needs communicate with the user, or work on the state of the host application, 
some functionality needs to be added, which is most easily done by adding functions to the runtime.

The \verb|Function| interface is the way to do that. To implement the interface a single function \verb|apply|, that takes an array of Yolan objects as parameter, and returns an object is required.
Notice that the parameters are passed lazily, e.g. they are only evaluated when the called function chooses to evaluate them, so we need to call the \verb|value()|-function of the Yolan objects when we want the actual value.
Execution of the Yolan object may also have side effects, so whether, and the number of times, the \verb|value()| is called matters.
In order to add a new Java function to be callable from the runtime, the method \verb|Yolan.addFunction| takes a name as a string and a \verb|Function| as parameters, and binds the name to the function. As an example the following code makes a new function, println, available to the runtime. This function takes one argument, which it prints out to the standard output:
\begin{lstlisting}
class PrintingFunction implements Function {
    Object apply(Yolan args[]) {
        System.out.println(args[0].value());
    }
}
class Main {
    public static void main(String [] args) throws java.io.IOException {
        Yolan.addFunction("println", new PrintingFunction);
        Yolan.eval(new FileInputStream(new File("script.yl")));
    }
}
\end{lstlisting} 
The above program reads and evaluates the file script.yl, with an augmented runtime which also has the println function.

\subsubsection{Values and types}
The builtin types in Yolan are mapped to Java classes for easier interoperability,
so lists are implemented as java.util.Stack, dictionaries are implemented as java.util.Hashtable, strings are implemented as java.lang.String, nil/false are implemented as the value null, integers are implemented as java.lang.Integer, and iterators are implemented as java.util.Enumeration. 
Operations on those data types are just as the native builtin types. 

Any Java object can be passed around within Yolan, so adding support for new data types is just a question of adding functions that work on those data types.

\subsubsection{Functions defined withinin Yolan}
When a user defines a function within Yolan, they are instances of the Yolan class. 
Before calling such a function, the number of arguments can be found using the \verb|nargs| method.
If the Yolan object is not a callable user defined function, the result of \verb|nargs()| is -1, which thus can be used to check if a Yolan object is a callable function.
The function is then applied with the \verb|apply| method, which takes the arguments to the function as arguments, for example:
\begin{lstlisting}
...
    // evaluation some yolan object that yields a function
    Yolan function = yl.value();
    // ensure that it is a function and it takes two arguments
    if(function.nargs() == 2) {
        // apply the function 
        result = function.apply(arg1, arg2);
    } else ...
....
\end{lstlisting}
The apply method is defined from zero, up to three arguments. If there is a need for an apply method with more arguments, they are simple to add, see page~\pageref{source-yolan-apply} for the implementation details. There is also a general apply method, that takes an array of arguments as argument.

\subsubsection{Modifying the runtime}

In order for the scripting language to be practical, it should be able to work and share data with the host application. 
Of course this can be done with functions, and evaluation, as described above, but an additional connection with the language can be added by accessing the variables defined, and used, by the running scripts.
For this there are three functions: \verb|Yolan.resolveVar|, \verb|Yolan.getVar|, and \verb|Yolan.setVar|.

When a value is accessed, this is done through a handle, which is found with \verb|resolveVar|. This handle can then be used for reading and writing the variable. The motivation for the handle is that it takes time to lookup what a variable name, so this computation can be done once for each variable that needs to be accessed, and then additional accesses to the resolved variable are significant faster. The \verb|resolveVar| function takes the variable name as a string parameter, and return the handle, which is an integer. If the variable does not exist in the runtime, space is allocated for it.

With a handle, it is then possible to set the value of a variable with \verb|setVar|. For example setting the variable foo to 42 can be done with:
\begin{lstlisting}
    Yolan.setVar(Yolan.resolveVar("foo"), new Integer(42));
\end{lstlisting} 
and similarly the variable can be read with \verb|getVar|:
\begin{lstlisting}
    Object result = Yolan.getVar(Yolan.resolveVar("foo"));
\end{lstlisting}

If it the variable is commonly accessed, it saves time to cache the handle across calls, as follows:
\begin{lstlisting}
class Class {
    int fooHandle;
    Class() {
        fooHandle = Yolan.resolveVar("foo");
    }

    int someMethod() {
        ... perhaps some scripts modifying foo is executed ...
        Object foo = Yolan.getVar(fooHandle);
        ...
    }

    void otherMethod() {
        ... 
        Yolan.setVar(fooHandle, "A literal value or some variable");
        ...
    }
}
\end{lstlisting}
When defining functions, as described earlier, it is actually the same that is happening: the function is encapsulated in a Yolan object and added to the runtime as with \verb|setVar|.

\subsubsection{Resetting the runtime and saving space}
When the scripting language is only used in some part of the application, it can be pratical to be able to unload its runtime data in order to save memory. 
For this there are two utility functions \verb|Yolan.wipe()| and \verb|Yolan.reset()|.

\verb|Yolan.wipe()| sets all references in the runtime are set to zero, allowing data to be garbage collected.
When the runtime has been wiped, Yolan expressions can no longer be evaluated, and trying to evaluate them will yield errors. 

\verb|Yolan.reset()| resets the runtime: all variable handles are invalidated, all variables are removed from the runtime, and only the builtin functions are added. Existing Yolan expressions are invalid, and evaluation of them may lead to unexpected behavior. User defined functions and variables need to be re-added.
Resetting is necessesary before scripts are executed after a \verb|Yolan.wipe()|.
It can also be practical when multiple scripts are run, one after each other, and they must not mess up the runtime for each other.

\subsection{More on adding functions to the runtime}
The naive approach for adding functions to the runtime would be to create a new class implementing the \verb|Function| interface for each function. This adds significantly to the JAR-file
.
If the code size is critical, this can often be reduced by combining the multiple functions into a single class, for example via a switch dispatch as shown below:
\begin{lstlisting}
class ManyFunction implements Function {
    int id;
    ManyFunction(int id) {
        this.id = id;
    }
    Object apply(Yolan args[]) {
        switch(id) {
            case 0: // first function
                    ....
                break;
            case 1: // second function
                    ....
                break;
            case 2: // third function
                    ....
                break;
            ....
            default:
                throw SomeKindOfError();
    }
    static void register() {
        Yolan.addFunction(new ManyFunction(0), "firstFunction");
        Yolan.addFunction(new ManyFunction(1), "secondFunction");
        Yolan.addFunction(new ManyFunction(2), "thirdFunction");
        ....
    }
}
\end{lstlisting}

When implementing functions, it is also possible to create control structures, due to the laziness of Yolan objects. This is done by not necessarily calling the parameters \verb|value|-functions exactly one time each. The example below shows how the usual if-statement could be implemented:
\begin{lstlisting}
class YolanIf implements Function {
    Object apply(Yolan args[]) {
        // first evaluate the condition
        // and find out if it i true (not null)
        if(arg[0].value() != null) {
            // only evaluate the if the condition yields true
            return arg[1].value();
        } else {
            // only evaluate the if the condition yield false
            return arg[2].value();
        }
    }
}
\end{lstlisting}

\section{Summary}
\chapter{LightScript}
\label{lightscript}
\index{LightScript}
\section{Design choices}
\begin{comment}

The paragraphs below, elaborates on the following design choices:
null-is-false
javascript-like scope

\begin{itemize}
\item LightScript should be based on EcmaScript, and run within EcmaScript interpreters.
\item 
\end{itemize}

LightScript should be based on EcmaScript, such that LightScript scripts runs without modification within an EcmaScript interpreter. 
This will make it possible to use LightScript to write applications that both run on low end mobile devices and also in web browsers, binding those two platforms together.
It also makes it easier for EcmaScript/JavaScript programmers to learn LightScript.
It is not possible to make LightScript EcmaScript compliant, as EcmaScript has requirements that are not possible to fullfill on low end mobile devices.

\subsection{*Design choices from the platform}

As CLDC/1.0 is target, there are no support for floating point numbers in the vm,
and thus all numbers in LightScript will be integers, which opposite to EcmaScript
where the numbers are floating point.
For addition, subtraction, multiplication, and remainder of division, the results are equivalent of floats and integers, if we start out with integers and do not have overflows. The division operator has the issue that it yields different results on integers and floats, so the \verb|/| operator is not implemented. Instead it is possible to make an integer division function \verb|div(a, b)|, both on top EcmaScript and as a Java function exported to LightScript.
Casting to integers in the EcmaScript can be done like:
\verb~function div(a, b) { return (a/b)|0; }~
The limits of the low end mobile devices also encourages choosing the simple and fast solutions.

A design question is also the implementation of truth values. 
One question here is whether to have a distinct Boolean type and undefined type or just let 
\verb|null|, \verb|false| and \verb|undefined| be the same.
The distinction \verb|false| and \verb|undefined| can be practical\cite{luahopl}, as it for example allows on to see if a value in a table has been set to \verb|false|, or just not been set yet.
On the other hand, a unified null value is simpler and faster as truth tests are then just comparision with null, so LightScript only has the value \verb|null| as a false value, and true is just "true". 
This is different from EcmaScript where \verb|false|, \verb|null|, \verb|undefined|, \verb|0|, and \verb|""| are all false, so for compatibility, a requirement should be added, that numbers and strings should not be used directly as boolean values, which is also good coding style. 


\subsection{* Implemented parts of EcmaScript, and strictness}
This first implementation of LightScript will not try to implement as much of EcmaScript as possible, but rather implement enough such that it is useful, and avoid parts of the language that is considered bad style.

EcmaScript it has objects with prototypical inheritance, but instead of a self like clone operator\cite{See section~\ref{survey-self}}, it adds some apparantly Java inspired syntax with a \verb|new| operator, coupled with a special prototype property.
Instead of the Java inspired \verb|new| syntax for instantiation, LightScript just supports objects inheritance the clone operator, which can easily be implemented in pure EcmaScript as well.



\subsection{*Design choices and differences from EcmaScript/JavaScript}


\subsection{*Scoping and stack}

LightScript should have static scope, in a way similar to EcmaScript. 

\end{comment}
\section{Implementation details}
\subsection{Optimised imperative top down operator precedence parser}
The top down operator precedence parser described in section~\ref{tdop} is targeted functional languages.
This section looks at how it can be implemented efficiently in an imperative language.

To reduce the code size of the parser, we assume that the source code is well formed, and do not require that the parser reports syntax errors. This for example removes the need for keeping track of where we are in the code and also allows some tokens to be joined, for example list termination does need not distinguish between different kinds of list.

First class functions use a lot of space in the Java platform, as they require a class each.
The solution is to use a dispatch function instead, and replace the function properties of the token, with integers.
Actually it is simpler with two dispatch functions, one for the null denominator functions and one for the left denominator functions.

The token object contains information about the denominator functions, and corresponding abstract syntax tree node IDs, and also a priority/binding power.
This is just five small integers, which, due to their limited range, easily can be represented in a single 32bit integer. 
Some tokens represents literal values or identifier, where the value or identifier also has to be passed to the parser.
So a token can be represented compactly by adding two properties to the parser: an integer and possible an object for the value, rather than a new class/object.

The token types can be encoded by the token string followed by the five integers.
So the tokens can be written as {\tt "tokenname" + (char) binding\_power + 
(char) null\_deno}\-{\tt{}minator\_function + (char) AST\_ID\_for\_null\_denominator
+ (char) left\_denomina}\-{\tt{}tor}\-{\tt{}\_function + (char) AST\_ID\_for\_left}\-{\tt{}\_denominator}. 
So the task of writing the parser is to make a list of tokens, connected with their binding power, denominator function ids and abstract syntax tree ids, -- a simple parse loop, plus definition of sensible denominator function bodies in the dispatch.
The actual implementation can be seen on page~\pageref{code-lightscript-parser}.


\subsubsection{Performance properties of the parser implementation}

For each token, there is a instruction cost of 1-2 function calls, 1 switch-dispatch, 0-1 comparison, reading of 2-3 properties of the token object, and storing a copy of the token val, plus the cost of building the actual AST node, and the cost of parsing the token.

The size of the implementation can be kept very small, as the denominator functions can be reused across different token-types. For example: the binary operators share a single case in the left denominator dispatch, and adding a new binary operator only takes the length in characters of the operator plus 5 bytes. 

\subsection{Implementation of variables and scope}
In the EcmaScript standard, identifier resolution is done by searching through the scope chain, which is a list of objects. Objects in the EcmaScript context is a mapping from property names to values. This approach to implementation would be very performance expensive.

Instead we want to resolve the variables at compile time, while preserving as much of the semantics as is practical. The main issue here is that we when we resolve the variable at compile time, we do not keep information to be able to resolve dynamically at run time, not supporting access to local variable with \verb|eval| statements, but as mentioned in section~\ref{designstyle}, eval is not availble in the first version of the language (though it could easily be added as an external java function that just calls LightScript).

Of the different implementation methods discussed in section~\ref{survey-scope}, the only real possibilities for a partly imperative scripting language which also want good support for higher order functions are either to allocate the activation records on the heap, or keep track of variables that could live on after exiting a function, and box those variable on the heap.
The other options are ruled out, as we want to have closures, and we want the outer scope variables to be mutable.
Allocating the activation records on the heap has the issue, that we are not in control of the garbage collector, and it may not be designed for that kind of usage, leading to expensive performance, so the approach will be only to box variables onto the heap, that could be alive after function exit.

To simplify this, it is done such that every variable that is added to a closure of an inner function is boxed on the heap, and other local variables are just stack allocated in the usual way. 
I have not seen this exact approach to implement closures other places, but it seems so obvious, that it is probably done somewhere before, although it clearly is different from the Lua approach with upvals\cite{luasrc}, and also from the approach following the EcmaScript standard\cite{ecmascript} directly.

For the practical implementation of the stack on top of the jvm, there are two obvious possibilities: A \verb|java.util.Stack| could be used, or a stack could be implemented manually with an array and an index pointer. 
In order to select implementation strategy, a microbenchmark was done on the kvm, which indicated that the array approach is significantly faster than the \verb|java.util.Stack|. The array grows dynamically when entering a function which uses more stack spaces than is available. 
The code footprint size is similar for both approachs, though probably a bit smaller for the Stack, as that one automatically grows, unlike then the array approach, which need a manual implementation.
The actual access to a \verb|Stack| requires a method calls, which are 3 bytes plus 1-3 bytes for self and parameter loading, where reading from an array is a single byte opcode plus 2-4 bytes for self, index and parameter loading, and at the same time the array approach sometimes need to adjust the index, costing 3 bytes.

\begin{comment}

\subsection{Virtual Machine}
The virtual machine is stack-based in order to reduce the footprint of the compiler.
It is implemented with a single larger switch statement as it is running on top of JVM, which has no support for references to labels, etc.
The instruction set is inspired by the JVM and calling conventions on i386, and is also product of the iterative development, such that new instructions are added, as they are needed by the compiler.

\subsection{Compiler}
The core part of the compiler is a function that compiles a single node in the syntax tree. This function also takes a parameter 




\section{Language specification}
The language is base on the EcmaScript specification \cite{ecma232}, and scripts written in LightScript runs unaltered in any EcmaScript compliant interpreter, with a couple of extra functions defined within EcmaScript.
EcmaScript scripts may or may not run within LightScript, as it is only a subset in order to be implemented on low-end devices.


\subsection{Object system and inheritance}
Objects can be seen as a mappings from keys to values.
When iterating through the keys in an object, the order of visiting the keys is unspecified, unlike EcmaScript, where they visited in order they were added to the object.

Inheritance is implemented via prototypes.

\end{comment}

\section{Version numbering, and future  direction}
This specification describes LightScript version 1.0.417.
The version number consist of a major version number, a minor version number and a revision number.
The revision number correspond to the svn version, and is incremented on each commit.

The minor version number is incremented with larger milestones and expansions of language/added functionality. 
The major version number is incremented on major rewrites or new implementations of the language, and may break backwards compatibility.
Version 1.0 is the one used for the benchmarks and throughout this report. 
Version 1.2 will add more functions and operators, but will not make major changes to the semantics.

TODO: further direction, and move this section down.

\section{Language specification}
The language is mostly a subset of EcmaScript, and the description of the different parts of the languages is written in the same sequence as the EcmaScript standard, to make it easier to compare the two. The focus will on where LightScript differs from EcmaScript.

The specification is stricter than the actual implementation, for example: the implementation does not distinguish between statements and expressions, and allows a statement everywhere an expression could be written, whereas the specification follows EcmaScript, and distinguishes the two.

Only a subset relatively small subset of EcmaScript is implemented: operators has been added as needed, meaning that rare operators has not yet been added. 
On the other hand, more interesting language aspects, such as exceptions and prototypical inheritance, have been implemented and tested, though they may not have been needed for the example programs or benchmarks.

While the specification of LightScript is mostly the implemented EcmaScript subset, there are also som choices taken to make it more practical to also implement it on embedded systems.


\subsection{Lexical conventions}
The lexical conventions are slightly different from EcmaScript. 
Whitespaces are space, tab, carriage return and line feed. 
LightScript does not distinguish between whitespace and line terminators, and do not have automatic semicolon insertion.

Comments are started with two consecutive slashes \verb|//| and runs to the next newline character.

Keywords reserved by EcmaScript are also reserved by LightScript. The keywords currently used by LightScript are: {\tt catch do else for function if return this throw try var while}. This will be expanded in the next version of LightScript.

Identifiers start with a letter or an underscore, and continues with any combination of letters, underscores and digits. 
LightScript does not support escaped unicode sequences, but do accept utf-8 encoded. Non-letter unicode symbols with value larger than 127 are not supported. This allows a parsers to be implemented easier as they can just treat any 8-bit character with a value larger than 127 as a part of a unicode letter.

The punctuators in the current version of LightScript are: {\tt \verb|{| \verb|}| ( ) [ ] . ; , < > <= >= === !== + - * \% >> ! \&\& || ? : = += -=}. 
This will expand in the next version of LightScript where the remaining operators will be added, except for division as discussed on page~\ref{division}.

String literals are always enclosed in double qoutation \verb|"|, and single quotation is not supported. It is possible to use backslash \verb|\| to escape quotation marks, backslashes and newline (\verb|"\n"| is the string containing a newline).

Integer literals are written as a sequence of digits, not starting with a zero, unless the number is zero. Only base 10 literals are supported.

Other literals are \verb|true| which translates to a value that has a true truth value, and \verb|false|, \verb|null| and \verb|undefined| which translates to a value that has a false truth value.

\subsection{Types}
\subsection{Expressions}
... division... \label{division}
... ==, !=...
\subsection{Statements}
\subsection{Function definitions}
\subsection{Native objects and functions}

\begin{comment}
\subsection{Control flow}
For control flow LightScript supports \verb|while(..) ..|, \verb|do .. while(..)|, \verb|if(..) ..|, \verb|if(..) .. else ..| and \verb|.. ? .. : ..|, similarly to EcmaScript.
% []subscript []array . {}hashtable {}block >> * %  ()function-call ()paren + - === !==  <= < >= > && || else in ?: = += -= var return ! ++ -- throw try catch function do-while while for if this undefined/null/false true "string" 123number
\subsection{Exceptions}
LightScript objects can be thrown and caught as an exception.
Exceptions a thrown/raised using the \verb|throw| keyword, followed by the object that should be thrown.

This will break the control flow till it reaches an enclosing \verb|try {| $\cdots$ \verb| } catch(| $obj$ \verb|) {| $\cdots$ \verb|}|, possibly across function calls.

\subsection{Functions}

\subsection{Objects}
Objects can be initialised using the EcmaScript object notation, for example:
\begin{lstlisting}
obj = {"name": "Hello world", "answer": 42};
\end{lstlisting}
The elemen
\subsection{Arrays}
\subsection{Strings}
\subsection{Numbers}
\subsection{Truth values}

% []subscript []array . {}hashtable {}block >> * %  ()function-call ()paren + - === !==  <= < >= > && || else in ?: = += -= var return ! ++ -- throw try catch function do-while while for if this undefined/null/false true "string" 123number
% print gettype parseint clone Array.push Array.pop Array.join Object.hasOwnProperty 

\subsection{Methods and functions}

\end{comment}





\section{Developers guide}
To evaluate code with LightScript, you first have to instantiate a LightScript object, which keeps track of global values, loaded libraries, and other stuff. The constructor takes no parameters, so creating it is just:
\begin{lstlisting}
    LightScript lsContext = new LightScript();
\end{lstlisting}
This context can then be used to evaluate LightScript code, using the \verb|eval| method. This method either takes a string or an \verb|java.io.InputStream| as parameter, which is then read and executed:
\begin{lstlisting}
    lsContext.eval("print(\"Hello world \" + 17 * 42)");
    lsContext.eval(new FileInputStream(new File("myscript.js")));
\end{lstlisting}
Global variables of the context can be read and written with the \verb|get| and \verb|set| method, so for example:
\begin{lstlisting}
    lsContext.set("foo", new Integer(17));
    lsContext.eval("bar = foo + 25;");
    System.out.println(lsContext.get("bar");
\end{lstlisting}
would print \verb|42|.

\subsection{Adding native functions to the runtime}

A method on a Java object can be called from LightScript if it implements the \verb|LightScriptFunction| interface, which defines an \verb|apply| method.
The interface is:
\lstinputlisting{../code/LightScript/LightScriptFunction.java}
So for example a function that returns the current number of milliseconds could be implemented as:
\begin{lstlisting}
class MillisecondsFunction implements LightScriptFunction {
    public Object apply(Object thisPtr, Object[] args, int argpos, 
                        int argcount) throws LightScriptException {
        return new Integer((int)System.currentTimeMillis());
    }
}
\end{lstlisting}

Adding a function to the runtime, is just like adding any other variable,
via using the \verb|put| method of the LightScript object.
So using the function above measuring some timings in LightScript can be done like:
\begin{lstlisting}
    lsContext.set("timer",new MillisecondsFunction());
    lsContext.eval("begin = timer();"
                  +"for(i=0;i<1000000;++i);"
                  +"print(\"Time used: \" + (timer() - begin));");
\end{lstlisting}

When registrering several functions, it is more compact to join them via a dispatch, 
so a class implementing a couple of functions could be implemented like:
\begin{lstlisting}
class FunctionLibrary implements LightScriptFunction {
    int id; // This tells which function the object represents
    public Object apply(Object thisPtr, Object[] args, int argpos, 
                        int argcount) throws LightScriptException {
        switch(id) {
            case 0: // integer division
                return new Integer(((Integer)args[argpos]).intValue()
                                  /((Integer)args[argpos+1]).intValue());
            case 1: // increment property i, not of superclass
                int i = ((Integer)((Hashtable)thisPtr).get("i")).intValue();
                ((Hashtable)thisPtr).put("i", new Integer(i + 1));
        }
        return null;
    }
    private FunctionLibrary(int id) { this.id = id; }
    public static void register(LightScript lsContext) {
        lsContext.set("div", new FunctionLibrary(0));
        lsContext.set("propinc", new FunctionLibrary(1));
    }
}
\end{lstlisting}
which could be used like:
\begin{lstlisting}
    FunctionLibrary.register(lsContext);
    lsContext.eval("obj = {}; obj.i = 1; obj.inc = propinc;"
                  +"while(obj.i < 10) {"
                  +"  print(div(42, obj.i));"
                  +"  obj.inc();"
                  +"}");
\end{lstlisting}

\subsection{Datatypes}
LightScript uses ordinary Java objects for most data.
Strings, stacks, tables, are as usual Java.
Boolean values are the constants \verb|LightScript.TRUE| and \verb|LightScript.FALSE|, which are not Java Booleans, but the string "true" and null.

LightScript objects with inheritance are instances of the \verb|LightScriptObject| class, which is subclass of \verb|java.lang.Hashtable|. LightScript objects have a constructor that corresponds to clone in Self, se section~\ref{survey-self}, and the parameter to the constructor is a hashtable or a LightScript object. The LightScript objects also overloads the hashtable get operator to access the cloned object, if the key was not found in this one.

Exceptions that can be thrown to/from LightScript are of the class \verb|LightScriptException|. This exception has a property \verb|value| that is the object that is thrown/caught within LightScript. The constructor just take the value as parameter.

\section{Summary}
\chapter{Benchmarks}
\label{benchmark}

\section{Code footprint}
This section looks at the code footprint: first footprint estimates are compared for different scripting languages, then details on the footprint of LightScript is investigated, and finally the actual size of the library embedded in a minimal application is found.

\subsection{Comparison with other languages}
To compare the code footprint size across languages, I have added an estimate of the jar file size. 
The estimate is done by making a zip archive of the non-obscurified class files,
trying to exclude class files that are part of extra libraries.
The motivation for this approach is to make the comparison more fair, as some of the languages have large GUI libraries,
which would count against them, if we just looked at the jar file. 
In addition, compilation of some of the languages is obscurified/optimised by default,
which would give those an advantage. So this should give a more fair comparison.

\subsubsection{Languages}
\label{codefootprint-languages}
The code size benchmark looks at the languages implemented in this project, Yolan and LightScript, it looks at scripting languages for mobile devices, FScriptMe, Kahlua, Hecl, Simkin, and CellularBasic, and it looks at some general scripting language implementations,  JScheme, and Rhino.

FScriptME\cite{fscriptme} is the mobile edition of femto-script, which ``is an extremely simple scripting language''\cite{fscript}. 
Out of the box, it only supports strings and integers as datatypes -- no compound types -- which limits use somewhat, and it is still in beta, since 2002. 

JScheme\cite{jscheme} is a small Scheme implementation. An early version is benchmarked, as later versions have vastly larger code footprint, and would thus perform badly on that point. It depends on some reflection, and does therefor not run on Java Mobile Edition, but is included as the implementation is compact, and may be changed to target mobile devices.

Kahlua\cite{kahlua} is an implementation of the Lua Virtual Machine for Java Mobile Edition. 
The implementation requires CLDC-1.1 due to the use of floating point arithmetics, and does therefor not run on the lowest end mobile devices, which only supports CLDC-1.0.
Unlike the other languages, kahlua is not a full language interpreter, but only a virtual machine, so the the script cannot executed directly on the device, but needs to be compiled on another computer before being executed.

Hecl\cite{hecl} seems to be \emph{the} major scripting language for mobile devices, or at least the one keep popping up in the top of most queries when searching for scripting languages for mobile devices.
It is very portable, with different editions running on CLDC-1.0, CLDC-1.1, Android, as applets and as usual applications.
It has lots of libraries targeting mobile devices, which in this comparison were removed from the zip file of class files, not to give languages with fewer libraries an advantage in the size comparison.
The language is a dialect of Tcl, and is simplified such that arithmetic operators, and the like, are prefix operators so expressions ends up a bit lisp-like.

Simkin\cite{simkin} is a scripting language for being embedded in XML. The depends on kxml xml library, which is not included in the measured size. 

CellularBasic\cite{cellularbasic} is a dialect of Qbasic, implemented for mobile devices. 
It includes a floating point support library which is not included in the measured size. 

Rhino\cite{rhino} is a JavaScript implementation. This language is not designed for, nor does it run on, mobile devices. It is included as an example of an implementation of a usual non-mobile scripting language.

\subsubsection{Results}
The approximated JAR sizes of the scripting languages are:
\begin{center}
\begin{tabular}{|c|r|} \hline 
Yolan & 7K \\ \hline 
LightScript & 14K \\ \hline 
\end{tabular}
\begin{tabular}{|c|r|} \hline 
FScriptME & 17K \\ \hline 
Jscheme & 29K \\ \hline 
Kahlua & 39K \\ \hline 
Hecl & 54K \\ \hline 
Simkin & 81K \\ \hline 
CellularBasic & 83K \\ \hline 
Rhino & 397K \\ \hline 
\end{tabular}
\end{center}

The languages developed in this project smaller than other scripting languages for the platform. 
Yolan is approximately half the size of LightScript. 

\subsection{Details on the footprint of LightScript}
The sizes of the different part of the LightScript class is listed below.
Reduction is the reduction of the full class file when the mentioned part is left out. Alone is the size of the class file with everything else than the mentioned part left out. These numbers are different as some things are shared and some things cannot be left out when compiling the class file.

\begin{center} \begin{tabular}{|r|r|rl|} \hline
\multicolumn{2}{|r|}{Reduction} & \multicolumn{2}{|l|}{Alone}\\ \hline
Everything & 15030 & 17706 & \\ \hline
API & 645  & 3597  & \\ \hline
Tokeniser & 1261 & 4048 & \\ \hline
Parser & 2542 & 5850 & \\ \hline
Compiler & 5324 & 8413 & \\ \hline
Vm & 4589 & 7650 & \\ \hline
Parser+Tokeniser & 3825 & 7093 & \\ \hline
\end{tabular} \end{center}

\subsection{Optimised JAR-file footprint}
Previous sections have looked at comparable footprints for implementations by approximated JAR file size, and also at what parts contributes to the size of the LightScript class.
From a practical view it is also interesting to see the actual size of the optimised obscurified JAR-file of embedding the languages in a trivial host application.
The host application is a simple Midlet that adds a print function to the language, and includes a hello-world program.
The size of the entire application, including the embedded scripting language implementation is 5290 bytes for Yolan and 11279 bytes for LightScript. 

\section{Execution speed}
This section benchmarks the execution speed of the languages. 
In the following subsections, the languages that are benchmarked, and the actual benchmark programs are described.
The source code for the benchmarks can be seen in appendix~\ref{benchmarksource}.
The results of the benchmarks can be seen in figure~\ref{figure-execution-speed}.

\subsection{Languages}
The benchmarks are run on those of the languages from section~\ref{codefootprint-languages}, that have an approximated jar size of less than 64K. They also run on two JavaScript interpreters: Rhino 1.6r7 and SpiderMonkey 1.7.0, which are described in section~\ref{spidermonkey}, and are the default versions when installed on Ubuntu Linux.

\subsection{The benchmarks}
The benchmarks are the following:

\paragraph{Fibonacci:} Recursive calculation of the 30'th Fibonacci number
\paragraph{Loops:} Nested loops with counters, 10.000.000 iterations
\paragraph{Recursion:} Highly recursive benchmark, similar to recursive control-flow benchmark from \cite{sunspider, shootout}. Uses lots of stack space. On some of the languages where it fails, only the first part of it was implemented.
\paragraph{Sieve:} Simple implementation of Erasthones sieve - not implemented in languages which have already shown to be very slow in earlier benchmarks
\paragraph{For-in:} Nested loops across keys of a dictionary, 1.000.000 iterations - not implemented in languages which have already shown to be very slow in earlier benchmarks
\paragraph{Primes:} Simple primality test by looking at the remainders of division - not implemented in languages which have already shown to be very slow in earlier benchmarks
\paragraph{Exception:} Throw/catch 500.000 exceptions - only implemented for LightScript/JavaScript
\paragraph{Fannkuch:} Access-fannkuch benchmark from \cite{sunspider, shootout} - only implementd for LightScript/JavaScript

\subsection{Results}
The measurement of the performance of the different scripting languages are shown below. The timings are seconds per benchmark. $\bot$ indicates that the benchmark does not complete due to running out of stack space.

\begin{center} \begin{tabular}{|r|r|r|r|r|r|r|r|rr|} \hline 
& \multicolumn{2}{|l|}{Fibonacci} & \multicolumn{2}{|l|}{Recursion} & \multicolumn{2}{|l|}{For-in} & \multicolumn{2}{|l}{Exceptions} & \\
& & \multicolumn{2}{|l|}{Loops} & \multicolumn{2}{|l|}{Sieve} & \multicolumn{2}{|l|}{Primes} & \multicolumn{2}{|l|}{Fannkuch} \\
\hline Rhino       & 1.20 & 1.74 & 1.75   & 2.97 & 1.18 & 12.35 & 45.99 & 6.35 & \\ 
\hline SpiderMonkey& 1.28 & 1.71 & $\bot$ & 2.08 & 1.14 & 11.03 & 0.45  & 5.10 & \\ 
\hline LightScript & 1.37 & 3.45 & 2.35   & 1.19 & 0.57 & 11.70 & 0.65  & 11.15 & \\
\hline Yolan       & 1.47 & 2.23 & $\bot$ & 1.95 & 0.32 &  9.20 &  &  & \\
\hline Kahlua      & 2.13 & 1.18 & $\bot$ &  5.73 & 2.26 & 5.49 &  &  & \\ 
\hline JScheme    & 29.77 & 93.22 & $\bot$ & & & & & & \\ 
\hline FscriptME & 176.27 & 112.68& $\bot$ & & & & & & \\ 
\hline Hecl      & 207.96 & 47.21 & $\bot$ & & & & & & \\ 
\hline \end{tabular}
\end{center} 

\section{Summary}
\chapter{Discussion}
\label{discussion}
\begin{comment}
  - surprisingly good performance
    - perspective, discussion of the results
      - further studies

\section{Further directions}
\subsection{How to make LightScript more EcmaScript-like}
\subsection{A LightScript validator and prettyprinter}
...{* Further directions}
\section{* Design of runtime on top of C}
\subsection{* Unique strings}
\subsection{* Compact memory representation}
\subsection{* Garbage collection}
\section{* How to make LightScript more EcmaScript-like}
\section{* A few-register virtual machine for small code size, easy JIT, and fast execution}
\section{* A LightScript validator and prettyprinter}
\end{comment}
\chapter{Conclusion}
\label{conclusion}


%\chapter{Introduction to scripting \\ -- a user guide}
%\input{language/userguide}
%

\newpage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography}
\bibliographystyle{alpha}

\appendix

\input{../code/bench/marks.tex}

\chapter{Source code of the Yolan class}
\lstinputlisting{../code/Yolan/src/Yolan.java}

\chapter{LightScript}
\section{The LightScript class}
\lstinputlisting{../code/LightScript/LightScript.javapp}
\section{Standard library}
\lstinputlisting{../code/LightScript/LightScriptStdLib.java}
\section{Object class}
\lstinputlisting{../code/LightScript/LightScriptObject.java}


\newpage
\addcontentsline{toc}{chapter}{Index}
\printindex

\end{document}

