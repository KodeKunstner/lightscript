\documentclass[11pt]{report} 
\usepackage{a4} 
%\usepackage[paperheight=220mm,paperwidth=170mm]{geometry}
%\usepackage[paperheight=150mm,paperwidth=200mm]{geometry}
\usepackage{geometry}
\usepackage{makeidx}
%\usepackage[danish]{babel} 
\usepackage[utf8]{inputenc}
\usepackage{textcomp}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx} 
\usepackage{verbatim} 
\usepackage{fancyhdr}
\usepackage{listings} 
%\usepackage[colorlinks,pagebackref]{hyperref}
\usepackage[colorlinks, linkcolor=black, anchorcolor=black, citecolor=black, menucolor=black, pagecolor=black, urlcolor=black]{hyperref}
%\usepackage{backref}
\usepackage{url}
\frenchspacing
\makeindex
\pagestyle{plain}
\lstset{language=java}
\lstset{escapeinside=`'}
\lstset{basicstyle=\scriptsize}

\title{
\emph{Document currently under development} \\ ~ \\
Design and implementation of \\
compact scripting languages \\ 
for low-end mobile devices \\
{\scriptsize version \input{version}}}

\author{
  Rasmus Erik Voel Jensen\footnote{
    sumsar@solsort.dk
  }
} 

% Remember 
%    \index terms
%    update template as well as document when structural updates.
\date{2009}

\begin{document}

\maketitle
\begin{abstract}
This thesis creates two new languages: LightScript and Yolan, which run on top of Java Micro Edition and enable scripting on very low-end mobile devices.

The code footprint is the major limitation on low end mobile devices, and both languages have a significantly smaller code footprint than existing scripting languages. 
The languages are comparable in speed to larger scripting language implementations,
and an order of magnitude faster than most of the benchmarked scripting languages for mobile devices.

Both scripting languages have first class functions, dynamic typing, built-in support for hashtables, stacks, etc., and support interactive programming. They are also able to load and execute scripts presented in source form at run-time. 
The Java Micro Edition does not support dynamic loading of code, so both languages are interpreted.

LightScript is a subset of JavaScript; it is static scoped, and also includes support for objects and exceptions. It is compiled to, and executed on, a stack-based virtual machine. One of things that makes the compact implementation possible is that the LightScript parser is an imperative optimised version of the top-down operator precedence parser.

Yolan is highly optimised for reducing the size of the implementation code footprint. It has dynamic scope, a Lisp-inspired syntax, and is interpreted by evaluating each node of the syntax tree.

Yolan has a code footprint less than half the size of LightScript, and they are similar in speed, even though they have very different evaluation strategies.
\end{abstract}

\setcounter{tocdepth}{2}
\tableofcontents

\chapter{Introduction}

    The topic of the project is to design and implement a scripting language
that runs on very low-end mobile devices. This is both to create a practical tool, and
also a focus for exploration of programming language theory. 

\begin{comment}
The motivation is that a scripting language makes it is easier to make applications for mobile 
devices, and that existing freely available scripting languages
are very limited, slow, or simply does not run on the low-end mobile devices.
\end{comment}

    The educational goals are to learn about programming language design and
implementation, and to learn about programming on mobile devices. Through the
project, I should be able to evaluate and choose programming language features
and implementation techniques, and design and implement a scripting language.

    The focus of the language is that it should be portable, embeddable and have
a low memory footprint. Portable implies that it should run on different devices,
from very low-end mobile phones to high-end computers, possibly also within a web browser. 
Embeddable implies that it should be easy to include within and interface with
other applications. Low memory footprint implies that it should be suitable for
running on platforms where the available memory is measured in kilobytes rather
than megabytes. 

    The approach will be pragmatic and favor simplicity.

\section{Motivation}

Scripting languages make it easier to write applications\cite{scripting-ousterhout}. Beside higher productivity, they also opens up for user based scripting and scriptable configurations.
On more powerful devices, ranging from high-end smartphones to personal computers, there are very good scripting languages available.
Scripting language implementations usually take a lot of resources, which is a problem on low-end mobile devices. On those devices, implementations may not be available, may be very slow, or have limitations, such as they cannot be executed directly, but need to be compiled on another machine, or they do not have basic data types.
The focus on a better implementation of scripting languages for mobile devices is thus a niche, where the result may actually be of practical use.

The focus on low-end devices also has another benefit:
it broadens the number of devices on which the language may run.
While very low-end devices are becoming uncommon in Denmark,
they still live on in countries with less information technology penetration.
Thus,  by targeting very low-end devices, 
this project may make scripting, and thus easier content creation,
more available,
and thus could be the beginning of a stepping stone 
towards more information and computing literacy.
The restrictions of low end mobile devices also imposes challenges, that may lead to interesting solutions.

\begin{comment}
From a personal point of view, 
I would like to get started on development for mobile devices, 
and would also like to brush up on programming language implementation.
Design and implementation of a scripting language for mobile devices is spot on this topic.
\end{comment}


\section{The structure and content of the report}

The report starts out with some background information: this introduction, and a survey on topics related to the project in Chapter~\ref{survey}.
Then,  the methodology and design for the project are elaborated in Chapter~\ref{method}, including some of the principal design choices for the language implementations.
The next couple of chapters then describe the actual results: the LightScript language in Chapter~\ref{lightscript}, the Yolan language in Chapter~\ref{yolan}, and some benchmarking of the two new languages compared to existing scripting languages in Chapter~\ref{benchmark}.
Finally there is a discussion of the results in Chapter~\ref{discussion}, rounding off with a conclusion in Chapter~\ref{conclusion}

Some of the developed source code has been attached, such that the implementation details described in Chapter~\ref{lightscript},~\ref{yolan} and~\ref{benchmark}, can also be seen in practice.
There is also an index to make it easier to find specific parts of the report.

Each chapter contains a short introduction and summary, to makes the chapter easier to skim, and get an overview of the report.

\chapter{Survey}
\label{survey}

\section{Mobile platforms}

Mobile devices ranges from low end phones, which, if they are programmable at all, only support limited Java Micro Edition midlets, up to advanced smartphones with performance resource similar to older desktop computers.

When looking mobile devices, it should be noticed some survey \cite{smartphonesurvey} only focus smartphones, and thus the market shares does not include the more low end devices.

Most low end devices support some kind of mobile Java.
Here there are different APIs and device profiles, but the basic execution and deployment model is the same for those.

Low end devices rarely support loading of native code, and higher end devices may require different kind of code signing to allow native programs to be distributed.

Besides Java and native code, JavaScript is becoming a potential language for applications for high end phone. 
This is both due to its integration with the web, which means that it will be available on the phones with advanced browsers, due to the increased amount of memory on high end phones
and due to recent major performance advances within the JavaScript implementations, which are propagating towards mobile devices.

\subsection{Embedded systems}
The personal computers, PCs, as we know them, are only a very small fraction of the computers in use.
Billions of electronic devices nowadays have small computers embeddes. They are in mobile phones, kitchen equipment, washing machines, music instruments, car, even advanced greeting cards. 
These embedded computer systems, vary tremendously, from low end micro controllers with memory measured in bytes, up to powerful CPUs with vector processing units and many megabytes of memory.

\subsubsection{Machine architecture}
There are two significantly different classes of computers: The Harvard architecture, and the von Neumann architecture. The difference is whether the program code is in the same memory as the data or if they are separated.

Larger computers usually use the von Neumann architecture where program and data is in the same memory, and this is the most commonly known model.
On small micro controllers on the other hand, the data memory is usually separated from the program memory, which is what is called the Harvard architecture. 
These two approaches to architecture dates back to some of the early computers: the Harvard Mark I \cite{harvard-mark1} and the EDVAC \cite{edvac} by John von Neumann.

On small micro controllers, programs are usually stored in flashable ROM, and on the Harvard architecture the separation of data and code makes it easier and cheaper to implement, for example size of the code words can be independent of the size of the data words.
These may have less than a kilobyte of RAM, and some kilobytes of possibly flashable ROM for the executable code. Examples of this are the PIC-processors \cite{picspec}, or the ATmega 8-bit microcontrollers \cite{atmegaspec}.
An interesting operating system for low end devices is the Contiki \cite{contiki}, which also has some details on very lightweight implementation of threads.

On more powerful embedded devices, for example mobile phones, and larger computers,
it may sometimes be practical to be able to work with the code as data, and thus von Neumann architecture makes more sense there.
For mobile and larger embedded devices a typical CPU architecture is the ARM \cite{arm-architecture}, which is a 32 bit RISC architecture.

\subsubsection{Development platform}

While embedded systems are very very common, the software is a shipped, or possibly flashable updateable on larger system, but there are generally no support for software development.
So usually access to debug boards and expensive hardware is needed to get experiment with embedded devices.
Another approach is that some devices is hackable in the sense that somebody has found out how to upload and customise the firmware, usually without documentation and support from the manufacturer -- examples here ranges from digital cameras, mobile phones and handheld gaming devices.
An exceptional case on small embedded devices, where the manufacturer has actually opened up for development is the Lego brick that is a part of the Lego Mindstorm NXT, which will be elaborated in the next section.
On larger devices it happens more often, though it is still rare: two smartphones \cite{openmoko, htc-android}, an E-book-reader, and some wireless routers and network attached storage \cite{wrtg, nslu, buffalo} have also opened, or partly opened, up for development.

The Lego Mindstorm NXT is interesting because it is an easy to get started with embedded development platform. 
The main cpu is an ARM7TDMI \cite{arm7tdmi}, which is commonly used in mobile phones, and also in many other embedded devices. 
It is clocked at 48MHz, and is attached to 256 KB of flashable ROM and 64 KB of RAM, which means that it is low end, compared to mobile phones, while powerful viewed as an embedded device.
There is also an ATmega48 \cite{atmega48} 8-bit coprocessor, which has 4KB of flashable ROM and 512 bytes of RAM, which is a typical microcontroller. 
The coprocessor usually reads sensors, controls motors, and can power the main cpu down, to reduce power usage.
The system also have a 100x64 black/white lcd display, and a couple of buttons, so it has a lot of potential as a development substitute platform for low-end mobile devices, which do not allow 3rd party firmware.

\subsection{The J2ME / Java Micro Edition}
J2ME (Java Micro Edition\footnote{Java Micro Edition is a recent rebranding of J2ME}) is the most common platform for mobile application, supported by more than a billion devices \cite{sun-j2me}. 
It seems like two thirds of mobile phones shipped today supports Java \cite{esmertec-globenews}.
%http://www.globenewswire.com/newsroom/news.html?d=149713
Most mobile devices either require strict code signing, or do not allow native applications to be loaded at all, which leads to that J2ME is often the only for mobile application development.
J2ME is a trimmed down Java Virtual Machine (JVM) so it has most of the features and limitations of a standard Java JVM, with some notable exceptions, such as the lack of dynamic class loading and reflection. It is fragmented platform which makes it more difficult to make applications that works across devices: there are different APIs/extensions from different vendors, and different device profiles for different hardware capabilities.

Applications for J2ME, are called Midlets and are distributed via JAR-files (Java ARchive). A JAR file is essentially a zip file containing the compiled Java classes, data files, and some meta information.

J2ME has two device configurations CLDC 1.0 (Compact Limited Device Configuration 1.0) \cite{cldc10}  and CLDC 1.1  \cite{cldc11}. The major difference between the two is that CLDC 1.0 is integer only, whereas CLDC 1.1 supports floating point numbers.
It seems like approximately $\frac{1}{6}$ of the mobile phones that supports J2ME is limited to CLDC 1.0, whereas CLDC 1.1 is supported by the remaining $\frac{5}{6}$ of the phones \cite{mobref}.
% http://stats.getjar.com/statistics/world/gJavaCLDCVer
A limitation of both CLDCs is that they lack of support for reflection and dynamic code loading.
The lack of a reflection API impacts a scripting language in that it cannot discover native functionsn Instead such functions have to be added explicitly to the system.
The lack of run-time class loading, means that the only way to execute run-time loaded scripts is through intepretation. JIT compiling to JVM or native code is not possible.

The CLDCs are based upon the Java JVM \cite{jvmref}, with some instructions removed, and some metadata added to ensure stack discipline. 
In order to simplify the J2ME JVM implementation, Java class files has to be preverified, before they can be loaded.
The preverification add meta data about stack use, and removes certain instructions, such as local jump-and-save-register, to make it easier to implement a JVM, that is safe against malicious code trying to overflow the stack.
The reference implementation of CLDC 1.0, kvm, is a switch-based interpreter with a compacting mark-and-sweep garbage collector \cite{kvm}.


The JVM limits interpreter implementation: it does not support label references as values, nor does it support functions as values. This makes some of the optimisations discussed in 
Section~\ref{interpreter-implementation} impossible.
Instead it does have a builtin switch opcode as well as support for method-dispatch based on the type of an object, so an interpreter would either be switch-based, or have a class for every opcode.

The resources available for J2ME applications on mobile phones start at 64KB for the size of the JAR-file and 200KB for the run time memory on the lowest end devices, and goes upwards from there \cite{nokia-optim}.
These numbers are for the full application, so the resources available for an embeddable scripting language, could be significantly less than this, depending on the resource usage of the host application.

\subsubsection{Reducing the memory usage of J2ME applications}

Some devices only have little memory available,
and an optimisation here is to be able to avoid having memory intensive parts of the program run at the same time. For example, with a scripting language, it is desirable to be able to garbage collect the executed parts of long script when they are done, so the memory becomes available for other computations.
The usual size optimisations, such as finding compact representations, trimming dynamic data structures, avoiding sparse data, etc., are also applicable.
Here may be a tradeoff between the code footprint and run time memory, as compact representations and other optimisations may require more code to be implemented.

\subsubsection{Reducing the footprint of J2ME applications}
Some optimisations to reduce the code or JAR-file footprint:
\begin{itemize}
\item Reduce the number of class-files.
\item Write initialisation manually, where the automatic generated initialisation is inefficient.
\item Use a JAR-optimiser/obscurifier.
\item Put the classes in the unnamed package.
\end{itemize}

Another optimisation is to reduce the number of classes \cite{nokia-optim, kahlua-thesis}.
This may reduce the size of the JAR file significantly,
even though it does not change the amount of code:
Each class file has its own symbol table, which means that if classes are merged, then common symbols only needs to be represented once, rather than once for each class.
Furthermore JAR files are essentially zip-archives, and each file in a zip archive is compressed individually \cite{zipspec}, which means that small files typically get compressed less than larger files, due to the small compression context.
The downside of reducing the number of classes is that it could go against the object oriented design, and the implementation may more difficult to read and edit.

Initial values are not supported directly by the Java class file format, but instead
JVM-code is generated, which does the initialisation. 
This often inefficient. For example the initialisation:
\begin{verbatim}
byte[] bytes = { 1, 4, 3, 4, 5, 6, 2, 3, 1 } ;
\end{verbatim}
generates the code corresponding to
\begin{verbatim}
byte[] bytes = new byte[9];
bytes[0] = 1; bytes[1] = 4; bytes[2] = 3;
bytes[3] = 4; bytes[4] = 5; bytes[5] = 6;
bytes[6] = 2; bytes[7] = 3; bytes[8] = 1;
\end{verbatim}
which for larger initialisation is significantly more expensive than manually written initialisers,
which for the above example could be:
\begin{verbatim}
byte[] bytes = "\001\u004\u003\u004\u005\u006\u002\u003\u001".getBytes();
\end{verbatim}

Another thing to be aware of is, that strings in Java class files are encoded, such that only characters with unicode values between 1 and 127 (inclusive) uses one byte per character, so for large binary data, it may make sense to load it from external sources and not include it in the class file.

JAR file optimisation/obfuscation can beneficial for code footprint because it may:
remove unused code,
rename methods and classes, such that they use less space in the symbol table,
make \verb|static const|s work as \verb|#define|s,
 and optimise the code, thus shortening it.
This also allows one to make more readable code with less concern of code footprint, knowing that some of the more verbose parts will be optimised away.

Using the unnamed package also saves space in the symbol table, as class references becomes shorter.

The code footprint limit may be tighter that the run time memory limit, and it may be possible to partition the execution such that different parts of the applications do not need to use run time memory at the same time.
Thus, in this project with the design of an embedded scripting language, code footprint will be prioritised slightly higher than the run time memory usage, although both are important.

\subsection{Smartphones}
Many high end phones -- smartphones --  allows loading of native code. 
This covers approximately 12\% of the mobile phone market measured in number of units\footnote{In the fourth quarter of 2008 38.1 million smartphones where sold \cite{gartner}, whereas the total number of mobile phones sold in the same period were 314.7 million devices \cite{cellular-news}.}.
% http://www.cellular-news.com/story/36315.php
Smartphones gives more access to the devices, than J2ME, but is at the same time more difficult to devlop for, due to the large number of platforms, and limits on distribution, which usually also requires cryptographical signing of code.

The main operating systems for smartphones are: Symbian, RIM, Windows Mobile, Mac OS X and Linux, which will be discussed below.
An important thing to notice is, development is not only possible as native applications, but many of the devices also support J2ME discussed earlier, and on those devices that include a modern web browser, JavaScript is also a possibility for application development.

\subsubsection{Symbian}

Symbian is the classical operating system for smartphones. Applications are usually written in C/C++, with some restrictions to allow it to run better on with a moderately small amount memory. 
In order to distribute native applications on Symbian, it has to be cryptographically signed \cite{symbiansigned}.
Symbian is a currently closed source, but is on its way to be released as open source \cite{symbianopensource}.

It is the most popular operating system for smartphones, having 47.1\% of the market \cite{gartner-phone-survey-2008q4}. There are different incompatible user interfaces on top of Symbian, e.g. UIQ, QT, S60. 

\subsubsection{RIM, Windows Mobile and Mac OS X}

RIM (Research In Motion), Microsoft Windows Mobile and Mac OS X are the second to fourth most popular smart phone operating system, with 19.5\%, 12.4\% and 10.7\% market share respectively \cite{gartner-phone-survey-2008q4}.
RIM is connected with the BlackBerry devices, Mac OS X is connected with the iPhone, and Windows Mobile covers different hardware vendors. 
All these system are properitary closed source.

\subsubsection{Linux}

Linux has 8.4\% of the market share of smart phone operating systems.
While most of these are properitary systems, two promising initiatives are opening up, wnad while they do not any significant market share yet, they are interesting from a developer point of view, and described below. These are the Android platform and the Openmoko project.


\paragraph{Android}

Android is a software stack for smartphones. It is open source, and driven forward by google.
It is build upon a linux kernel, but user applications are not distributed natively, but executed on the Dalvik vm. Applications are written in Java, which is then compiled to Dalvik vm, rather than the usual JVM. The motivation for this is both to overcome some of the shortcomings of the JVM on mobile devices, and possibly also some cooperate issues with ownership of JVM technology.
Beside supporting Dalvik applications, there is also a modern webbrowser -- a WebKit derivative -- meaning that JavaScript can also be executed on the device.

While the software stack itself is open source, it also allows the manufactorer to make and keep proparitary changes for themselves. 
Meaning that actual phones designed with the stack -- at least those currently on the market -- are not truely open, in the sense that thay contain properitery drivers and special developer-versions of the phones are needed if you want to be able to load customised operating system images/native code.

Android is of course being ported to the devices from the Openmoko project, and in that case it is of course truely open.

\paragraph{Openmoko}
Openmoko is a project towards having open source phones.
It is both a software stack for smartphones, and it is also a small development division of the hardware company FIC, where they are making the first open source phones.
The two phones released so far have lots of rough edges both software and hardware wise, but from a developer point of view, these very interesting as they are not just phones but full Linux computers with X11 and most things as known from larger scale Linux environments -- and the source code is available for everything, so it is even possible to write new operating systems for them, and customise them in every way.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Programming languages}
The following sections surveys different programming languages:
\begin{itemize}
\item Forth is surveyed as this language runs on very minimalistic systems, and is also a stack language, thus relevant for the design of virtual machines. 
\item Hecl is the major scripting language for mobile devices, which should therefor be investigated for this project.
\item JavaScript is the basis for LightScript implemented in this thesis
\item Lua has a focus on embedded devices, and solutions from their implementation can inspire the languages created here. There is also an implementation of the Lua virtual machine, targeting mobile devices, why this language is also related to the benchmarks later on.
\item Python is becoming relevant on mobile devices, as it runs on many smart phones. Some of the language features may be usable in the design of new languages.
\item Lisp and Scheme are some of the inspirations for Yolan, especially with regard to the syntax.
\item Self is highly relevant for the LightScript implementation, as the object system is based on this, via JavaScript.
\end{itemize}


\subsection{Forth and other stack languages}
Forth is a stack based language.
It is very interesting for this project, as it is an example of an interpreted minimalistic language running on even very low-end devices.

The syntax is differnent from most modern languages, due to the reverse polish notation.
The functions work on a stack, which the programmer is explicit aware of. 
Functions or `words', replaces the parameters on the top of the stack with the result. 
New words/functions can be defined, and are concatenative in the sense that functions written after each other, has the same semantics as content of the functions written after eachother.
Beside the stack for parameters, there is also a stack for return addresses.

Forth looks very strange when starting looking at the programs, but it seems like it is the question of getting habit of being aware of the stack.
The programming style has focus towards decompositions. It also supports meta programming.
Forth systems has explicit compilation, the system has two modes: compile mode and interpret mode. 
It is possible to create words that are executed at compile time, using the immediate keyword.
Words -- ``functions'' in Forth -- are first class data types.

A couple of very interesting pointers related to Forth are: 
``Thinking Forth'' is a very good book, not only on Forth programming, but also touches a lot programming and problem solving in general.
There is also a tutorial Forth implementation \cite{jonesforth}, which is a good place to get started on how Forth can be implemented - it is the implementation of a full Forth system, written in assembler, but very readable and well documented.

Forth is also interesting from the virtual machine point of view, as this is a programming language which is similar to stack based virtual machines. So forth implementation techniques is shared with a lot of virtual machines. And research within this topic overlaps, an example is a dissertation on implementation of stack languages on register based machine \cite{ertl-dissertation}, as the representitive stack-based language.

Other more recent stack languages are Joy \cite{joy-language}, Cat \cite{cat-language} and Factor \cite{factor-language}.
Interesting development here are more typing, and also the use of anonymous code blocks, 
to create the control structure.


\subsection{Hecl and Tcl}
Hecl appears to be the major mobile scripting language, and is a dialect of Tcl. 
Tcl (Tool Command Language) is an embeddable scripting language \cite{tclbook} with a prefix notation, e.g. the syntax is similar to Logo, -- and also similar to Lisp if we see each unquoted line break as an end+begin parenthesis \verb|)(|. Unquoted here means not within quotations or brackets which is also a kind of quoting in Tcl.
In order to avoid prefix mathematical expressions, Tcl has the notion of expressions, which can be evaluated with the \verb|expr| command or as the condition in \verb|if|-statments etc, for example:
\begin{verbatim}
expr (1 + 2) * 3 + 4 * 5
\end{verbatim}
yields 29. Hecl does not have this notions of expressions, meaning that the above calculation would have to be written as:
\begin{verbatim}
+ [* [+ 1 2] 3] [* 4 5]
\end{verbatim}

A very interesting feature of Tcl and Hecl is the \verb|upeval| which allows introduction of new syntax by evaluating an expression one level earlier on the stack. 
This is possible as conditions and code blocks are usually quoted, for example the tcl code:
\begin{verbatim}
if { 1 < 2 } {
    puts Foo
} else {
    puts Bar
}
\end{verbatim}
would be similar to lisp-notation \verb|(if '(1 < 2) '(puts 'Foo) 'else '(puts 'Bar))|, where it is the responsibility of \verb|if| to evaluate the condition as an expression and then evaluate the correct block. 

Another aspect of Tcl is that everything are string, meaning that for example variable names are just strings, and if we want to read the value of a variable, we have to explicit tell that it should be looked up, by prepending it with \verb|$|.

\subsection{JavaScript/EcmaScript}
\label{JavaScript}
JavaScript was created as scripting language for web browsers, and was later standardised under the name of EcmaScript.
The reason it has become interesting is that it is included within most webbrowsers, and there by one of the most widely available platforms.
The following sections will describe the language, some of the implementations and finally some issues and approache to getting JavaScript to run on mobile and embedded devices.

\subsubsection{Language description}
It is a dynamically typed scripting language with first class functions, a prototypical object system, and a C-like syntax.
A good introduction to JavaScript is given in the book ``JavaScript the good parts''\cite{goodparts} and some of its material is also available online \cite{crockford-javascript-web} which serves well as an overview of the language. The EcmaScript standard \cite{ecma-262} can be used for getting to know the details of the languages.


\subsubsection{Implementations}
The following section described the main open source implementations of JavaScript/EcmaScript.

\paragraph{JavaScriptCore} 
JavaScriptCore is the implementation of JavaScript in Webkit.
The latest version has is called SquirrelFish and is an optimised register-based virtual machine,
with emerging support for JIT compilation.

The engine has been optimised a lot recently: the earlier versions was evaluating by walking through the abstract syntax tree, and was at that that time similar in performance to SpiderMonkey.
The new version -- SquirrelFish -- is much faster, which means that it is similar in performance to TraceMonkey and V8.

\paragraph{QScript}
QScript is the EcmaScript implementation which is a part of QT. This implementation is targeting application scripting rather that web scripting. An interesting part of QScript is its support for automatic binding generation. As WebKit is being integrated with QT, QScript is likely to be replaced with or merged a WebKit scripting engine in the long run.

\paragraph{Rhino}
Rhino is an implementation of JavaScript on top of Java. This is mainly as an embedded language for java applications. The implementation transforms the JavaScript program to Java classes, which is then executed.

\paragraph{SpiderMonkey}
\label{spidermonkey}
SpiderMonkey \cite{spidermonkey} is the JavaScript engine in Mozillas browsers, and it is also usable as an embeddable engine in other applications. The execution is based on a switch dispatched virtual machine TODO:verify-that-I-remember-correctly-here.

\paragraph{TraceMonkey} 
TraceMonkey is a branch of SpideMonkey, which incoperates JIT compilation, with a JIT compiler based on trace trees\cite{trace-tree}, 
which rather than JIT compiling the entire program,
traces and then compiles the most executed paths through the program.
This means that the JIT compiled code gets optimised to the actual execution,
and the approach also reduces the amount of code needed to be compiled.

\paragraph{V8} V8 \cite{v8} is a JavaScript implementation made by Google, and released in 2008 in conjunction with the release of their browser ``Chrome''. 
While it is not directly connected with the Android platform, a JIT compiler is already in place targeting the ARM CPU, so it seems likely that it will also target mobile devices in the long term.

A main focus and benefit of V8 is high execution speed.
The design builds upon experience from Self/Smalltalk implementations \cite{articles-before-v8}.
It is designed for JIT compilation from the beginning, and it also does some class inference to optimise methods and property accesses.
The idea here is that whenever a property is added to a JavaScript object this create an implicit class. When code is JIT'ed, a property access is compiled to a type check followed by fast code for accessesing the property, similar to that of more static, class based languages. This is possible due to the type check and implicit class, and it is much faster than a traditional JavaScript object property lookup.

The garbage collector is fast, - it is a generational garbage collector with two generations. The young generation is collected with a copy-collector which is linear in the time of live data, and thus heap allocation of activation records is almost free. 
This is combined with a mark and sweep collector when a full collect is needed.

\subsubsection{Targeting mobile devices}

There is the standardised EcmaScript Mobile Profile, which removes some features from the language, but is not implementable in practice on top of CLDC 1.0 as it still relies on floating point numbers

During the project it has turned out that there are other projects working on JavaScript languages for mobile devices.

Mojax (Mobile Ajax) \cite{mojax} is a properitary virtual machine for an EcmaScript like language. This language is also a subset of of EcmaScript, integer only, and appears not to support exception exceptions nor regular expressions.
The Mojax implementation seems to be a virtual machine which cannot execute scripts directly, but they need to be compiled before they can be loaded.


Mbedthis has a closed source implementation of a subset of JavaScript, this one targeting embedded devices \cite{mbedthis}. This uses integers by default, and does not have exceptions, labelled statements, switch, while, do-while, regular expressions, function literals, object literals, array literals, prototypes nor class methods.
% http://www.embedthis.com/products/appweb/doc/guide/appweb/users/ejs/overview.html
Recently that implementation seems to have been abandoned, in favor of new, more compliant implementation: Ejscript \cite{ejscript}.
%http://www.ejscript.org/products/ejs/index.html
While the C based implementation is available under GPL, there apparently is a J2ME implemention which is not published yet.

\subsection{Lua}

Lua is interesting, both as a language design/implementation, as it have looked into issues that also comes up in this project, and also as a language for low end devices, as lua implementations are available on embedded and mobile devices.
Lua is a dynamically typed, statically scoped embeddable scripting language, which is characterized by only have one datastructure: associative tables.
It has both good performance, and relatively low code footprint, and is as such also interesting for this project.

On mobile devices there is an implementation of the Lua virtual machine called Kahlua \cite{kahlua}.
This is only the Lua virtual machine.

\subsection{Python}
As phones are becoming more powerful, Python is beginning to play a role as a mobile scripting language on high end devices. This is carrying over from being a popular and major scripting language on computer platforms.

Python have some nice features, that may be inspiring when implementing other languages. Comments/documentation is integrated in the language via the concept of docstrings, which is a special kind of comment that will also be available for inspection during runtime. Unit testing is also integrated within the documentation framework, where a special syntax indicates that code in the documentation can also be executed when running tests.
Required indentation leads to easier readable programs.

The issue with python on mobile devices is, that it is relatively resource intensive, and thus use more energy, and is only available on high end phones.

\subsection{Scheme and Lisp}
Scheme and Lisp are especially interesting in this context, due to their minimalism, which inspiring in the design of a language with strong restrictions on the implementation environment.
The syntax is mostly isomorphic with the abstract syntax tree, meaning that parsing is mostly trivial.

For mobile devices, early versions of JScheme \cite{norvig-jscheme} are small enough, that they can be made to run, even with the limitations of footprint on the low end devices. It would need to be ported, as it uses parts of the language not available on the limited devices, but it can be usable as a benchmark tartget due to the sensible footprint size.

Another Scheme implementation, which targets mobile devices is ULM (Un Langage pour la MobilitÃ©) which is a part of a thesis on agent systems \cite{ulm}. Essentially the implementation is a virtual machine, which runs as a backend of a modified version of the Bigloo \cite{bigloo} Scheme compiler.
This was considered as a potential benchmark target, but I did not have time to get it to run, due to issues with dependencies, and at the same time it has a relatively large code footprint -- 96KB.

\subsection{Self}
\label{survey-self}
Self \cite{self} is especially interesting due to the prototypical inheritance.
In traditional object oriented programming, we have classes and objects, where classes can inherit from other classes, and objects are instantiation of classes.
Instead of classes and subclasses for inheritance, Self has a clone operator, which creates a new object using an existing object as blueprint.
An object in self contains a pointer to the parent (cloned) object, and a mapping from propertynames to values or methods. When a property is read, mapping is first searched, and if the name is not found there, the parent object is searched.

\section{Interpreter implementation}
\label{interpreter-implementation}
This section looks into a couple of aspects of interpreter implementation.
First is a discussion on different types of virtual machines, namely register and stack machines, and their different benefits and tradeoffs.
Then there is a discussion on the implementation of dispatch, which is one of the bottlenecks of intepreters.
Third there is a discussion of garbage collection techniques.
And finally there is a discussion on different approaches managing to run time stack.

\subsection{Register and stack machines}
Virtual machines are usually either register based or stack based.

Stack based virtual machines operates similar to the language Forth, where the operations work on the top of the stack. 
Operands are implicit coded, such that for example the add instruction just pops the two top elements of the stack, and pushes the sum. 
Stack machines are easy to compile to, 
which can simply be done by emitting the opcodes of a post-order walk through of the abstract syntax tree. 
There are no issues of register allocation, spilling, etc.
Stack machines are commonly used, the most known example is the Java Virtual Machine, and there many others when looking under the hood, for example: Python, the SpiderMonkey JavaScript implementation, and the .NET Common Intermediate Language.

Register based virtual machines are becoming more common. 
Usually they have a high number of registers, leading to longer opcodes than stack based virtual machines. But on the other hand, fewer opcodes are needed leading to faster execution \cite{register-vs-stack1, register-vs-stack2}. 
Examples of register based virtual machines are the Dalvik \cite{dalvik-vm} virtual machine, LLVM \cite{llvm}, Parrot \cite{parrot}, and the virtual machine of Lua 5.0 \cite{luavm}.

A third approach to virtual machines is just to use the abstract syntax tree for evaluation. This was for example used in earlier versions of WebKits JavaScript implementation, but has now been superseded by a stack virtual machine, and is currently being replaced by JIT compilation.

\subsection{Optimising the dispatch}
The dispatch is an important bottleneck when implementing interpreters.
An interpreter usually has a loop that fetches the next instruction, and executes it. The process of dispatching is to fetch opcode and goto the code for the next virtual instruction.
The reason this is a bottleneck it has to be executed for every opcode, and while dispatching may be done in few instructions, so may the code for the virtual instruction, and the dispatch is still a relatively large part of the execution time. 

The usual way is just to have a switch statement, which usually compiles to a jump table, such that:
\begin{verbatim}
for(;;) {
    // pc is the program counter into an array
    // of virtual instructions.
    switch(*(++pc)) { 
        case OP_FOO:
            ... implementation of FOO ...
            break; 
        case OP_BAR:
            ... implementation of BAR ...
            break; 
        case OP_BAZ:
            ... implementation of BAZ ...
            break; 
    }
}
\end{verbatim}
compiles optimised into something like:
\begin{verbatim}
.data:
jumptable = {
address of FOO,
address of BAR,
address of BAZ}

.code:
...
label looptop:
   inc register1  ; register1 contains the variable pc
   load register1 -> register2
   load jumptable[register2] -> register2
   jump_to register2
FOO:
    ... implementation of FOO ...
    jump looptop
BAR:
    ... implementation of BAR ...
    jump looptop
BAZ:
    ... implementation of BAZ ...
    jump looptop
...
\end{verbatim}
which is a simple and working approach, but is a bit inefficient. It is portable across compiler, and is the only option on some virtual machines,
This approach is used in SpiderMonkey, KVM\footnote{The reference implementation of JVM for mobile devices}, and in many other implementation.

If the compiler supports direct addressing of labels, we can instead do something like:
\begin{verbatim}
ops = { &OP_FOO, &OP_BAR, &OP_BAZ };
   
goto ops[++pc];

OP_FOO:
    ... implementation of FOO ...
    goto ops[++pc];
OP_BAR:
    ... implementation of BAR ...
    goto ops[++pc];
OP_BAZ:
    ... implementation of BAZ ...
    goto ops[++pc];
\end{verbatim}
This has two benefits: it removes the jump to the loop top, as it jumps directly to the next instruction after the table lookup, and the jumps themselves may be faster due to better branch prediction.
The reason the branch prediction may improve, is that branch prediction is often based on where previous branch at the specific code position were taken taken to. 
When the branch is at the end of the code for the virtual instruction,
the branch predictor has the current instruction as context for predicting the next instruction.
When the implementation is a switch based interpreter, the branch predictor has no context for predicting the next instruction.
This approach is called threaded code, and are up to twice as fast as switch based implementations \cite{ertl-efficient-2003}.
The cost is a bit more code, as the code for the table lookup needs to be at each instruction, rather than a single place. 

A further optimisation of the dispatch is to remove the table lookup. This is often done by replacing virtual machine instructions with the address of the implementation of the instruction. At the same time this costs space as an address usually uses 32 bit, where an virtual machine instruction often uses 8 bit. This is also better suited for stack machines, where the operands are coded implicit, rather that register machines, where the operands need to encoded beside the address of the implementation. This optimisation is commonly used in Forth implementations, and is also used other virtuel machines, for example optionally in the QScript \cite{qscript} and SquirrelFish \cite{webkit-source} JavaScript implementations.

Another way to remove the table lookup for the dispatch is to have a fixed size of each instruction implementation, and then calculate the position of the implementation from the opcode \cite{dalvik-talk}. When some instruction implemenations are significantly longer than others, the longer instruction implementations can contain a jump to the code that is beyond the fixed instruction implementation size.
For example:
\begin{verbatim}
INSTRUCTION_IMPLEMENTATIONS:
   ... implementation of FOO ...
   goto INSTRUCTION_IMPLEMENTATIONS + IMPLEMENTATION_SIZE * (++pc)
   ... padding ...
// address INSTRUCTION_IMPLEMENTATIONS + 1 * IMPLEMENTATION_SIZE
   ... implementation of BAR ...
   goto INSTRUCTION_IMPLEMENTATIONS + IMPLEMENTATION_SIZE * (++pc)
   ... padding ...
// address INSTRUCTION_IMPLEMENTATIONS + 2 * IMPLEMENTATION_SIZE
   ... first part of implementation of BAZ  which is a long instruction...
   goto REMAINING_IMPLEMENTATION_OF_BAZ
// address INSTRUCTION_IMPLEMENTATIONS + 3 * IMPLEMENTATION_SIZE
   ... implementation of other instructions...

REMAINING_IMPLEMENTATION_OF_BAZ:
   ... more implemenation of BAZ...
   goto INSTRUCTION_IMPLEMENTATIONS + IMPLEMENTATION_SIZE * (++pc)
\end{verbatim}
This has the benefit of being even faster, as the table lookup is eliminated,
but it is also more difficult to implement, 
as the size of the implementation depends on the machine architechture
and thus it is not possible to do in a high level language.
Notice that \verb|IMPLEMENTATION_SIZE| should be a power of two such that the multiplication is just a shift.
This optimisation is for example used in the Dalvik Virtual Machine \cite{dalvik-vm}


\subsection{Garbage collection}
Garbage collection is an important issue in the design of virtual machines.
There is a good introduction/survey on the topic \cite{gc-survey}.
An additional way to get into the subject is to see what approach recent scripting language implementations have taken.

The next section looks into aspects of garbage collections.
The following sections describes different approaches to garbage collection,
with a final section looking at garbage collection implementation in actual language implementations.

\subsubsection{Issues of dynamic memory management}
Some of the following issues are relevant for dynamic memory management in general, and other are more focussed towards garbage collection.

Locality covers that memory that is used at similar times also lies together in memory.  This has impact on the performance, due to levels of caching, and poor locality may also lead to more swapping to secondary storage when memory is scarce, and secondary storage is available.

Fragmentation is when there is unused space between allocated blocks of memory. 
Compacting garbage collection algorithms addresses this problem, by moving the blocks of memory to remove the fragmentation.

Performance of the garbage collection is also an issue. This covers both CPU usage, and delays during execution.
The garbage collection takes time, -- this comes both from the time from running the actual garbage collection algorithm, and also from lost caching, as garbage collection methods usually messes up the cache.
Another issue with garbage collection is whether it is interactive or that there may be longer interruptions or delay of execution, in which the garbage collection cycle runs. This leads to development concurrent garbage collection, that may take more time overall as a tradeof of shorter interruptions of the execution.

Garbage collection algorithms can be conservative or exact, where conservative garbage collection is not directed by strong typing and assumes that data are pointers, whereas exact garbage collection knows which of the data that are actually pointers that need to be tracked.

\subsubsection{Reference counting}

Each allocated block of memory also keeps track of the number of pointers to it, and when this is decreased to zero, the block is deallocated.
This has the benefit of being reasonable interactive.
The additional bookkeeping for each pointer assignment, gives it a bit slow performance, which i proportional to the program execution rather than the memory allocation. 
Another issue is that cyclic references does not get caught by it, so it needs to be combined with another garbage collector or some kind of cycle detection.

\subsubsection{Copying collector}
A typical copying garbage collector, splits the heap in two, and when one part of it is filled up, it copies the root nodes and all of their descendents into the other part, and updates the pointers.  This also has the benefit that it removes fragmentation. 
The execution time only depends on the amount of live data, and is executed whenever a part of the heap is filled up.
The issue here is that the heap is split in two, leading to less available memory, and also that this process is often a stop-the-world approach, leading to a moderately long delay when the collection occur.

\subsubsection{Mark and sweep}
Mark and sweep essentially works in two phases: first walk through the live objects and mark them as alive, and then walk through the heap and make unmarked objects available as free.
There are several variations over this topic. Rather than stop-the-world, it can be made more interactive by running in parallel with the execution, for example by coloring/marking ``alive and children marked'', ``alive but children not marked yet'' and ``not marked yet''.
Instead of a mark and sweep, a mark and compact approach is also possible, where fragmentation is removed at the cost of a bit more execution time.

\subsubsection{Generational Garbage Collection}
\label{generational-gc}
Generational garbage collection, can improve the performance of the collection. This is based on the hypothesis that recent allocated memory are more likely to be freed than older memory.
This is exploited by dividing the memory into generations, where the youngest generations are garbage collected more often than the older ones. 
Thus by only looking at a single generation/a smaller part of the memory, the garbage collection cycle is significant faster.
An issue here is to keep track of pointers from older generations to younger ones, such that the younger generations can be collected independently.
Another question is when to promote memory from younger to older generations.

\subsubsection{Examples of garbage collection implementation}
A common approach is to combine reference counting with some kind of cycle cleanup. 
An example of this is the python implementation. 
One of the benefits of this approach is most of the garbage collection only yields small pauses which will not be felt by the user in an interactive environment, unlike the stop-the-world approach.

The KVM, which is the reference implementation for mobile Java Virtual Machines, is now using a compacting mark and sweep. It originally started out with a copying collector, which had a larger memory requirement, and also had a period with non-compacting mark and sweep.


A practical example of a generational garbage collector is the one which is a part of the V8 JavaScript implementation.
It has two generations, where the young generation is a copying-compacting collector, which is called often, combined with a mark-and-sweep for the full heap, when that is needed.
The complexity of a copying collector is linear in the size of live objects, which means that there are essentially no overhead for heap allocating the run time stack with this garbage collection strategy, which solves some of the problems of the run time stack, as described later on.

\section{Parsing}
A language implementation must parse the source text into the abstract syntax tree to work on. 

An issue here is that parsers often take quite a large amount of code, especially if they are generated by compiler-compilers.
Generated lexers and LALR(1) parsers, generated by for example  \cite{yacc, yacc2}, usually have quite large state tables.
Recursive descent parsers seems to use a bit less code footprint that LALR parsers, but still requires functions for the all the grammar productions, which may still be expensive.

Grammar based parser generation and implementation is extensively studied and described for example in  \cite{basics-of-compiler-design, grammar}, and will not be discussed further in these sections.
Another approach, which is also very elegant, 
but have recieved less focus is the
top down operator precedences parser,
which will be discussed below.

\subsection{Top down operator precedence parsers}
\label{tdop}
Top down operator precedence parsing combines recursive descent parsing with operator precedence, which simplifies the implementation significantly.
It was described thirty years ago in  \cite{top-down-operator-precedence}, but has not had much attention until lately with  \cite{beautiful-code, crockford-tdop}.

A token can have a null denominator function, a left denominator function and a precedence.
The null denominator function is used to build the abstract syntax tree node, if the token stands first (is leftmost) in an expression.
The left denominator function is used to build the abstract syntax tree node, if we already have something to the left of the token within the expression.
The precedence of the next token is used to determine whether we are done parsing an expression or need to use the that token to build another abstract syntax tree node, by calling left denominator function of the token.

To be able to build the abstract syntax tree node, the left denominator function gets the parsed node to the left of the token as a parameter. Additionally it is possible for the denominator functions to parse expressions to the right of the token by calling the parsing function recursively. Here it is necessary that the parsing function also takes a priority as a parameter. This priority parameter makes sure that you do not call left denominator functions for tokens with lower priority.

The parsing itself is then simply done, by first calling null denominator function of the first token, and then calling the left denominator functions of the next tokens, as long as the next token has higher priority than the priority passed to the parsing function.
The denominator functions attached to each token are then responsible for building the syntax tree.
So the core loop of the parser is implemented like:
\begin{verbatim}
define parse(int priority):
    syntax_tree = next_token.null_denominator()
    while next_token.priority > priority:
        syntax_tree = next_token.left_denominator(syntax_tree)
    return syntax_tree
\end{verbatim}

Now we need to assign meaningfull precedence and denominator functions to the tokens:
Atoms, variable names, literals etc., just need to have a null denominator functions that return their node. Unary operations such as minus, not, ..., needs to have null denominator that calls parse once, and create a node that applies the operator to the parsed node.
Binary (infix) operators are made by assigning the left denominator function to something that create a node with its parameter as the left hand side and calling parse to make the right hand side. Binary operators can be made right associative by reducing the priority passed to parse.
Lists just calls parse until the end of the list is reached. And other constructions can be made similarly. 

The limitation of this parser as described here is that only one left hand side expression is passed to the left denominator function, making reverse polish notation languages difficult to implement. A non-recursive version with an explicit stack instead could solve that, though it may not be an issue with most syntaxes. 

\section{Scope}
The scope is important when designing programming languages.
There are two major kind of scopes: static scoping and dynamic scoping.
Static scope is also called lexical scope, and corresponds to the lexical structure of the source code.
This is in contrast to dynamic scope, where the variables are accessible other places than in the blocks where they are defined. This is exemplified by the following:
\begin{verbatim}
function f() {
    x := 17
}
function g(x) {
    f()
    print(x)
}
g(42)
\end{verbatim}
which would print 42 if it were written in a statically scoped language, and would print 17 if it were written in a dynamically scoped language.

Static scope is more natural as the scope matches the structure of the code. Dynamic scope requires more discipline, as it allows the programmer to tamper with local variables of other parts of the code, and counters good habits of information hiding. A very importeant feature is also that static scope can be used to create closures for functions, which gives extra flexibility to the language.
On the other hand, dynamic scope very trivial to implement, and can thus use slightly less space in the code footprint. Dynamic scope does also not have the issue with the funarg problem discussed in Section~\ref{funarg}.
For more advanced language implementations static scope has the benefit over dynamic scope, that it is easier to analyse and optimise due to locality, in the sense that its use is limited to the local lexical scope, and thus the optimisations on this part of the code need be concerned with other parts of the code.

\subsection{Stacks and activation records}
A typical way to implement local variables is with the use of a stack.
The following text pretends we are on a full stack machine, 
in practical implementations on CPUs, the top of the stack is usually a number of registers instead.

Whenever a function is called, the parameters is pushed onto the stack. Then when the function is entered, a return pointer is often pushed to the same stack, and there is allocated space for the local variables, the computations are done, and we return, jumping back and restoring the stack size to the original.
The allocation on the stack allows for local variables, and functions can also be recursive without a problem. The space the stack with the local variables, etc., is called the activation record for the function.

\subsection{Higher order functions, and the funarg problem}
\label{funarg}

When we have higher order functions and static scoping, we cannot just place the activation records on the stack. The reason for this is that inner functions may live on, and acces local variables of the outer function, after the outer function has returned. Consider the following code:
\begin{verbatim}
function f(x) {
    function g(y) {
        return x + y
    }
    return g
}
\end{verbatim}

The issue in this code is that the new function $g$ lives on after $f$ has returned, and at the same time $g$ uses a value that lies on the activation record of $f$. Therefor, if the activation record is just allocated on the call stack, and nothing further is done about it, the value of $x$ will no longer be available at the time $g$ is called.

There are several solutions to this problem:
\begin{itemize}
\item Disallowing/not supporting first order nested functions or access to outer scope within nested functions. Or not having static scope.
\item Outer scope variables cannot be changed from inner scope, but is instead passed as immutable values at function/scope creation. 
\item Allocate the activation records on the heap, instead of the stack, and let them be garbage collected. 
\item Keep track of which may live on after the function exits, and move those to the heap.
\end{itemize}

Not having nested functions, only having a single-function-local and a global scope, or not having static scope, simplifies the implementation and is a solution in mostly imperative languages.
The effect of this is that the funarg-problem is disallowed from the language rather than solved, and has the cost of not supporting closures either.

Copying outer scope variables to inner scope makes good sense in functional languages, where they are will not be mutated. The issue with this approach is that if the variable is mutated in an inner function, it is only the copy that is mutated, and the mutation is local to the function.
A workaround for mutations in languages that copies variables from outer to inner scope is to us a reference to the mutated variable rather than the variable itself, such that when the reference is copied to the inner scope it is still possible to mutate the value, as we need not mutat the reference itself.

Another approach is to let the outer scope stay alive until the inner closure dies, and then just reference variables as usual.
This can be done by allocating the activation records on a garbage collected heap,
which is not as expensive as it may seem, due to the high effiency of modern garbage collectors.
Another way to do this is to keep track on which variables may stay alive at the exit of a function, and move them to the heap at that time.
Or just identify variables that are used by in inner functions, and put those on the heap.

\subsection{Implementation of variables}
When we are implementing the scope, an issue is how to implement the variables. 
Here are several options: 

The  classical good-performing approach is to implement them on a stack. On each function call, an activation record is added on a linear stack, and each of the variables can be spilled into its place on the activation record. This is very fast as allocation and deallocation is just a question of increasing/decreasing the stack pointer. The issue with this approach is that extra code is needed to handle the funarg problem.

Another approach is to allocate the stack frame on the heap, and let it be cleaned up with the garbage collector. The quality of this approach depends a lot on the garbage collector, as it has nearly no overhead with a good generational garbage collector \cite{generational-heap}, see section~\ref{generational-gc}.
This also solves the funarg problem as the closure has a reference to the activation record, which will then not be garbage collected.

Usually the variable is resolved at compilation time, or the first time it is encountered, but 
some interpreters have a less efficient approach, where they lookup up variables at run time, either through a stack, or a table of variables.

\section{Summary}
\chapter{Approach}
\label{method}
\section{Overall methodology}

When making projects, the approach can be
more or less bottom-up or top-down.
A top-down approach could be like the waterfall model\cite{waterfall} of software devlopment,
where we start with the specification, design and then implementation and test.
The bottom-up is could be like stepwise refinement\cite{stepwise-refinement}, more agile\cite{agile-manifesto, extreme-programming}, starting out with a quick prototype, which gets more and more refined.

The purpose of this project is to learn about programming languages and mobile development, and create a scripting language. 
The bottom-up approach is choosen, as it opens more up for experimentation, and new ideas are easier follow, than with the top-down approach.

The methodology of the project is to make a series of prototypes related to mobile scripting language implementation, while surveying the field. 
The best parts of the prototypes are expanded and built further on, to get the scripting language, while documenting and specifying the language in parallel.

\subsection{Prototypes}
The design and implementation of the languages is not just a single design, but comes through ca. 30 prototypes of aspects of the languages.
This leads up to LightScript and Yolan, which will be described in the next chapters.
Discarded prototypes include a Forth-like language, several parser/compiler implementations on top of JavaScript, a couple of virtual machins on the jvm, some experiments towards an implementation in C, experiments with mobile applications and their GUI, and drafts of parts of what becomes LightScript and Yolan.

\section{Evaluation}
The evaluation should compare the code footprint and execution speed of the the developed languages, with existing scripting language.
In addition to this, it could also be interesting to look at how different parts of a language implementation adds to the code footprint.

\section{Summary}
\chapter{Yolan}
\label{yolan}
\index{Yolan}
\section{Design choices}
The overall design direction of Yolan is to minimise the code footprint, while still having a scripting language with first class functions.
The next paragraphs elaborate the following design choices:
\begin{itemize}
\item Should be able to run on CLDC 1.0/MIDP 1.0
\item Use standard Java classes, for easier interaction with Java code
\item Execution of code, loaded as source text at run time
\item Functions as first class values
\item Efficient variable access
\item Integer only, - no support for floating point number
\item Null is false
\item Lisp like syntax
\item Dynamic scope
\item Builtin support for arrays, hashtables, etc.
\item Lazy Java interface
\item Online exection
\end{itemize}

CLDC 1.0 is the most limited device configurations and API for mobile devices. If the language is able to run this configuration, it will also be able to run on the other Java configurations, and thus be able to run on most devices.

\label{yolandesign}
Use of standard Java classes instead of custom classes for builtin data types such as arrays or tables has three benefits: the code footprint is smaller, as these data types do not have to be implemented, it may be faster, as the builtins may be implented natively, and it is easier embeddable, as as the developers already know the data types from standard Java.
The issue of using the builtins is that it gives less flexibility to the design of the behaviour of the data types in Yolan.

Being able to load code at run time is important for several reasons:
It allows larger programs, than would otherwise be possible within the devices, as the different parts of the program can be loaded and unloaded as needed.
It allows adding updates and new features to the programs, without needing them to be reinstalled manually.
It allows the language to be used for configuration files, and data initialisation.
Being loaded as source code both gives more clarity, and makes it easier to distribute and edit, thus making it more suitable for for use as configuration language, and scripting by users.

Functions should be first class values, both to enable functional programming and increase the expressiveness of the language.

If variables are looked up at every access at runtime, this is likely to be bottleneck for performance, and instead the implementation must ensure that they are only resolved once for each place they are used in the source text.
The platform is already slow, so performance wise we probably can not afford not doing this optimisation.

Integers is the only number type supported on the platform. Floating point numbers would have to be emulated, which would both give a performance penalty, and more importantly a huge increase in code footprint due to emulation code.

Null is also the false value. This simplifies the implementation slightly, and thus reduces the footprint.

Yolan has a Lisp-inspired syntax. This trivialises the parser ipmlementation, and thus reduces code footprint.

Dynamic scope removes the issue of the funarg problem, and thus simplifies the implementation of variables. This is generally bad language design, but at the same time it may reduce the code footprint slightly.

The language is more like a traditional scripting language, than a functional language, 
as it does not support tail recursion nor has cons-list, but instead it uses resizable arrays and hashtable as data structures, and has more imperative programming model, with while- and foreach-loops, etc.
The motivation for this is that it is simpler to implement efficiently on top of an already imperative/objectoriented virtual machine, and that functional languages often are less space efficient.

If the interface with Java is lazy, this allows new control flow constructs to be added, by the embedded, which again allows the implementation to be a small core, with support for expansion.

Execution should be online, in the sense that whenever a full statement is read from the input, it should be executed, not needing to read the full file. This is both practical for interactive evaluation, and also has a benefit memorywise as the entire program never needs to be fully in memory, as executed code may be garbage collected.

\section{Syntax tree rewriting}

To make the interpreter as simple as possible, it interprets the syntax tree directly. Still this has some performance issues, as, for example variable and function lookup would be done at each execution. To avoid the cost of this some of the evaluation functions instead resolves the variables the first time they are executed and replaces the node itself with a node of the resolved variable or function.

\section{Language specification}

\subsection{Syntax}
The syntax is similar to lisp, in the sense that function applications are written as lists, where the first element in the list is the function to be applied.
Programs consist of lists, where each element is either a list, a variable name, or a literal.

\subsubsection{Function application / lists}
Function applications are written in lisp-style as lists. The first element in the list is the function to be applied.
Lists are enclosed within square brackest \verb|[|$\cdots$\verb|]|, and may be nested. The elements within the lists are separated by whitespaces. 
As lists are the notation for function applications, every list must have at least one element, which is the function to be applied.

\subsubsection{Variable names}
A variable name is a sequence of characters. The possible characters are letters, numbers, the symbols \verb"!#$'()*+-,-./:<=>?@\^_`{|}~", and any unicode symbol with an unicode alue of 127 or higher. The first character in the name of a variable must be non-numeric.

\subsubsection{Integer literals}

Integers are written as a sequence of digits (\verb|0123456789|). Only base 10 input is possible and only non-negative numbers can be written as literals. Negative integers must be generated by subtraction.

\subsubsection{Comments and whitespaces}
Characters with a unicode value of 32 or less are regarded as whitespaces. This includes the usual space, tab, newline, and line-feed. Whitespaces are used to separate list elements, and are discarded during parsing. A comment must be preceded by a whitespace, starts with a semicolon \verb|;| and continues until the end of the line. Comments are discarded during parsing.

\subsection{Builtin functions}
The builtin functions are listed in the following sections. As the language is designed for embedding in other applications, there are no standard input/output function, file access, network, etc. as these might not be present or differ significantly between target devices/platforms.

In the following, the function names are written with {\tt fixed width} font, and the parameters are written in $cursive$. Parameters can be any expression, and are named according to their type or function: $num$s are expressions that should evaluate to numbers, $exp$s are expressions that may be optionally evaluated (e.g. in {\tt if}), $val$s are expressions will be evaluated, $string$s are expressions that should evaluate to a string, and so on.

\subsubsection{Variables}
\subsubsection*{\tt{[set }$name$ $value$\tt{]}}
Evaluate $value$ and let $name$ refer to the result.

\subsubsection*{\tt{[locals [}$name_1 \cdots name_n$\tt{]} $expr_1 \cdots expr_n$\tt{]}}
Let $name_1 \cdots name_n$ be local variables in $expr_1 \cdots expr_n$: First save the values corresponding to $name_1 \cdots name_n$, then evaluate $expr_1 \cdots expr_n$, next restore the values of $name_1 \cdots name_n$ and finally return the result of the evaluation of $expr_n$.

\subsubsection{Conditionals and logic}
\subsubsection*{\tt{[if }$cond$ $expr_1$ $expr_2$\tt{]}}
Evaluate $cond$ and if the result is non-$nil$ then evaluate and return $expr_1$, else evaluate and return $expr_2$.

\subsubsection*{\tt{[not }$cond$\tt{]}}
If $cond$ is $nil$ return $true$ else return $nil$.

\subsubsection*{\tt{[and }$expr_1$ $expr_2$\tt{]}}
Evaluate $expr_1$ and if it is non-$nil$, evaluate and return the value of $expr_2$, else return $nil$.

\subsubsection*{\tt{[or }$expr_1$ $expr_2$\tt{]}}
Evaluate $expr_1$ and if it is non-$nil$ return its value, else evaluate and return the value of $expr_2$.

\subsubsection{Repetition and sequencing}
\subsubsection*{\tt{[repeat }$num$ $expr_1 \cdots expr_n$\tt{]}}
Evaluate $expr_1 \cdots expr_n$ $num$ number of times ($num$ must evaluate to a number). The result is the last execution of $expr_n$, or $nil$ if no expressions were evaluated, i.e. $num \leq 0$.

\subsubsection*{\tt{[foreach }$name$ $iterator$ $expr_1 \cdots expr_n$\tt{]}}
For every value from the $iterator$, bind it to the local $name$ and evaluate $expr_1 \cdots expr_n$. The result of the evaluation is the last executed $expr_n$ or $nil$ if no expressions were evaluated. $name$ is a local variable, and is thus saved before the loop, and restored afterwards.

\subsubsection*{\tt{[while }$cond$ $expr_1 \cdots expr_n$\tt{]}}
While $cond$ evaluates to non-$nil$, evaluate $expr_1 \cdots expr_n$, and return the value of the last $expr_n$ or $nil$ if no expressions were evaluated.


\subsubsection*{\tt{[do }$expr_1 \cdots expr_n$\tt{]}}
Evaluate $expr_1 \cdots expr_n$ and return the result of $expr_n$.


\subsubsection{Functions}
\subsubsection*{\tt{[lambda [}$name_1 \cdots name_n$\tt{]} $expr_1 \cdots expr_n$\tt{]}}
Create a new anonymous function, with the parameters $name_1\cdots name_n$. Application of the function will bind its arguments to local variables $name_1\cdots name_n$, evaluate $expr_1\cdots expr_n$ and return $expr_n$.
\subsubsection*{\tt{[defun [}$name_{function}$ $name_1 \cdots name_n$\tt{]} $expr_1 \cdots expr_n$\tt{]}}
Create a new function, and bind it to the variable $name_{function}$. It is equivalent to {\tt{[set }}$name_{function}${\tt{ [lambda [}}$name_1 \cdots name_n${\tt{]}} $expr_1 \cdots expr_n${\tt{]]}}.

\subsubsection*{\tt{[apply }$function$ $param_1 \cdots param_n$\tt{]}}
Apply the $function$ to the parameters $param_1\cdots param_n$. The difference between this and the usual function application {\tt{[}$function$ $param_1\cdots param_n$\tt{]}} is that that \verb|apply| allows $function$ to change between invocations, whereas the usual function application assumes that $function$ is static to be able to optimise it during runtime.

\subsubsection{Integer operations}
\subsubsection*{\tt{[+ }$num_1$ $num_2$\tt{]}}
Calculate the sum of two integers.
\subsubsection*{\tt{[- }$num_1$ $num_2$\tt{]}}
Calculate the difference of two integers, the result is $num_2$ subtracted from $num_1$.
\subsubsection*{\tt{[* }$num_1$ $num_2$\tt{]}}
Calculate the product of two integers.
\subsubsection*{\tt{[/ }$num_1$ $num_2$\tt{]}}
Integer division, $num_1$ is divided by $num_2$.
\subsubsection*{\tt{[\% }$num_1$ $num_2$\tt{]}}
Returns the remainder of dividing $num_1$ by $num_2$.

\subsubsection{Type predicates}
\subsubsection*{\tt{[is-integer }$val$\tt{]}}
Returns $true$ if $val$ is an integer.
\subsubsection*{\tt{[is-string }$val$\tt{]}}
Returns $true$ if $val$ is a string.
\subsubsection*{\tt{[is-list }$val$\tt{]}}
Returns $true$ if $val$ is a list.
\subsubsection*{\tt{[is-dictionary }$val$\tt{]}}
Returns $true$ if $val$ is a dictionary.
\subsubsection*{\tt{[is-iterator }$val$\tt{]}}
Returns $true$ if $val$ is a iterator.

\subsubsection{Polymorphic functions}
\subsubsection*{\tt{[equals }$val_1$ $val_2$\tt{]}}
Compare $val_1$ to $val_2$ and return $true$ if they are the same, or $nil$ if they are different. $val_1$ and $val_2$ must have the same type, and should either be integers or strings.
\subsubsection*{\tt{[is-empty }$val$\tt{]}}
Returns $true$ if a list, dictionary or iterator does not have any elements. Else it returns $nil$.
\subsubsection*{\tt{[put }$container$ $position$ $value$\tt{]}}
Store a value into a a list or a dictionary. If it is a list, the $position$ must be an integer in the range $0,1, \cdots, ${\tt{[size }}$container${\tt{]}}$-1$.
If it is a dictionary, the position must be a string or an integer. An entry is deleted from a dictionary by storing $nil$ as the $value$.
\subsubsection*{\tt{[get }$container$ $position$\tt{]}}
Retrieve a value from a list or a dictionary. It has the same constraints on $position$ as with \verb|put|. Retrieving an uninitialised entry from a dictionary yields $nil$.
\subsubsection*{\tt{[random }$val$\tt{]}}
If $val$ is an integer, return a random number in the range $0,1, \cdots, val -1$. If $val$ is a list, pick a random value from the list.
\subsubsection*{\tt{[size }$val$\tt{]}}
Return the length of a string, the number of values in a list, or the number of entries in a dictionary.
\subsubsection*{\tt{[< }$val_1$ $val_2$\tt{]}}
Compares $val_1$ with $val_2$. If $val_1$ and $val_2$ are an integers, return $true$ if $val_1$ is strictly less than $val_2$ and otherwise $nil$.
If $val_1$ and $val_2$ are strings, do a lexicographical comparison and return $true$ if $val_1$ comes strictly before $val_2$, and otherwise $nil$.
\subsubsection*{\tt{[<= }$num_1$ $num_2$\tt{]}}
Compares $val_1$ with $val_2$. If $val_1$ and $val_2$ are an integers, return $true$ if $val_1$ is less than or equal to $val_2$ and otherwise $nil$.
If $val_1$ and $val_2$ are strings, do a lexicographical comparison and return $true$ if they are equal or $val_1$ comes before $val_2$, and otherwise $nil$.

\subsubsection{String functions}
\subsubsection*{\tt{[stringjoin }$val_1\cdots val_n$\tt{]}}
Create a string by concatenating $val_1\cdots val_n$.
If $val_i$ is an integer or a list, it is converted to a string.
A list is converted to a string by concatenating its elements, as if {\tt stringjoin} were called with the list elements as parameters.

\subsubsection*{\tt{[substring }$string$ $num_{begin}$ $num_{end}$\tt{]}}
Create a substring from a string, starting inclusively at character position $num_{begin}$ and ending exclusively at character position $num_{end}$. The positions starts counting at $0$, so thus {\tt{[substring }$string$ $0$ \tt{[size }$string$\tt{]]}} is the entire string.

\subsubsection{List functions}
\subsubsection*{\tt{[list }$val_1\cdots val_n$\tt{]}}
Create a new list, containing the elements $val_1\cdots val_n$.

\subsubsection*{\tt{[resize }$list$ $num$\tt{]}}
Change the size of the $list$ to be $num$ values. 
If the new size is larger than the previous, new values will be added to the end of the list, and they are initialised to be nil. If it is smaller, then the list will be truncated at the end. The list is returned.

\subsubsection*{\tt{[push }$list$ $val$\tt{]}}
Push the value $val$ at the end of the $list$. The size of the list grows by one, and the last element is now $val$.

\subsubsection*{\tt{[pop }$list$\tt{]}}
Remove the element from the end list. The function returns that element, and reduces the size of the list by one.

\subsubsection{Dictionary functions}
\subsubsection*{\tt{[dict }$key_1$  $val_1$ $\cdots$ $key_n$ $val_n$\tt{]}}
Create a new dictionary with $n$ entries, where $key_1$ maps to $val_1$ and so forth.

\subsubsection{Iterator functions}
\subsubsection*{\tt{[keys }$dictionary$\tt{]}}
Create a new iterator across the keys of a dictionary.
\subsubsection*{\tt{[values }$container$\tt{]}}
Create a new iterator across the values of either a dictionary or a list.
\subsubsection*{\tt{[get-next }$iterator$\tt{]}}
Get the next element from the iterator, or nil if the iterator is empty.

\subsubsection{Debugging}
\subsubsection*{\tt{[log }$string$\tt{]}}
Logs the message from $string$, possibly ignored if debugging is disabled.
\subsubsection*{\tt{[assert }$string$ $val$\tt{]}}
Halt the execution with error message $string$ if $val$ is nil, possibly ignored if debugging is disabled.


\section{Developers guide}
Yolan is a minimal scripting language implemented on Java.
It allows scripting to be added to application, with a minimal overhead on the size of the JAR file -- which is the most limiting factor on low end mobile devices. The features and limitations of Yolan are:
\begin{itemize}
\item A scripting language, with support for higher order functions
\item Support for loading of code at run time
\item Operates directly on standard Java classes, such as \verb|java.lang.Integer|, \verb|java.util.Hashtable| and \verb|java.util.Stack|
\item Runs on Java Micro Edition/J2ME, and requires only CLDC 1.0/MIDP 1.0
\item Adds approximately 5KB to the size of the optimised JAR file. Only single-threaded/non-reentrant, to achive the small size
\item Parses and executes one expression at a time, allowing interactive programming, and implying that the entire program need not to be in memory at once
\item Interpreted - code can be entered directly on the device, not needing an extra step of compilation, and thus it is also suitable for scriptable configuration files, user scripts etc.
\end{itemize}

The implementation of Yolan consists of a single class \verb|Yolan| with the actual implementation, and an interface \verb|Function| which is what a class need to implement in order to be callable from Yolan.
While Yolan only has a single classfile for the implementation, in order to reduce the JAR file size, it consists several logical classes: a parser, a single static runtime, and Yolan instantiated objects are Yolan code that can be evaluated, i.e. delayed computations.

Having a single runtime reduces memory usage, but also limits applications to only execute a single script, and only having a single execution context, at a time. The reduction of memory usage comes from that refererences to the execution context can be hard coded, and thus the delayed computations does not have to carry a reference to the context. There are also a memory reduction due to that less code is needed and the class for the context can be joined into the main class file, as static properties.

The \verb|Function| interface consist of a function that takes an array of Yolan objects -- delayed computations -- as parameter, and the return a value. In this sense Java objects callable from Yolan are essentially lazy functions, and themselves responsible for evaluating their arguments.

\subsection{Getting Started}
The core method of a Yolan object is the \verb|value()| method which evaluates the code the Yolan object represents, and returns the result. This method may throw \verb|Exeception|s as well as \verb|Error|s if the code it represents has faults, so if we are executing user code, or want to be robust against errors in scripts, the Yolan evaluation should be surrounded by a \verb|catch(Throwable)|.

Yolan objects are created with the static \verb|readExpression| method that parses the next Yolan expression from an input stream. So if we want to create a simple interactive interpreter, we can write: \begin{lstlisting}
class Main {
    public static void main(String [] args) throws java.io.IOException {
        Yolan yl = Yolan.readExpression(System.in);
        while(yl != null) {
            try {
                System.out.println("Result: " + yl.value().toString());
            } catch(Throwable yolanError) {
                System.out.println("Error: " + yolanError.toString());
            }
            yl = Yolan.readExpression(System.in);
        }
    }
}\end{lstlisting} 

This code could be saved in a file called Main.java, placed in a directory with Yolan.class and Function.class, and then compiled and executed by executing \verb|javac Main.java| and \verb|java Main|.

Notice that the input stream \verb|System.in| can be replaced with any input stream, so the same basic idea for can be used for evaluating files, programs as strings within the application, or even as streams across the network, where Yolan could work as a shell for remote scripting/controlling an application.

If we want to execute an entire stream, there is a short hand builtin method for doing that: \verb|eval|. For example:
\begin{lstlisting}
class Main {
    public static void main(String [] args) throws java.io.IOException {
        Yolan.eval(new FileInputStream(new File("script.yl")));
    }
}
\end{lstlisting}
This code opens the file "script.yl", and evaluate all the expressions within it. 
\verb|eval| throws away the results of the individual expressions and does not print them,
so the above code is only useful if we have added some user defined functions to Yolan that allows it to do something practical.

\subsubsection{Adding functions to the runtime}
This section describes how to make Java code callable from Yolan.
While the builtin Yolan functions supports basic data structures etc., there is no built in way to do input/output from yolan that is platform dependent: Java Standard Edition supports files, where Java Micro Edition has a record store, and user interfaces ranges between Midlets, Applets, graphical applications, and text standard-in/out.
So when the language needs communicate with the user, or work on the state of the host application, 
some functionality needs to be added, which is most easily done by adding functions to the runtime.

The \verb|Function| interface is the way to do that. To implement the interface a single function \verb|apply|, that takes an array of Yolan objects as parameter, and returns an object is required.
Notice that the parameters are passed lazily, e.g. they are only evaluated when the called function chooses to evaluate them, so we need to call the \verb|value()|-function of the Yolan objects when we want the actual value.
Execution of the Yolan object may also have side effects, so whether, and the number of times, the \verb|value()| is called matters.
In order to add a new Java function to be callable from the runtime, the method \verb|Yolan.addFunction| takes a name as a string and a \verb|Function| as parameters, and binds the name to the function. As an example the following code makes a new function, println, available to the runtime. This function takes one argument, which it prints out to the standard output:
\begin{lstlisting}
class PrintingFunction implements Function {
    Object apply(Yolan args[]) {
        System.out.println(args[0].value());
    }
}
class Main {
    public static void main(String [] args) throws java.io.IOException {
        Yolan.addFunction("println", new PrintingFunction);
        Yolan.eval(new FileInputStream(new File("script.yl")));
    }
}
\end{lstlisting} 
The above program reads and evaluates the file script.yl, with an augmented runtime which also has the println function.

\subsubsection{Values and types}
The builtin types in Yolan are mapped to Java classes for easier interoperability,
so lists are implemented as java.util.Stack, dictionaries are implemented as java.util.Hashtable, strings are implemented as java.lang.String, nil/false are implemented as the value null, integers are implemented as java.lang.Integer, and iterators are implemented as java.util.Enumeration. 
Operations on those data types are just as the native builtin types. 

Any Java object can be passed around within Yolan, so adding support for new data types is just a question of adding functions that work on those data types.

\subsubsection{Functions defined withinin Yolan}
When a user defines a function within Yolan, they are instances of the Yolan class. 
Before calling such a function, the number of arguments can be found using the \verb|nargs| method.
If the Yolan object is not a callable user defined function, the result of \verb|nargs()| is -1, which thus can be used to check if a Yolan object is a callable function.
The function is then applied with the \verb|apply| method, which takes the arguments to the function as arguments, for example:
\begin{lstlisting}
...
    // evaluation some yolan object that yields a function
    Yolan function = yl.value();
    // ensure that it is a function and it takes two arguments
    if(function.nargs() == 2) {
        // apply the function 
        result = function.apply(arg1, arg2);
    } else ...
....
\end{lstlisting}
The apply method is defined from zero, up to three arguments. If there is a need for an apply method with more arguments, they are simple to add, see page~\pageref{source-yolan-apply} for the implementation details. There is also a general apply method, that takes an array of arguments as argument.

\subsubsection{Modifying the runtime}

In order for the scripting language to be practical, it should be able to work and share data with the host application. 
Of course this can be done with functions, and evaluation, as described above, but an additional connection with the language can be added by accessing the variables defined, and used, by the running scripts.
For this there are three functions: \verb|Yolan.resolveVar|, \verb|Yolan.getVar|, and \verb|Yolan.setVar|.

When a value is accessed, this is done through a handle, which is found with \verb|resolveVar|. This handle can then be used for reading and writing the variable. The motivation for the handle is that it takes time to lookup what a variable name, so this computation can be done once for each variable that needs to be accessed, and then additional accesses to the resolved variable are significant faster. The \verb|resolveVar| function takes the variable name as a string parameter, and return the handle, which is an integer. If the variable does not exist in the runtime, space is allocated for it.

With a handle, it is then possible to set the value of a variable with \verb|setVar|. For example setting the variable foo to 42 can be done with:
\begin{lstlisting}
    Yolan.setVar(Yolan.resolveVar("foo"), new Integer(42));
\end{lstlisting} 
and similarly the variable can be read with \verb|getVar|:
\begin{lstlisting}
    Object result = Yolan.getVar(Yolan.resolveVar("foo"));
\end{lstlisting}

If it the variable is commonly accessed, it saves time to cache the handle across calls, as follows:
\begin{lstlisting}
class Class {
    int fooHandle;
    Class() {
        fooHandle = Yolan.resolveVar("foo");
    }

    int someMethod() {
        ... perhaps some scripts modifying foo is executed ...
        Object foo = Yolan.getVar(fooHandle);
        ...
    }

    void otherMethod() {
        ... 
        Yolan.setVar(fooHandle, "A literal value or some variable");
        ...
    }
}
\end{lstlisting}
When defining functions, as described earlier, it is actually the same that is happening: the function is encapsulated in a Yolan object and added to the runtime as with \verb|setVar|.

\subsubsection{Resetting the runtime and saving space}
When the scripting language is only used in some part of the application, it can be pratical to be able to unload its runtime data in order to save memory. 
For this there are two utility functions \verb|Yolan.wipe()| and \verb|Yolan.reset()|.

\verb|Yolan.wipe()| sets all references in the runtime are set to zero, allowing data to be garbage collected.
When the runtime has been wiped, Yolan expressions can no longer be evaluated, and trying to evaluate them will yield errors. 

\verb|Yolan.reset()| resets the runtime: all variable handles are invalidated, all variables are removed from the runtime, and only the builtin functions are added. Existing Yolan expressions are invalid, and evaluation of them may lead to unexpected behavior. User defined functions and variables need to be re-added.
Resetting is necessesary before scripts are executed after a \verb|Yolan.wipe()|.
It can also be practical when multiple scripts are run, one after each other, and they must not mess up the runtime for each other.

\subsection{More on adding functions to the runtime}
The naive approach for adding functions to the runtime would be to create a new class implementing the \verb|Function| interface for each function. This adds significantly to the JAR-file
.
If the code size is critical, this can often be reduced by combining the multiple functions into a single class, for example via a switch dispatch as shown below:
\begin{lstlisting}
class ManyFunction implements Function {
    int id;
    ManyFunction(int id) {
        this.id = id;
    }
    Object apply(Yolan args[]) {
        switch(id) {
            case 0: // first function
                    ....
                break;
            case 1: // second function
                    ....
                break;
            case 2: // third function
                    ....
                break;
            ....
            default:
                throw SomeKindOfError();
    }
    static void register() {
        Yolan.addFunction(new ManyFunction(0), "firstFunction");
        Yolan.addFunction(new ManyFunction(1), "secondFunction");
        Yolan.addFunction(new ManyFunction(2), "thirdFunction");
        ....
    }
}
\end{lstlisting}

When implementing functions, it is also possible to create control structures, due to the laziness of Yolan objects. This is done by not necessarily calling the parameters \verb|value|-functions exactly one time each. The example below shows how the usual if-statement could be implemented:
\begin{lstlisting}
class YolanIf implements Function {
    Object apply(Yolan args[]) {
        // first evaluate the condition
        // and find out if it i true (not null)
        if(arg[0].value() != null) {
            // only evaluate if the condition yields true
            return arg[1].value();
        } else {
            // only evaluate if the condition yield false
            return arg[2].value();
        }
    }
}
\end{lstlisting}

\section{Summary}
\chapter{LightScript}
\label{lightscript}
\index{LightScript}
\section{Design choices}
The next paragraphs elaborate the following design choices:
\begin{itemize}
\item A subset of EcmaScript.
\item Should be able to run on CLDC 1.0/MIDP 1.0
\item Use standard Java classes, for easier interaction with Java code
\item Execution of code, loaded as source text at run time
\item Functions as first class values
\item Parsing without error checking
\item Integer only, - no support for floating point number.
\item Undefined, null and false, is joined in a single type/value and is the only false value.
\item Support for closures, lexical scope
\item Efficient variable access
\item Objects, and prototypical inheritance with a self-like \verb|clone| function
\item Exceptions
\end{itemize}

There are several reasons to make the language a subset of EcmaScript:
When LightScript is a subset of EcmaScript, it automatically runs within most webbrowsers, including within smartphones where Java may not be installed.
Being an EcmaScript subset also makes it easier to get started on for developers, that already know EcmaScript, or other languages with C-like syntax.
The issues are that being a subset of EcmaScript will add more complexity to LightScript, and also that EcmaScript has some unfortunate design choices \cite{crockford}, e.g. global variables by default, function-scoping rather then block-scoping, non-transitive equality test, traversal of parent object in for-each, arrays and null are objects in typeof-operator, restrictions on names due to many reserved words, automatic semi colon insertion, the with-statement, 
where some of these will also be a part of LightScript,
although many can be omitted by being a subset and thus more strict. 

Like Yolan, LightScript will run on CLDC 1.0/MIDP 1.0, and will use standard classes for better embedding and code footprint size, will support run time loading of source code, and will have functions as first class values. See the Yolan design choices on page~\pageref{yolandesign}.

The parser may assume that the programs have valid syntax. 
Usually parsers both build a syntax tree of valid programs and also reject programs with invalid syntax. 
By removing the rejecting-part, and only require that the can parsers build a syntax tree from valid programs, the parser may be optimised to run better on the limited devices.
The parser should still guarentee to terminate, even with invalid programs though the resulting syntax tree may be undefined.

LightScript only supports integers as number type, due to the platform. This is radically different from EcmaScript, which only supports floating point numbers. Script can still be compatible: addition, and multiplication, subtraction and modulo operations stay within the integer subset of floating point numbers, as long as there are no overflow, and they start out with integers. The shifts and bitwise operators temporarily cast to integer in EcmaScript, so they are not an issue. The only problem of the operators is division, and the solution here is to omit the \verb|/| binary operator from lightScript, and instead allow integer division to be implemented as a function \verb|div|, which can also be implemented in EcmaScript with a combination of division and rounding.

Similar to Yolan, LightScript also only have one false value, which also joins the \verb|undefined|, \verb|null|, and \verb|false| value of EcmaScript, which also removes the boolean type. This is an optimisation that makes truth test faster and slightly smaller, as they can be written as \verb|obj != null|, rather than
\verb%!( obj == null% \verb%|| (obj instanceof% \verb%Boolean% \verb%&&% \verb%!((Boolean)obj).booleanValue() )% \verb%||% \verb%(obj instanceof Integer &&% \verb%((Integer)obj).intvalue()% \verb%== 0 )% \verb%|| (obj instanceof String && obj.equals(""))%. It is still possible to preserve compatibility of scripts with EcmaScripts by requiring that conditions are always boolean values -- as it is required in many static typed languages. Another issue with this choice is, that there are no distinction between \verb|undefined| and \verb|null|, which may desirable.

LightScript should have a lexical scope similar to EcmaScript.
This has the benefit of allowing closures which makes the use of functions much more expressive.
The lexical scope in EcmaScript, and thus LightScript, is a bit different from most other lexical scoping, as the scope limit is the enclosing function rather than the block.

Similarly to Yolan, LightScript should support fast variable access. This also means that another approach than looking through the chain of execution context objects, as described in the EcmaScript standard must be used. Here LightScript should use a usual execution stack, boxing objects in the closure onto the heap.

LightScript should support an object system similar to EcmaScript.
Inheritance will be slightly different: while EcmaScript has prototypical inheritance, it mixes it with some Java/C++ like syntax. The semantics will be the same for LightScript, but the inheritance will be done with a self-like \verb|clone| clone, which seems more pure prototypical than EcmaScripts syntax. The \verb|clone| function can easily be implemented in EcmaScript.

Exceptions should also be supported, and it should also be possible to throw between, and across Java-functions and LightScript functions.

\section{Imperative implementation of top down operator precedence parsers}
\label{tdop-imp}
The top down operator precedence parser described in section~\ref{tdop} is targeted functional languages.
This section looks at how it can be implemented efficiently in an imperative language.

To reduce the code size of the parser, we assume that the source code is well formed, and do not require that the parser reports syntax errors. This for example removes the need for keeping track of where we are in the code and also allows some tokens to be joined, for example list termination does need not distinguish between different kinds of list.

First class functions use a lot of space in the Java platform, as they require a class each.
The solution is to use a dispatch function instead, and replace the function properties of the token, with integers.
Actually it is simpler with two dispatch functions, one for the null denominator functions and one for the left denominator functions.

The token object contains information about the denominator functions, and corresponding abstract syntax tree node IDs, and also a priority/binding power.
This is just five small integers, which, due to their limited range, easily can be represented in a single 32bit integer. 
Some tokens represents literal values or identifier, where the value or identifier also has to be passed to the parser.
So a token can be represented compactly by adding two properties to the parser: an integer and possible an object for the value, rather than a new class/object.

The token types can be encoded by the token string followed by the five integers.
So the tokens can be written as {\tt "tokenname" + (char) binding\_power + 
(char) null\_deno}\-{\tt{}minator\_function + (char) AST\_ID\_for\_null\_denominator
+ (char) left\_denomina}\-{\tt{}tor}\-{\tt{}\_function + (char) AST\_ID\_for\_left}\-{\tt{}\_denominator}. 
So the task of writing the parser is to make a list of tokens, connected with their binding power, denominator function ids and abstract syntax tree ids, -- a simple parse loop, plus definition of sensible denominator function bodies in the dispatch.
The actual implementation can be seen on page~\pageref{code-lightscript-parser}.


\subsection{Performance properties of the parser implementation}

For each token, there is a instruction cost of 1-2 function calls, 1 switch-dispatch, 0-1 comparison, reading of 2-3 properties of the token object, and storing a copy of the token val, plus the cost of building the actual AST node, and the cost of parsing the token.

The size of the implementation can be kept very small, as the denominator functions can be reused across different token-types. For example: the binary operators share a single case in the left denominator dispatch, and adding a new binary operator only takes the length in characters of the operator plus 5 bytes. 

\section{Implementation of variables and scope}
In the EcmaScript standard, identifier resolution is done by searching through the scope chain, which is a list of objects. Objects in the EcmaScript context is a mapping from property names to values. This approach to implementation would be very performance expensive.

Instead we want to resolve the variables at compile time, while preserving as much of the semantics as is practical. The main issue here is that we when we resolve the variable at compile time, we do not keep information to be able to resolve dynamically at run time, not supporting access to local variable with \verb|eval| statements, but as mentioned in section~\ref{designstyle}, eval is not availble in the first version of the language (though it could easily be added as an external java function that just calls LightScript).

Of the different implementation methods discussed in section~\ref{survey-scope}, the only real possibilities for a partly imperative scripting language which also want good support for higher order functions are either to allocate the activation records on the heap, or keep track of variables that could live on after exiting a function, and box those variable on the heap.
The other options are ruled out, as we want to have closures, and we want the outer scope variables to be mutable.
Allocating the activation records on the heap has the issue, that we are not in control of the garbage collector, and it may not be designed for that kind of usage, leading to expensive performance, so the approach will be only to box variables onto the heap, that could be alive after function exit.

To simplify this, it is done such that every variable that is added to a closure of an inner function is boxed on the heap, and other local variables are just stack allocated in the usual way. 
I have not seen this exact approach to implement closures other places, but it seems so obvious, that it is probably done somewhere before, although it clearly is different from the Lua approach with upvals\cite{luasrc}, and also from the approach following the EcmaScript standard\cite{ecmascript} directly.

For the practical implementation of the stack on top of the jvm, there are two obvious possibilities: A \verb|java.util.Stack| could be used, or a stack could be implemented manually with an array and an index pointer. 
In order to select implementation strategy, a microbenchmark was done on the kvm, which indicated that the array approach is significantly faster than the \verb|java.util.Stack|. The array grows dynamically when entering a function which uses more stack spaces than is available. 
The code footprint size is similar for both approachs, though probably a bit smaller for the Stack, as that one automatically grows, unlike then the array approach, which need a manual implementation.
The actual access to a \verb|Stack| requires a method calls, which are 3 bytes plus 1-3 bytes for self and parameter loading, where reading from an array is a single byte opcode plus 2-4 bytes for self, index and parameter loading, and at the same time the array approach sometimes need to adjust the index, costing 3 bytes.

\section{Overview of the implementation}
The core of the LightScript implementation consists of three parts: 1) a top down operator precedence parser that is responsible for building the syntax tree, and identify which variables needs to be boxed on the heap for closures, 2) a compiler which translate the abstract syntax tree to a sequence of opcodes while keeping track of the stack depth, and 3) a stack based virtual machine for executing the code.
Beside these core elements, the implementation also contain an API for embedding, and a library of standard functions.

The parser is a concrete implementation of the top down operator precedence parser for imperative laanguage, as described in section~\ref{tdop-imp}, with the additional detail, that during the parsing, the parser keep track of which variable is defined and used in each function, such that this information can be used for scoping later on. 
The source code for the parser starts on page~\pageref{lightscript-parser}.

\subsection{Compiler}

The core of the compiler is a function that compiles a node of the syntax tree, possibly calling itself recursively for the children.
As it is compiling to a stack based virtual machine, the compilation itself can be quite simple/small, as functions and operators, just need to evaluate its parameters in a way that pushes the results to the stack, followed by doing the operation on the top elements of the stack.

As the activation records is also allocated on the stack, the compiler needs to keep track of the current stack depth, so whenever emitting a function that changes the stack depth, the stack depth variable also needs to be kept updated.

The abstract syntax tree does not distinguise between statements and expressions. Instead the compiler function takes a parameter which indicates whether the generated code is expected to push a result on the stack or not, and corresponding code will be generated. 

By having the integer for the abstract syntax tree node type match the virtual machine instruction, many of the compilation cases can be joined.


\subsection{Virtual Machine}
The virtual machine is stack-based in order to reduce the footprint of the compiler.
It is implemented with a single larger switch statement as it is running on top of JVM, as the JVM, does not support for references to labels, etc.
The instruction set is inspired by the JVM and calling conventions on i386, and is also product of the iterative development, such that new instructions are added, as they are needed by the compiler.


\section{Language specification}
The language is mostly a subset of EcmaScript \cite{ecma-262}, and the description of the different parts of the languages is written in the same sequence as the EcmaScript standard, to make it easier to compare the two. The focus will on where LightScript differs from EcmaScript.
Scripts written for LightScript also runs unaltered in EcmaScript compliant interpreters, with af couple of extra functions defined within EcmaScript. On the other hand, EcmaScript may or may not run within LightScript, as LightScript is only a subset of EcmaScript.

The specification is stricter than the actual implementation, for example: the implementation does not distinguish between statements and expressions, and allows a statement everywhere an expression could be written, whereas the specification follows EcmaScript, and distinguishes the two.

Only a subset relatively small subset of EcmaScript is implemented: operators has been added as needed, meaning that rare operators has not yet been added. 
On the other hand, more interesting language aspects, such as exceptions and prototypical inheritance, have been implemented and tested, though they may not have been needed for the example programs or benchmarks.

The specification below follows the overall structure of the EcmaScript specification, to make it easier to compare the two.


\subsection{Lexical conventions}
The lexical conventions are slightly different from EcmaScript. 
Whitespaces are space, tab, carriage return and line feed. 
LightScript does not distinguish between whitespace and line terminators, and do not have automatic semicolon insertion.

Comments are started with two consecutive slashes \verb|//| and runs to the next newline character.

Keywords reserved by EcmaScript are also reserved by LightScript. The keywords currently used by LightScript are: {\tt catch do else for function if return this throw try var while}. This will be expanded in the next version of LightScript.

Identifiers start with a letter or an underscore, and continues with any combination of letters, underscores and digits. 
LightScript does not support escaped unicode sequences, but do accept utf-8 encoded. Non-letter unicode symbols with value larger than 127 are not supported. This allows a parsers to be implemented easier as they can just treat any 8-bit character with a value larger than 127 as a part of a unicode letter.

The punctuators in the current version of LightScript are: {\tt \verb|{| \verb|}| ( ) [ ] . ; , < > <= >= === !== + - * \% >> ! \&\& || ? : = += -=}. 
This will expand in the next version of LightScript where the remaining operators will be added, except for division as discussed in section~\ref{division} on page~\pageref{division}.

String literals are always enclosed in double qoutation \verb|"|, and single quotation is not supported. It is possible to use backslash \verb|\| to escape quotation marks, backslashes and newline (\verb|"\n"| is the string containing a newline).

Integer literals are written as a sequence of digits, not starting with a zero, unless the number is zero. Only base 10 literals are supported.

Other literals are \verb|true| which translates to a value that has a true truth value, and \verb|false|, \verb|null| and \verb|undefined| which translates to a value that has a false truth value.

\subsection{Types}
LightScript has 5 builtin types: nil, string, number, array, and objects.

The nil type only has one value, which is referenced to as \verb|undefined|, \verb|null| or \verb|false|. 
Strings are immutable sequences of characters.
Numbers are 32 bit integers.
An array is a growable and mutable sequence of values, where the values can have any type.
Objects are mappings between names and values, where the name should be of the string type, and the value can be of any type. Objects can have another object prototype, where, if the lookup in the first object does not find a mapping, the name is then looked up in the prototype object.
The order of traversal of the object names/values, may be implementation dependent such that implementations may choose to implement them via hashtables or lists, depending on what makes most sense for the platform. This is different from EcmaScript, where the traversal is in the order the properties were added to the object.

\subsection{Execution contexts}

With each function call a new execution context / activation record is created. 
Variabled defined with the \verb|var| keyword are allocated on the current activation record. 
If a variable is not in the current functions activation record,
it is looked up in the lexical outer function, or if there is no outer function, the global scope.
This is the usual static scoping, where functions are the only way to make a scope closure.

\subsection{Expressions}

When a function is called as a property on an object, evaluation of \verb|this| within the function yields the object.  Identifiers are evaluated in the scope to yield a value. Literal values are just their value.

Array are initialised with square brackets \verb|[| $elem_1,\cdots,elem_n$ \verb|]|. Elements can be any expression, and are seperated with commas (\verb|,|). 
Objects can similarly be initialised with curly brackets: \verb|{| 
$key_1: value_1, \cdots key_n: value_n$ \verb|}|, where the keys must be string literals, and values can be any expression.

Parenthesis can be used to group expressions together, for example: $2 * (3 + 4) \neq 2 * 3 + 4$

Objects and arrays can be subscripted with the bracket notation: $object$\verb|[|$key$\verb|]|, where $key$ can be be any expression that yields something that can be used as a subscript. The dot notation can be used as a shorthand for the case where $key$ is a string literal, such that $object$\verb|.|$name$ is equivalent to $object$\verb|["|$name$\verb|"]|.

The \verb|new| operator from JavaScript is not supported in LightScript, but instead objects can be initialised with \verb|{}|, or in case of inheritance, the \verb|clone| library function can be used.

Function calls are written as an expression followed by, a possibly empty, parenthesised argument list, e.g. $func$\verb|(|$arg_1,\cdots,arg_n$\verb|)|, where the arguments are separated by commas. 
A function written in LightScript, currently has a fixed number of arguments.

The postfix versions of increment/decrement are not supported, and the prefix versions should be used instead. The postfix operators has the effect of altering the variable, but returning the unaltered result, which can lead to difficult-to-read programs, and thus in the current version of LightScript, this is disencouraged by not having the postfix operators at all.

The implemented prefix operators are \verb|++|, \verb|--|, \verb|-|, and \verb|!|, which works as usual. \verb|delete|, \verb|typeof|, \verb|void| and \verb|~|, is not implemented in the current version, but may be added later on.

Implemented binary arithmetic operations are: multiplication \verb|*|, modulo \verb|%|, addition \verb|+|, subtraction \verb|-| and right shift \verb|>>|. These works only on numbers, except addition, that, if one of the parameters is not a number does string concatenation instead.  Division \label{division} is not implemented as \verb|/|, due to the semantic differences in integer and floating point division, but can instead be implemented as an integer division function, \verb|div(|$a,b$\verb|)|. The remaining shifting and bitwise operators, \verb|<<|, \verb|>>>|, \verb|&|, \verb%|%, \verb|^|, has not been needed, and thus not implemented yet, but will be added to the next version of LightScript.

Comparison operators are: \verb|<|, \verb|<=|, \verb|>|, \verb|>=|, \verb|!==|, \verb|===|, which works as usual. The equality operators does an actual comparison and not just a reference comparison. 
The type coerced equality operators from JavaScript \verb|!=| and \verb|==|, are omitted and should generally not be used \cite{javascriptnoeqeq} as they have some semantic issues, they are for example not transitive.

The current assignment operators supported by the language are: \verb|=|, \verb|+=| and \verb|-=|.
The remaining will be added in the next version.
The conditional operator \verb|? :| is supported. The comma operator \verb|,| is not supported, as that one generally is considered bad programming style \cite{crockford-comma}.

\subsection{Statements}
Statements are separated by semicolon \verb|;|.
Blocks consist of a sequence of statments within curly brackets \verb|{|$\cdots$\verb|}|. This may for example be needed in conjunction with \verb|if| or \verb|function| declaration. Notice that in JavaScript and thus also LightScript, a block is not a scope limit. In LightScript, blocks are only allowed, where they could actually be needed, e.g. in conjunction with different the conditional/iteration statements, function declaration and try/catch.

The conditional and iteration statements supported by LightScript are: \verb|if|, \verb|do|$\cdots$\verb|while|, \verb|while|, and \verb|for|, which works similarly to JavaScript.

Functions can be exited with the \verb|return| statement, where the optional parameter to the return statement is returned.

LightScript does not support the \verb|with|, as this is considered bad programming style.

\verb|switch| statements, \verb|break|, \verb|continue| and labelled statements are not supported. In JavaScript the semantics of the switch statement are mostly syntactic sugar for a sequence of \verb|if|-statements combined with an anonymous variable, as the expressions of the \verb|case|-clauses are evaluated sequentially and then compared to the result of the expression in the switch clause.
These may be added in later versions of LightScript.

Exceptions are thrown with the \verb|throw| statements, which works similarly to JavaScript.
The \verb|try| statment is slightly different, as LightScript only supports the \verb|try|$\cdots$\verb|catch| version, and does not currently support \verb|finally| option.
LightScript also has slightly different scoping rules for the catch block, where the caught variable is a usual variable, where JavaScript is creating a new object in the scope chain only containing this variable. The motivation for this change is that it is a more consistent approach to scoping, where this would otherwise be a special case, requireing more code space, where at the same time, this only an incompatibilities if the variable in the catch-statment shadows another variable, whose value is used after the catch block, - which also should be avoided as that would lead to more difficult-to-read code.

\subsection{Function definitions}
Functions are defined using the \verb|function| keyword, followed by an optional function name, a list of parameters and the function body.
If the function name is specified, a variable with that name is assigned to the function.

The LightScript currently does not support a variable numbers of parameters, nor can the functions have properties.
This is going to be added in the next version.
Functions are first class objects, and can be passed around, and used as such. Application of functions are done with the usual $f$\verb|(|$\cdots$\verb|)|.

\subsection{Native LightScript objects and functions}
The current LightScript implementation only has a small standard library, with the purpose of being a proof of concept that functions can easily be added. The following functions and methods are implemented:

\paragraph{print(value)} Utility function printing a value to standard out.
\paragraph{gettype(value)} Returns a string representation of the type of the parameter, either ``object'', ``array'', ``number'', ``undefined'' or ``builtin''. This can be used instead of the \verb|typeof| operator, which in EcmaScript has some semantic issues.
\paragraph{parseint(str)} Parses a string as a base 10 integer, and return the integer result.
\paragraph{clone(parent)} Create a new object using the parameter as the prototype.
\paragraph{Array.push(elem)} Method of arrays, pushes an element to the end of the array.
\paragraph{Array.pop()} Pops an element from the end of the array and returns it.
\paragraph{Array.join(sep)} Join the elements of an array as strings, with seperator between each neighbours.
\paragraph{Object.hasOwnProperty(name)} Property of prototype for objects, return whether the name, is actually a key in the current object. Used for determining whether a property is a part of the object, or its prototype.
\paragraph{*.length} A special property, indicating the length of an array or string, or the number of properties in an object.


% []subscript []array . {}hashtable {}block >> * %  ()function-call ()paren + - === !==  <= < >= > && || else in ?: = += -= var return ! ++ -- throw try catch function do-while while for if this undefined/null/false true "string" 123number



\section{Developers guide}
To evaluate code with LightScript, you first have to instantiate a LightScript object, which keeps track of global values, loaded libraries, and other stuff. The constructor takes no parameters, so creating it is just:
\begin{lstlisting}
    LightScript lsContext = new LightScript();
\end{lstlisting}
This context can then be used to evaluate LightScript code, using the \verb|eval| method. This method either takes a string or an \verb|java.io.InputStream| as parameter, which is then read and executed:
\begin{lstlisting}
    lsContext.eval("print(\"Hello world \" + 17 * 42)");
    lsContext.eval(new FileInputStream(new File("myscript.js")));
\end{lstlisting}
Global variables of the context can be read and written with the \verb|get| and \verb|set| method, so for example:
\begin{lstlisting}
    lsContext.set("foo", new Integer(17));
    lsContext.eval("bar = foo + 25;");
    System.out.println(lsContext.get("bar");
\end{lstlisting}
would print \verb|42|.

\subsection{Adding native functions to the runtime}

A method on a Java object can be called from LightScript if it implements the \verb|LightScriptFunction| interface, which defines an \verb|apply| method.
The interface is:
\lstinputlisting{../code/LightScript/LightScriptFunction.java}
So for example a function that returns the current number of milliseconds could be implemented as:
\begin{lstlisting}
class MillisecondsFunction implements LightScriptFunction {
    public Object apply(Object thisPtr, Object[] args, int argpos, 
                        int argcount) throws LightScriptException {
        return new Integer((int)System.currentTimeMillis());
    }
}
\end{lstlisting}

Adding a function to the runtime, is just like adding any other variable,
via using the \verb|put| method of the LightScript object.
So using the function above measuring some timings in LightScript can be done like:
\begin{lstlisting}
    lsContext.set("timer",new MillisecondsFunction());
    lsContext.eval("begin = timer();"
                  +"for(i=0;i<1000000;++i);"
                  +"print(\"Time used: \" + (timer() - begin));");
\end{lstlisting}

When registrering several functions, it is more compact to join them via a dispatch, 
so a class implementing a couple of functions could be implemented like:
\begin{lstlisting}
class FunctionLibrary implements LightScriptFunction {
    int id; // This tells which function the object represents
    public Object apply(Object thisPtr, Object[] args, int argpos, 
                        int argcount) throws LightScriptException {
        switch(id) {
            case 0: // integer division
                return new Integer(((Integer)args[argpos]).intValue()
                                  /((Integer)args[argpos+1]).intValue());
            case 1: // increment property i, not of superclass
                int i = ((Integer)((Hashtable)thisPtr).get("i")).intValue();
                ((Hashtable)thisPtr).put("i", new Integer(i + 1));
        }
        return null;
    }
    private FunctionLibrary(int id) { this.id = id; }
    public static void register(LightScript lsContext) {
        lsContext.set("div", new FunctionLibrary(0));
        lsContext.set("propinc", new FunctionLibrary(1));
    }
}
\end{lstlisting}
which could be used like:
\begin{lstlisting}
    FunctionLibrary.register(lsContext);
    lsContext.eval("obj = {}; obj.i = 1; obj.inc = propinc;"
                  +"while(obj.i < 10) {"
                  +"  print(div(42, obj.i));"
                  +"  obj.inc();"
                  +"}");
\end{lstlisting}

\subsection{Datatypes}
LightScript uses ordinary Java objects for most data.
Strings, stacks, tables, are as usual Java.
Boolean values are the constants \verb|LightScript.TRUE| and \verb|LightScript.FALSE|, which are not Java Booleans, but the string "true" and null.

LightScript objects with inheritance are instances of the \verb|LightScriptObject| class, which is subclass of \verb|java.lang.Hashtable|. LightScript objects have a constructor that corresponds to clone in Self, se section~\ref{survey-self}, and the parameter to the constructor is a hashtable or a LightScript object. The LightScript objects also overloads the hashtable get operator to access the cloned object, if the key was not found in this one.

Exceptions that can be thrown to/from LightScript are of the class \verb|LightScriptException|. This exception has a property \verb|value| that is the object that is thrown/caught within LightScript. The constructor just take the value as parameter.

\section{Versions and future directions}
The LightScript version described above is version 1.0.426

The version number consist of a major version number, a minor version number and a revision number.
The major version number is incremented at major rewrites and redesign.
The minor version number is incremented with milestones and expansions of language/added functionality, it uses an even/uneven strategy, such that even minor versions gets bugfixes, and the uneven minor versions are development versions where the new features are added, i.e. 
the features intended for version 1.2 are implemented in versions 1.1.\emph{something} and when all of them is added, the minor version number increases to 1.2, where only bug fixes will be added.
The revision number correspond to the svn version, and is incremented on each commit.


The following is the roadmap for changes to LightScript version 1:
\begin{itemize}
\item[1.0]
Prototype for thesis and proof of concept.
Version 1.0 is the one used for the benchmarks and described throughout this report, unless otherwise mentioned. 
\item[1.2]
Expansion of language and standard library, more operators from EcmaScript are added,
and an additional fixed point number representation will be added. Better compatibility with EcmaScript type system.
\item[1.4]
Extra optional libraries to enable writing of practical applications, mainly creating zooming user interface, and also add support for simple cryptography and web access.
\item[1.6]
Addition of a LightScript implementation for embedded C, targeting devices limited to 64K RAM.
\item[1.8]
Better development tool with a LightScript lint/prettyprinter, and possibly also interpreter with more run time checking and debugging support.
\end{itemize}

\subsection{Language update}
Where the previous parts of this chapter describes LightScript version 1.0, this section describes the most recent changes, and updates in the development branch, done after the report was written.
This section described what have been done in version 1.1 and what needs to be done before version 1.2
\subsubsection{Implemented changes}
\begin{itemize}
\item
\end{itemize}
\subsubsection{Remaining tasks}
\begin{itemize}
\item Add support for fixed-point arithmetics
\item Add remaining arithmetic operators, comparison, etc. 
\item Add postfix increment/decrement
\item Let functions support a variable number of arguments
\item Let catch, and function-def behave as implicit \verb|var|-decl
\item Change of the implementation to make the it even more EcmaScript-like by relying on LightScript object class in Java, rather than standard Java classes, This costs a slightly larger code footprint, and also makes it slightly more complex to integrate with existing Java applications.
To determine whether it is worthwhile needs an actual implementation.
\item Walk through library specification, and add functions that does not add too large a foorprint
\item Add prototype object to functions
\item Optimise code footprint, by reordering opcodes
\item Improve virtual machine by joining common instruction pairs to single instructions (adding superinstructions)
\item Replace stack allocation instruction with automatic code to do this, based on metadata of the function object 
\item Add clean-up code when exiting a function, to improve garbage collection
\end{itemize}

\subsection{Zooming user interface and other optional libraries}
... TODO: zui


\subsubsection{Other optional libraries}

Much interactions happens via web, so a simple function for getting and parsing web pages will be practical for enabling LightScript as a tool for mobile mashups would be needed. This will be a simple heuristic html parser mapping the XML-tree to JsonML\footnote{JsonML is an embedding of XML in JSON, somewhat similar to SXML in the Scheme} \cite{jsonml}, which can then easily be interacted with in LightScript.

When making applications that may handle the users private data, it should be secured, which is why support for simple cryptography should be added. There already exist a good open source library for mobile cryptography \cite{bouncycastle} where a couple of ciphers can be extracted and added as optional library in LightScript.

\subsection{Embedded implementation}

... TODO
... unique strings ... 16bit cells for more space

\section{Summary}
\chapter{Benchmarks}
\label{benchmark}

\section{Code footprint}
This section looks at the code footprint: first footprint estimates are compared for different scripting languages, then details on the footprint of LightScript is investigated, and finally the actual size of the library embedded in a minimal application is found.

\subsection{Comparison with other languages}
To compare the code footprint size across languages, I have added an estimate of the jar file size. 
The estimate is done by making a zip archive of the non-obscurified class files,
trying to exclude class files that are part of extra libraries.
The motivation for this approach is to make the comparison more fair, as some of the languages have large GUI libraries,
which would count against them, if we just looked at the jar file. 
In addition, compilation of some of the languages is obscurified/optimised by default,
which would give those an advantage. So this should give a more fair comparison.

\subsubsection{Languages}
\label{codefootprint-languages}
The code size benchmark looks at the languages implemented in this project, Yolan and LightScript, it looks at scripting languages for mobile devices, FScriptMe, Kahlua, Hecl, Simkin, and CellularBasic, and it looks at some general scripting language implementations,  JScheme, and Rhino.

FScriptME\cite{fscriptme} is the mobile edition of femto-script, which ``is an extremely simple scripting language''\cite{fscript}. 
Out of the box, it only supports strings and integers as data types -- no compound types -- which limits use somewhat, and it is still in beta, since 2002. 

JScheme\cite{jscheme} is a small Scheme implementation. An early version is benchmarked, as later versions have vastly larger code footprint, and would thus perform badly on that point. It depends on some reflection, and does therefor not run on Java Micro Edition, but is included as the implementation is compact, and may be changed to target mobile devices.

Kahlua\cite{kahlua} is an implementation of the Lua Virtual Machine for Java Micro Edition. 
The implementation requires CLDC-1.1 due to the use of floating point arithmetics, and does therefor not run on the lowest end mobile devices, which only supports CLDC-1.0.
Unlike the other languages, kahlua is not a full language interpreter, but only a virtual machine, so the the script cannot executed directly on the device, but needs to be compiled on another computer before being executed.

Hecl\cite{hecl} seems to be \emph{the} major scripting language for mobile devices, or at least the one keep popping up in the top of most queries when searching for scripting languages for mobile devices.
It is very portable, with different editions running on CLDC-1.0, CLDC-1.1, Android, as applets and as usual applications.
It has lots of libraries targeting mobile devices, which in this comparison were removed from the zip file of class files, not to give languages with fewer libraries an advantage in the size comparison.
The language is a dialect of Tcl, and is simplified such that arithmetic operators, and the like, are prefix operators so expressions ends up a bit lisp-like.

Simkin\cite{simkin} is a scripting language for being embedded in XML, which also runs on mobile devices. It depends on kxml xml library, which is not included in the measured size. 

CellularBasic\cite{cellularbasic} is a dialect of Qbasic, implemented for mobile devices. 
It includes a floating point support library which is not included in the measured size. 

Rhino\cite{rhino} is a JavaScript implementation. This language is not designed for, nor does it run on, mobile devices. It is included as an example of an implementation of a usual non-mobile scripting language.

\subsubsection{Results}
The approximated JAR sizes of the scripting languages are:
\begin{center}
\begin{tabular}{|c|r|} \hline 
Yolan & 7K \\ \hline 
LightScript & 14K \\ \hline 
\end{tabular}
\begin{tabular}{|c|r|} \hline 
FScriptME & 17K \\ \hline 
Jscheme & 29K \\ \hline 
Kahlua & 39K \\ \hline 
Hecl & 54K \\ \hline 
Simkin & 81K \\ \hline 
CellularBasic & 83K \\ \hline 
Rhino & 397K \\ \hline 
\end{tabular}
\end{center}

The languages developed in this project smaller than other scripting languages for the platform. 
Yolan is approximately half the size of LightScript. 

\subsection{Details on the footprint of LightScript}
The sizes of the different part of the LightScript class is listed below.
Reduction is the reduction of the full class file when the mentioned part is left out. Alone is the size of the class file with everything else than the mentioned part left out. These numbers are different as some things are shared and some things cannot be left out when compiling the class file.

\begin{center} \begin{tabular}{|r|r|rl|} \hline
\multicolumn{2}{|r|}{Reduction} & \multicolumn{2}{|l|}{Alone}\\ \hline
Everything & 15030 & 17706 & \\ \hline
API & 645  & 3597  & \\ \hline
Tokeniser & 1261 & 4048 & \\ \hline
Parser & 2542 & 5850 & \\ \hline
Compiler & 5324 & 8413 & \\ \hline
Vm & 4589 & 7650 & \\ \hline
Parser+Tokeniser & 3825 & 7093 & \\ \hline
\end{tabular} \end{center}

\subsection{Optimised JAR-file footprint}
Previous sections have looked at comparable footprints for implementations by approximated JAR file size, and also at what parts contributes to the size of the LightScript class.
From a practical view it is also interesting to see the actual size of the optimised obscurified JAR-file of embedding the languages in a trivial host application.
The host application is a simple Midlet that adds a print function to the language, and includes a hello-world program.
The size of the entire application, including the embedded scripting language implementation is 5290 bytes for Yolan and 11342 bytes for LightScript. 

\section{Execution speed}
This section benchmarks the execution speed of the languages. 
In the following subsections, the languages that are benchmarked, and the actual benchmark programs are described.
The source code for the benchmarks can be seen in appendix~\ref{benchmarksource}.
The results of the benchmarks can be seen in figure~\ref{figure-execution-speed}.

\subsection{Languages}
The benchmarks are run on those of the languages from section~\ref{codefootprint-languages}, that have an approximated jar size of less than 64K. They also run on two JavaScript interpreters: Rhino 1.6r7 and SpiderMonkey 1.7.0, which are described in section~\ref{spidermonkey}, and are the default versions when installed on Ubuntu Linux.

\subsection{The benchmarks}
The benchmarks are the following:

\paragraph{Fibonacci:} Recursive calculation of the 30'th Fibonacci number
\paragraph{Loops:} Nested loops with counters, 10.000.000 iterations
\paragraph{Recursion:} Highly recursive benchmark, similar to recursive control-flow benchmark from \cite{sunspider, shootout}. Uses lots of stack space. On some of the languages where it fails, only the first part of it was implemented.
\paragraph{Sieve:} Simple implementation of Erasthones sieve - not implemented in languages which have already shown to be very slow in earlier benchmarks
\paragraph{For-in:} Nested loops across keys of a dictionary, 1.000.000 iterations - not implemented in languages which have already shown to be very slow in earlier benchmarks
\paragraph{Primes:} Simple primality test by looking at the remainders of division - not implemented in languages which have already shown to be very slow in earlier benchmarks
\paragraph{Exception:} Throw/catch 500.000 exceptions - only implemented for LightScript/JavaScript
\paragraph{Fannkuch:} Access-fannkuch benchmark from \cite{sunspider, shootout} - only implementd for LightScript/JavaScript

\subsection{Results}
The measurement of the performance of the different scripting languages are shown below. The timings are seconds per benchmark. $\bot$ indicates that the benchmark does not complete due to running out of stack space.

\begin{center} \begin{tabular}{|r|r|r|r|r|r|r|r|rr|} \hline 
& \multicolumn{2}{|l|}{Fibonacci} & \multicolumn{2}{|l|}{Recursion} & \multicolumn{2}{|l|}{For-in} & \multicolumn{2}{|l}{Exceptions} & \\
& & \multicolumn{2}{|l|}{Loops} & \multicolumn{2}{|l|}{Sieve} & \multicolumn{2}{|l|}{Primes} & \multicolumn{2}{|l|}{Fannkuch} \\
\hline Rhino       & 1.20 & 1.74 & 1.75   & 2.97 & 1.18 & 12.35 & 45.99 & 6.35 & \\ 
\hline SpiderMonkey& 1.28 & 1.71 & $\bot$ & 2.08 & 1.14 & 11.03 & 0.45  & 5.10 & \\ 
\hline LightScript & 1.37 & 3.45 & 2.35   & 1.19 & 0.57 & 11.70 & 0.65  & 11.15 & \\
\hline Yolan       & 1.47 & 2.23 & $\bot$ & 1.95 & 0.32 &  9.20 &  &  & \\
\hline Kahlua      & 2.13 & 1.18 & $\bot$ &  5.73 & 2.26 & 5.49 &  &  & \\ 
\hline JScheme    & 29.77 & 93.22 & $\bot$ & & & & & & \\ 
\hline FscriptME & 176.27 & 112.68& $\bot$ & & & & & & \\ 
\hline Hecl      & 207.96 & 47.21 & $\bot$ & & & & & & \\ 
\hline \end{tabular}
\end{center} 

\section{Summary}
\chapter{Discussion}
\label{discussion}


Yolan does a good job having a tiny code footprint and decent performance, while being a scripting language with first class function, hashtables, easy embeddable, etc., and does a bad job trying to do anything else.
The dynamic scoping makes it unsuitable for developing larger applications, and the lisp-like syntax will probably scare away most developers.
So the major use of this language is probably just to set a bar for how small a code footprint a high level scripting language can have.

LightScript has tradeoffs between EcmaScript compliance and size/performance. 
One of those is the joining of \verb|false|, \verb|null| and \verb|undefined|, and not needing the boolean type.
As the overall size and performance of LightScript were better than expected,
that optimisation-tradeoff is probably worth undoing to get better semantics, 
but only an implementation and actual benchmark will show. 

\section{Benchmarks}
\subsection{Speed}

It is interesting to see that Rhino, SpiderMonkey, LightScript, Yolan, and Kahlua is all within the same magnitude of speed, indicating that this is the speed you get with a simple interpreter for these kinds of scripting languages. Here it is also suprising that Rhino and SpiderMonkey is not much faster, as Rhino seems to be doing something with generating and loading Java classes at run time, and SpiderMonkey is written in C rather than Java.

Other observation are that Rhino is slow with exceptions, and that Kahlua seems to be fast with loops and slow with arrays. The last is probably due to that Kahlua implement arrays as hashtables, and loops in Lua has the integer sequence in the \verb|for|-construct, allowing \verb|for|-loops to be simpler to evaluate that in JavaScript, LightScript and Yolan, where the counter variable is explicit updated within the scripting language.

It is also interesting how similar in performance Yolan and LightScript are, even though they have very different evaluation strategy: Yolan is interpreted by traversing a syntax tree data structure, whereas LightScript runs on a stack based virtual machine. 

Most of the scripting languages with a small code footprint is an order of magnitude slower than LightScript, Yolan, and Kahlua and the JavaScript implementations.
This may be due to inefficient implementation of variable access in JScheme, FScriptME and Hecl. Looking at the source code, JScheme and FScriptME variables are looked up in a linked list or hashtable at each access. Hecl has a more complex lookup mechanism with a stack of hashtables, but also employes some kind of caching.



\chapter{Conclusion}
\label{conclusion}


%\chapter{Introduction to scripting \\ -- a user guide}
%\input{language/userguide}
%

\newpage
\addcontentsline{toc}{chapter}{Bibliography}
\bibliography{bibliography}
\bibliographystyle{alpha}

\appendix

\input{../code/bench/marks.tex}

\chapter{Source code of the Yolan class}
\lstinputlisting{../code/Yolan/src/Yolan.java}

\chapter{LightScript}
\section{The LightScript class}
\lstinputlisting{../code/LightScript/LightScript.javapp}
\section{Standard library}
\lstinputlisting{../code/LightScript/LightScriptStdLib.java}
\section{Object class}
\lstinputlisting{../code/LightScript/LightScriptObject.java}


\newpage
\addcontentsline{toc}{chapter}{Index}
\printindex

\end{document}

